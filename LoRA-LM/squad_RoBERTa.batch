#!/bin/sh
#SBATCH --partition=amdgpu
#SBATCH --time=24:00:00
#SBATCH --gres=gpu:1
#SBATCH --ntasks-per-node=1 # tasks per node
#SBATCH --mem-per-gpu=80000
#SBATCH --job-name=RoBERTa_squad
#SBATCH --err=RoBERTa_InA_squad.err
#SBATCH --out=RoBERTa_InA_squad.out
#SBATCH --mail-user=kangchen@fel.cvut.cz    # where send info about job
#SBATCH --mail-type=ALL              # what to sen  d, valid type values are NONE, BEGIN, END, FAIL, REQUEUE, ALL

/bin/hostnamesbatch
srun -l /bin/hostname
srun -l /bin/pwd


ml Python/3.10.4-GCCcore-11.3.0
source /home/kangchen/Chatbot/Psych_BioGPT/EnvLlama/bin/activate


#####################################################################
#####################################################################
cd /home/kangchen/inhibited_lora/LoRA-LM/
python lm_QLoRA.py \
--model_name FacebookAI/roberta-large \
--dataset_name data/squad_v2/ \
--lora_r 8 \
--lora_alpha 16 \
--lora_inhibition 0.0 \
--lora_dropout 0.1 \
--num_warmup_epochs 1 \
--num_train_epochs 10 \
--batch_size 16 \
--output_dir Output_PEFT

cd /home/kangchen/inhibited_lora/visualization/
python visualize_lm.py --adapter_name=/home/kangchen/inhibited_lora/LoRA-LM/Output_PEFT/FacebookAI/roberta-large/ \
--model_name FacebookAI/roberta-large \
--task="squad_v2" \
--lora_inhibition 0.0


#####################################################################
#####################################################################
cd /home/kangchen/inhibited_lora/LoRA-LM/
python lm_QLoRA.py \
--model_name FacebookAI/roberta-large \
--dataset_name data/squad_v2/ \
--lora_r 8 \
--lora_alpha 16 \
--lora_inhibition 0.1 \
--lora_dropout 0.1 \
--num_warmup_epochs 1 \
--num_train_epochs 10 \
--batch_size 16 \
--output_dir Output_PEFT

cd /home/kangchen/inhibited_lora/visualization/
python visualize_lm.py --adapter_name=/home/kangchen/inhibited_lora/LoRA-LM/Output_PEFT/FacebookAI/roberta-large/ \
--model_name FacebookAI/roberta-large \
--task="squad_v2" \
--lora_inhibition 0.1


#####################################################################
#####################################################################
cd /home/kangchen/inhibited_lora/LoRA-LM/
python lm_QLoRA.py \
--model_name FacebookAI/roberta-large \
--dataset_name data/squad_v2/ \
--lora_r 8 \
--lora_alpha 16 \
--lora_inhibition 0.3 \
--lora_dropout 0.1 \
--num_warmup_epochs 1 \
--num_train_epochs 10 \
--batch_size 16 \
--output_dir Output_PEFT

cd /home/kangchen/inhibited_lora/visualization/
python visualize_lm.py --adapter_name=/home/kangchen/inhibited_lora/LoRA-LM/Output_PEFT/FacebookAI/roberta-large/ \
--model_name FacebookAI/roberta-large \
--task="squad_v2" \
--lora_inhibition 0.3


#####################################################################
#####################################################################
cd /home/kangchen/inhibited_lora/LoRA-LM/
python lm_QLoRA.py \
--model_name FacebookAI/roberta-large \
--dataset_name data/squad_v2/ \
--lora_r 8 \
--lora_alpha 16 \
--lora_inhibition 0.9 \
--lora_dropout 0.1 \
--num_warmup_epochs 1 \
--num_train_epochs 10 \
--batch_size 16 \
--output_dir Output_PEFT

cd /home/kangchen/inhibited_lora/visualization/
python visualize_lm.py --adapter_name=/home/kangchen/inhibited_lora/LoRA-LM/Output_PEFT/FacebookAI/roberta-large/ \
--model_name FacebookAI/roberta-large \
--task="squad_v2" \
--lora_inhibition 0.9




