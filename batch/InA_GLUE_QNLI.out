n29
0: n29
0: /home/kangchen/inhibited_lora/batch
08/06/2025 09:12:34 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
08/06/2025 09:12:34 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_revision=None,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
liger_kernel_config=None,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_final/BERT_large/QNLI/InA00/runs/Aug06_09-12-33_n29,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=10.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=output_final/BERT_large/QNLI/InA00/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=output_final/BERT_large/QNLI/InA00/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
08/06/2025 09:12:36 - INFO - datasets.builder - Generating dataset glue (/home/kangchen/.cache/huggingface/datasets/nyu-mll___glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
08/06/2025 09:12:36 - INFO - datasets.builder - Downloading and preparing dataset glue/qnli to /home/kangchen/.cache/huggingface/datasets/nyu-mll___glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c...
08/06/2025 09:12:37 - INFO - datasets.download.download_manager - Downloading took 0.0 min
08/06/2025 09:12:37 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
08/06/2025 09:12:37 - INFO - datasets.builder - Generating train split
08/06/2025 09:12:37 - INFO - datasets.builder - Generating validation split
08/06/2025 09:12:37 - INFO - datasets.builder - Generating test split
08/06/2025 09:12:37 - INFO - datasets.utils.info_utils - All the splits matched successfully.
08/06/2025 09:12:37 - INFO - datasets.builder - Dataset glue downloaded and prepared to /home/kangchen/.cache/huggingface/datasets/nyu-mll___glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c. Subsequent calls will reuse this data.
The task type of PEFT is not proper!
LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='bert-large-cased', revision=None, task_type=<TaskType.SEQ_CLS: 'SEQ_CLS'>, inference_mode=False, r=4, target_modules={'value', 'key', 'query'}, lora_inhibition=0.0, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
###############
PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): BertForSequenceClassification(
      (bert): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 1024, padding_idx=0)
          (position_embeddings): Embedding(512, 1024)
          (token_type_embeddings): Embedding(2, 1024)
          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0-23): 24 x BertLayer(
              (attention): BertAttention(
                (self): BertSdpaSelfAttention(
                  (query): lora.Linear(
                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=4, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=4, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (key): lora.Linear(
                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=4, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=4, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (value): lora.Linear(
                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=4, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=4, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
          (activation): Tanh()
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (classifier): ModulesToSaveWrapper(
        (original_module): Linear(in_features=1024, out_features=2, bias=True)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=1024, out_features=2, bias=True)
        )
      )
    )
  )
)
08/06/2025 09:12:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/kangchen/.cache/huggingface/datasets/nyu-mll___glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-857a5657ab1232c9.arrow
08/06/2025 09:12:57 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/kangchen/.cache/huggingface/datasets/nyu-mll___glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-8f3d515fc78a6c18.arrow
08/06/2025 09:12:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/kangchen/.cache/huggingface/datasets/nyu-mll___glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-bc9d8598d5afffcf.arrow
08/06/2025 09:13:00 - INFO - __main__ - Class distribution in train set:
08/06/2025 09:13:00 - INFO - __main__ -   Label 1: 52366 (49.99%)
08/06/2025 09:13:00 - INFO - __main__ -   Label 0: 52377 (50.01%)
08/06/2025 09:13:00 - INFO - __main__ - Class distribution in validation set:
08/06/2025 09:13:00 - INFO - __main__ -   Label 0: 2702 (49.46%)
08/06/2025 09:13:00 - INFO - __main__ -   Label 1: 2761 (50.54%)
08/06/2025 09:13:00 - INFO - __main__ - Class distribution in test set:
08/06/2025 09:13:00 - INFO - __main__ -   Label -1: 5463 (100.00%)
08/06/2025 09:13:00 - INFO - __main__ - Sample 83810 of the training set: {'question': 'What tactics of the PVA were US and ROK troops not prepared to handle?', 'sentence': "On 27 November at the Korean eastern front, a U.S. 7th Infantry Division Regimental Combat Team (3,000 soldiers) and the U.S. 1st Marine Division (12,000–15,000 marines) were unprepared for the PVA 9th Army Group's three-pronged encirclement tactics at the Battle of Chosin Reservoir, but they managed to escape under Air Force and X Corps support fire—albeit with some 15,000 collective casualties.", 'label': 0, 'idx': 83810, 'input_ids': [101, 1327, 10524, 1104, 1103, 153, 12152, 1127, 1646, 1105, 155, 22027, 2830, 1136, 4029, 1106, 4282, 136, 102, 1212, 1765, 1379, 1120, 1103, 3947, 2638, 1524, 117, 170, 158, 119, 156, 119, 4766, 4155, 1784, 3391, 1348, 10314, 2649, 113, 124, 117, 1288, 2803, 114, 1105, 1103, 158, 119, 156, 119, 2198, 4620, 1784, 113, 1367, 117, 1288, 782, 1405, 117, 1288, 5243, 1116, 114, 1127, 8362, 1643, 1874, 17482, 1174, 1111, 1103, 153, 12152, 5612, 1740, 1990, 112, 188, 1210, 118, 5250, 18288, 4035, 6617, 24490, 1880, 10524, 1120, 1103, 2651, 1104, 22964, 10606, 15220, 117, 1133, 1152, 2374, 1106, 3359, 1223, 1806, 2300, 1105, 161, 3158, 1619, 1783, 783, 12456, 1114, 1199, 1405, 117, 1288, 7764, 8487, 119, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}.
08/06/2025 09:13:00 - INFO - __main__ - Sample 14592 of the training set: {'question': "How did the Egyptian people feel about Nasser's response to the attack?", 'sentence': 'Nasser did not feel that the Egyptian Army was ready for a confrontation and did not retaliate militarily.', 'label': 1, 'idx': 14592, 'input_ids': [101, 1731, 1225, 1103, 6210, 1234, 1631, 1164, 11896, 14607, 112, 188, 2593, 1106, 1103, 2035, 136, 102, 11896, 14607, 1225, 1136, 1631, 1115, 1103, 6210, 1740, 1108, 2407, 1111, 170, 14002, 1105, 1225, 1136, 1231, 6163, 15045, 1940, 12888, 18206, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
08/06/2025 09:13:00 - INFO - __main__ - Sample 3278 of the training set: {'question': 'What kind of aircraft are capable of taking off vertically?', 'sentence': 'Although STOVL aircraft are capable of taking off vertically from a spot on the deck, using the ramp and a running start is far more fuel efficient and permits a heavier launch weight.', 'label': 0, 'idx': 3278, 'input_ids': [101, 1327, 1912, 1104, 2163, 1132, 4451, 1104, 1781, 1228, 22026, 136, 102, 1966, 23676, 2346, 2559, 2162, 2163, 1132, 4451, 1104, 1781, 1228, 22026, 1121, 170, 3205, 1113, 1103, 5579, 117, 1606, 1103, 14207, 1105, 170, 1919, 1838, 1110, 1677, 1167, 4251, 7856, 1105, 15267, 170, 12163, 4286, 2841, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
08/06/2025 09:13:01 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.6184, 'grad_norm': 18.360958099365234, 'learning_rate': 4.980944015886352e-05, 'epoch': 0.04}
{'loss': 0.459, 'grad_norm': 5.784853458404541, 'learning_rate': 4.961849843427786e-05, 'epoch': 0.08}
{'loss': 0.4174, 'grad_norm': 2.51664137840271, 'learning_rate': 4.94275567096922e-05, 'epoch': 0.11}
{'loss': 0.398, 'grad_norm': 10.682733535766602, 'learning_rate': 4.923661498510655e-05, 'epoch': 0.15}
{'loss': 0.3894, 'grad_norm': 15.745842933654785, 'learning_rate': 4.904567326052089e-05, 'epoch': 0.19}
{'loss': 0.3903, 'grad_norm': 12.938901901245117, 'learning_rate': 4.885473153593523e-05, 'epoch': 0.23}
{'loss': 0.3823, 'grad_norm': 4.350727558135986, 'learning_rate': 4.866378981134958e-05, 'epoch': 0.27}
{'loss': 0.3678, 'grad_norm': 7.401341915130615, 'learning_rate': 4.847284808676392e-05, 'epoch': 0.31}
{'loss': 0.3682, 'grad_norm': 6.7927045822143555, 'learning_rate': 4.828190636217826e-05, 'epoch': 0.34}
{'loss': 0.3676, 'grad_norm': 5.334386825561523, 'learning_rate': 4.809096463759261e-05, 'epoch': 0.38}
{'loss': 0.367, 'grad_norm': 1.8467634916305542, 'learning_rate': 4.790002291300695e-05, 'epoch': 0.42}
{'loss': 0.3636, 'grad_norm': 3.8947274684906006, 'learning_rate': 4.7709081188421295e-05, 'epoch': 0.46}
{'loss': 0.3627, 'grad_norm': 9.365954399108887, 'learning_rate': 4.7518139463835637e-05, 'epoch': 0.5}
{'loss': 0.3575, 'grad_norm': 8.389982223510742, 'learning_rate': 4.7327197739249985e-05, 'epoch': 0.53}
{'loss': 0.3491, 'grad_norm': 3.5220565795898438, 'learning_rate': 4.713625601466433e-05, 'epoch': 0.57}
{'loss': 0.3472, 'grad_norm': 8.75337028503418, 'learning_rate': 4.694531429007867e-05, 'epoch': 0.61}
{'loss': 0.3541, 'grad_norm': 10.88392162322998, 'learning_rate': 4.675437256549302e-05, 'epoch': 0.65}
{'loss': 0.3415, 'grad_norm': 9.188912391662598, 'learning_rate': 4.656343084090736e-05, 'epoch': 0.69}
{'loss': 0.3306, 'grad_norm': 12.384138107299805, 'learning_rate': 4.63724891163217e-05, 'epoch': 0.73}
{'loss': 0.3483, 'grad_norm': 3.4579851627349854, 'learning_rate': 4.618154739173605e-05, 'epoch': 0.76}
{'loss': 0.3282, 'grad_norm': 2.281789541244507, 'learning_rate': 4.599060566715039e-05, 'epoch': 0.8}
{'loss': 0.3339, 'grad_norm': 9.428650856018066, 'learning_rate': 4.579966394256473e-05, 'epoch': 0.84}
{'loss': 0.3296, 'grad_norm': 8.268836975097656, 'learning_rate': 4.560872221797908e-05, 'epoch': 0.88}
{'loss': 0.3282, 'grad_norm': 0.4855998158454895, 'learning_rate': 4.541778049339342e-05, 'epoch': 0.92}
{'loss': 0.3386, 'grad_norm': 2.2222981452941895, 'learning_rate': 4.522683876880776e-05, 'epoch': 0.95}
{'loss': 0.3348, 'grad_norm': 0.8433657288551331, 'learning_rate': 4.5035897044222105e-05, 'epoch': 0.99}
{'loss': 0.3212, 'grad_norm': 6.2412285804748535, 'learning_rate': 4.484495531963645e-05, 'epoch': 1.03}
{'loss': 0.3232, 'grad_norm': 2.8477301597595215, 'learning_rate': 4.4654013595050795e-05, 'epoch': 1.07}
{'loss': 0.3258, 'grad_norm': 5.843940734863281, 'learning_rate': 4.4463071870465137e-05, 'epoch': 1.11}
{'loss': 0.333, 'grad_norm': 9.273938179016113, 'learning_rate': 4.427213014587948e-05, 'epoch': 1.15}
{'loss': 0.3242, 'grad_norm': 4.433589458465576, 'learning_rate': 4.408118842129382e-05, 'epoch': 1.18}
{'loss': 0.3084, 'grad_norm': 10.67694091796875, 'learning_rate': 4.389024669670817e-05, 'epoch': 1.22}
{'loss': 0.3214, 'grad_norm': 1.1691218614578247, 'learning_rate': 4.369930497212251e-05, 'epoch': 1.26}
{'loss': 0.3164, 'grad_norm': 17.590309143066406, 'learning_rate': 4.350836324753685e-05, 'epoch': 1.3}
{'loss': 0.3222, 'grad_norm': 10.901020050048828, 'learning_rate': 4.331742152295119e-05, 'epoch': 1.34}
{'loss': 0.3198, 'grad_norm': 1.4415346384048462, 'learning_rate': 4.312647979836554e-05, 'epoch': 1.37}
{'loss': 0.3128, 'grad_norm': 7.912034034729004, 'learning_rate': 4.293553807377988e-05, 'epoch': 1.41}
{'loss': 0.3088, 'grad_norm': 0.6350243091583252, 'learning_rate': 4.2744596349194225e-05, 'epoch': 1.45}
{'loss': 0.2991, 'grad_norm': 8.652159690856934, 'learning_rate': 4.2553654624608566e-05, 'epoch': 1.49}
{'loss': 0.3065, 'grad_norm': 5.342833518981934, 'learning_rate': 4.2362712900022915e-05, 'epoch': 1.53}
{'loss': 0.3192, 'grad_norm': 1.5920817852020264, 'learning_rate': 4.2171771175437256e-05, 'epoch': 1.57}
{'loss': 0.3219, 'grad_norm': 8.042941093444824, 'learning_rate': 4.19808294508516e-05, 'epoch': 1.6}
{'loss': 0.3043, 'grad_norm': 7.042696475982666, 'learning_rate': 4.1789887726265946e-05, 'epoch': 1.64}
{'loss': 0.3092, 'grad_norm': 11.454668998718262, 'learning_rate': 4.159894600168029e-05, 'epoch': 1.68}
{'loss': 0.3163, 'grad_norm': 8.244353294372559, 'learning_rate': 4.140800427709463e-05, 'epoch': 1.72}
{'loss': 0.3175, 'grad_norm': 2.3262381553649902, 'learning_rate': 4.121706255250898e-05, 'epoch': 1.76}
{'loss': 0.3189, 'grad_norm': 4.908867359161377, 'learning_rate': 4.102612082792332e-05, 'epoch': 1.79}
{'loss': 0.3141, 'grad_norm': 7.171950340270996, 'learning_rate': 4.083517910333766e-05, 'epoch': 1.83}
{'loss': 0.3165, 'grad_norm': 10.330987930297852, 'learning_rate': 4.064423737875201e-05, 'epoch': 1.87}
{'loss': 0.3044, 'grad_norm': 15.190910339355469, 'learning_rate': 4.045329565416635e-05, 'epoch': 1.91}
{'loss': 0.3137, 'grad_norm': 3.767911672592163, 'learning_rate': 4.026235392958069e-05, 'epoch': 1.95}
{'loss': 0.3011, 'grad_norm': 0.4239515960216522, 'learning_rate': 4.0071412204995035e-05, 'epoch': 1.99}
{'loss': 0.316, 'grad_norm': 11.946000099182129, 'learning_rate': 3.988047048040938e-05, 'epoch': 2.02}
{'loss': 0.2748, 'grad_norm': 10.278236389160156, 'learning_rate': 3.9689528755823725e-05, 'epoch': 2.06}
{'loss': 0.3062, 'grad_norm': 22.435665130615234, 'learning_rate': 3.9498587031238066e-05, 'epoch': 2.1}
{'loss': 0.2984, 'grad_norm': 7.453586578369141, 'learning_rate': 3.9307645306652415e-05, 'epoch': 2.14}
{'loss': 0.3033, 'grad_norm': 14.602716445922852, 'learning_rate': 3.9116703582066756e-05, 'epoch': 2.18}
{'loss': 0.2907, 'grad_norm': 0.3789125084877014, 'learning_rate': 3.89257618574811e-05, 'epoch': 2.21}
{'loss': 0.2977, 'grad_norm': 0.6781274676322937, 'learning_rate': 3.8734820132895446e-05, 'epoch': 2.25}
{'loss': 0.2895, 'grad_norm': 3.4225807189941406, 'learning_rate': 3.854387840830979e-05, 'epoch': 2.29}
{'loss': 0.3053, 'grad_norm': 15.407108306884766, 'learning_rate': 3.835293668372413e-05, 'epoch': 2.33}
{'loss': 0.2882, 'grad_norm': 2.92846417427063, 'learning_rate': 3.816199495913848e-05, 'epoch': 2.37}
{'loss': 0.3008, 'grad_norm': 6.330750465393066, 'learning_rate': 3.797105323455282e-05, 'epoch': 2.41}
{'loss': 0.308, 'grad_norm': 12.26751708984375, 'learning_rate': 3.778011150996716e-05, 'epoch': 2.44}
{'loss': 0.2856, 'grad_norm': 15.937216758728027, 'learning_rate': 3.75891697853815e-05, 'epoch': 2.48}
{'loss': 0.3017, 'grad_norm': 7.195240497589111, 'learning_rate': 3.739822806079585e-05, 'epoch': 2.52}
{'loss': 0.2895, 'grad_norm': 0.5617009401321411, 'learning_rate': 3.720728633621019e-05, 'epoch': 2.56}
{'loss': 0.3025, 'grad_norm': 17.842260360717773, 'learning_rate': 3.7016344611624534e-05, 'epoch': 2.6}
{'loss': 0.3004, 'grad_norm': 8.97860050201416, 'learning_rate': 3.6825402887038876e-05, 'epoch': 2.63}
{'loss': 0.3108, 'grad_norm': 12.784257888793945, 'learning_rate': 3.663446116245322e-05, 'epoch': 2.67}
{'loss': 0.3029, 'grad_norm': 25.577070236206055, 'learning_rate': 3.644351943786756e-05, 'epoch': 2.71}
{'loss': 0.2983, 'grad_norm': 0.9964712858200073, 'learning_rate': 3.625257771328191e-05, 'epoch': 2.75}
{'loss': 0.3055, 'grad_norm': 13.49460506439209, 'learning_rate': 3.606163598869625e-05, 'epoch': 2.79}
{'loss': 0.2848, 'grad_norm': 7.361420631408691, 'learning_rate': 3.587069426411059e-05, 'epoch': 2.83}
{'loss': 0.2963, 'grad_norm': 1.112704873085022, 'learning_rate': 3.567975253952494e-05, 'epoch': 2.86}
{'loss': 0.3023, 'grad_norm': 7.998281955718994, 'learning_rate': 3.548881081493928e-05, 'epoch': 2.9}
{'loss': 0.3027, 'grad_norm': 11.416357040405273, 'learning_rate': 3.529786909035362e-05, 'epoch': 2.94}
{'loss': 0.3024, 'grad_norm': 21.185508728027344, 'learning_rate': 3.510692736576797e-05, 'epoch': 2.98}
{'loss': 0.2792, 'grad_norm': 0.5628984570503235, 'learning_rate': 3.491598564118231e-05, 'epoch': 3.02}
{'loss': 0.2875, 'grad_norm': 0.3315107524394989, 'learning_rate': 3.4725043916596654e-05, 'epoch': 3.06}
{'loss': 0.2777, 'grad_norm': 1.0509530305862427, 'learning_rate': 3.4534102192010996e-05, 'epoch': 3.09}
{'loss': 0.2946, 'grad_norm': 12.508663177490234, 'learning_rate': 3.4343160467425344e-05, 'epoch': 3.13}
{'loss': 0.277, 'grad_norm': 0.3088963031768799, 'learning_rate': 3.4152218742839686e-05, 'epoch': 3.17}
{'loss': 0.286, 'grad_norm': 0.6955169439315796, 'learning_rate': 3.396127701825403e-05, 'epoch': 3.21}
{'loss': 0.273, 'grad_norm': 24.983659744262695, 'learning_rate': 3.3770335293668376e-05, 'epoch': 3.25}
{'loss': 0.2966, 'grad_norm': 15.029561996459961, 'learning_rate': 3.357939356908272e-05, 'epoch': 3.28}
{'loss': 0.2783, 'grad_norm': 13.966164588928223, 'learning_rate': 3.338845184449706e-05, 'epoch': 3.32}
{'loss': 0.2879, 'grad_norm': 0.6207005381584167, 'learning_rate': 3.319751011991141e-05, 'epoch': 3.36}
{'loss': 0.2807, 'grad_norm': 12.50955581665039, 'learning_rate': 3.300656839532575e-05, 'epoch': 3.4}
{'loss': 0.267, 'grad_norm': 9.484294891357422, 'learning_rate': 3.281562667074009e-05, 'epoch': 3.44}
{'loss': 0.276, 'grad_norm': 0.2596403658390045, 'learning_rate': 3.262468494615444e-05, 'epoch': 3.48}
{'loss': 0.2828, 'grad_norm': 13.48850154876709, 'learning_rate': 3.243374322156878e-05, 'epoch': 3.51}
{'loss': 0.2929, 'grad_norm': 0.6291555762290955, 'learning_rate': 3.224280149698312e-05, 'epoch': 3.55}
{'loss': 0.2755, 'grad_norm': 7.284631252288818, 'learning_rate': 3.2051859772397464e-05, 'epoch': 3.59}
{'loss': 0.295, 'grad_norm': 14.974239349365234, 'learning_rate': 3.186091804781181e-05, 'epoch': 3.63}
{'loss': 0.2761, 'grad_norm': 1.2842857837677002, 'learning_rate': 3.1669976323226154e-05, 'epoch': 3.67}
{'loss': 0.2795, 'grad_norm': 7.948650360107422, 'learning_rate': 3.1479034598640496e-05, 'epoch': 3.7}
{'loss': 0.2762, 'grad_norm': 18.784486770629883, 'learning_rate': 3.1288092874054844e-05, 'epoch': 3.74}
{'loss': 0.282, 'grad_norm': 0.7772231101989746, 'learning_rate': 3.1097151149469186e-05, 'epoch': 3.78}
{'loss': 0.3, 'grad_norm': 1.2413908243179321, 'learning_rate': 3.090620942488353e-05, 'epoch': 3.82}
{'loss': 0.2916, 'grad_norm': 15.944286346435547, 'learning_rate': 3.0715267700297876e-05, 'epoch': 3.86}
{'loss': 0.296, 'grad_norm': 7.264745712280273, 'learning_rate': 3.052432597571222e-05, 'epoch': 3.9}
{'loss': 0.2876, 'grad_norm': 38.19340133666992, 'learning_rate': 3.0333384251126556e-05, 'epoch': 3.93}
{'loss': 0.2784, 'grad_norm': 8.109071731567383, 'learning_rate': 3.0142442526540904e-05, 'epoch': 3.97}
{'loss': 0.2679, 'grad_norm': 7.5704474449157715, 'learning_rate': 2.9951500801955246e-05, 'epoch': 4.01}
{'loss': 0.2673, 'grad_norm': 0.6916011571884155, 'learning_rate': 2.9760559077369587e-05, 'epoch': 4.05}
{'loss': 0.2743, 'grad_norm': 9.47843074798584, 'learning_rate': 2.956961735278393e-05, 'epoch': 4.09}
{'loss': 0.2619, 'grad_norm': 11.260448455810547, 'learning_rate': 2.9378675628198277e-05, 'epoch': 4.12}
{'loss': 0.2856, 'grad_norm': 8.411959648132324, 'learning_rate': 2.918773390361262e-05, 'epoch': 4.16}
{'loss': 0.2616, 'grad_norm': 0.6121649742126465, 'learning_rate': 2.899679217902696e-05, 'epoch': 4.2}
{'loss': 0.2793, 'grad_norm': 1.1079773902893066, 'learning_rate': 2.880585045444131e-05, 'epoch': 4.24}
{'loss': 0.2749, 'grad_norm': 27.089139938354492, 'learning_rate': 2.861490872985565e-05, 'epoch': 4.28}
{'loss': 0.2972, 'grad_norm': 17.4556941986084, 'learning_rate': 2.8423967005269992e-05, 'epoch': 4.32}
{'loss': 0.2827, 'grad_norm': 2.409071922302246, 'learning_rate': 2.8233025280684337e-05, 'epoch': 4.35}
{'loss': 0.2834, 'grad_norm': 0.4873517155647278, 'learning_rate': 2.804208355609868e-05, 'epoch': 4.39}
{'loss': 0.2618, 'grad_norm': 12.188663482666016, 'learning_rate': 2.7851141831513024e-05, 'epoch': 4.43}
{'loss': 0.2671, 'grad_norm': 7.655147075653076, 'learning_rate': 2.766020010692737e-05, 'epoch': 4.47}
{'loss': 0.268, 'grad_norm': 0.4121517539024353, 'learning_rate': 2.746925838234171e-05, 'epoch': 4.51}
{'loss': 0.2767, 'grad_norm': 17.718610763549805, 'learning_rate': 2.7278316657756052e-05, 'epoch': 4.54}
{'loss': 0.2711, 'grad_norm': 0.7225168943405151, 'learning_rate': 2.7087374933170394e-05, 'epoch': 4.58}
{'loss': 0.2693, 'grad_norm': 15.518265724182129, 'learning_rate': 2.6896433208584742e-05, 'epoch': 4.62}
{'loss': 0.2829, 'grad_norm': 28.41982078552246, 'learning_rate': 2.6705491483999084e-05, 'epoch': 4.66}
{'loss': 0.262, 'grad_norm': 25.81138038635254, 'learning_rate': 2.6514549759413426e-05, 'epoch': 4.7}
{'loss': 0.2671, 'grad_norm': 0.47597429156303406, 'learning_rate': 2.6323608034827774e-05, 'epoch': 4.74}
{'loss': 0.2552, 'grad_norm': 0.3632349669933319, 'learning_rate': 2.6132666310242116e-05, 'epoch': 4.77}
{'loss': 0.2585, 'grad_norm': 0.6447281241416931, 'learning_rate': 2.5941724585656457e-05, 'epoch': 4.81}
{'loss': 0.2805, 'grad_norm': 0.5180572867393494, 'learning_rate': 2.5750782861070806e-05, 'epoch': 4.85}
{'loss': 0.2556, 'grad_norm': 26.37303924560547, 'learning_rate': 2.5559841136485147e-05, 'epoch': 4.89}
{'loss': 0.2662, 'grad_norm': 14.301092147827148, 'learning_rate': 2.536889941189949e-05, 'epoch': 4.93}
{'loss': 0.2687, 'grad_norm': 3.4961001873016357, 'learning_rate': 2.5177957687313837e-05, 'epoch': 4.96}
{'loss': 0.2618, 'grad_norm': 9.513411521911621, 'learning_rate': 2.498701596272818e-05, 'epoch': 5.0}
{'loss': 0.261, 'grad_norm': 0.4332487881183624, 'learning_rate': 2.479607423814252e-05, 'epoch': 5.04}
{'loss': 0.2461, 'grad_norm': 1.2555571794509888, 'learning_rate': 2.4605132513556866e-05, 'epoch': 5.08}
{'loss': 0.2784, 'grad_norm': 0.7096536755561829, 'learning_rate': 2.4414190788971207e-05, 'epoch': 5.12}
{'loss': 0.2626, 'grad_norm': 15.398632049560547, 'learning_rate': 2.422324906438555e-05, 'epoch': 5.16}
{'loss': 0.2543, 'grad_norm': 33.75374984741211, 'learning_rate': 2.4032307339799894e-05, 'epoch': 5.19}
{'loss': 0.2737, 'grad_norm': 24.489904403686523, 'learning_rate': 2.3841365615214235e-05, 'epoch': 5.23}
{'loss': 0.2744, 'grad_norm': 15.5056791305542, 'learning_rate': 2.365042389062858e-05, 'epoch': 5.27}
{'loss': 0.2599, 'grad_norm': 0.9721960425376892, 'learning_rate': 2.3459482166042925e-05, 'epoch': 5.31}
{'loss': 0.2588, 'grad_norm': 40.99592208862305, 'learning_rate': 2.3268540441457267e-05, 'epoch': 5.35}
{'loss': 0.2685, 'grad_norm': 0.20367437601089478, 'learning_rate': 2.3077598716871612e-05, 'epoch': 5.38}
{'loss': 0.269, 'grad_norm': 19.80362319946289, 'learning_rate': 2.2886656992285954e-05, 'epoch': 5.42}
{'loss': 0.2502, 'grad_norm': 1.1278958320617676, 'learning_rate': 2.26957152677003e-05, 'epoch': 5.46}
{'loss': 0.2446, 'grad_norm': 12.88783073425293, 'learning_rate': 2.2504773543114644e-05, 'epoch': 5.5}
{'loss': 0.2579, 'grad_norm': 10.993349075317383, 'learning_rate': 2.2313831818528985e-05, 'epoch': 5.54}
{'loss': 0.2479, 'grad_norm': 19.62842559814453, 'learning_rate': 2.212289009394333e-05, 'epoch': 5.58}
{'loss': 0.2694, 'grad_norm': 12.47747802734375, 'learning_rate': 2.1931948369357675e-05, 'epoch': 5.61}
{'loss': 0.2651, 'grad_norm': 0.35605719685554504, 'learning_rate': 2.1741006644772017e-05, 'epoch': 5.65}
{'loss': 0.2655, 'grad_norm': 0.8397257328033447, 'learning_rate': 2.1550064920186362e-05, 'epoch': 5.69}
{'loss': 0.2389, 'grad_norm': 42.065460205078125, 'learning_rate': 2.1359123195600704e-05, 'epoch': 5.73}
{'loss': 0.267, 'grad_norm': 21.64744758605957, 'learning_rate': 2.116818147101505e-05, 'epoch': 5.77}
{'loss': 0.2807, 'grad_norm': 0.7971689105033875, 'learning_rate': 2.097723974642939e-05, 'epoch': 5.8}
{'loss': 0.2629, 'grad_norm': 13.163165092468262, 'learning_rate': 2.0786298021843735e-05, 'epoch': 5.84}
{'loss': 0.2529, 'grad_norm': 8.748577117919922, 'learning_rate': 2.0595356297258077e-05, 'epoch': 5.88}
{'loss': 0.2733, 'grad_norm': 19.036611557006836, 'learning_rate': 2.040441457267242e-05, 'epoch': 5.92}
{'loss': 0.2691, 'grad_norm': 11.571429252624512, 'learning_rate': 2.0213472848086764e-05, 'epoch': 5.96}
{'loss': 0.2373, 'grad_norm': 7.542545318603516, 'learning_rate': 2.002253112350111e-05, 'epoch': 6.0}
{'loss': 0.2587, 'grad_norm': 17.84356117248535, 'learning_rate': 1.983158939891545e-05, 'epoch': 6.03}
{'loss': 0.2532, 'grad_norm': 0.10502627491950989, 'learning_rate': 1.9640647674329795e-05, 'epoch': 6.07}
{'loss': 0.257, 'grad_norm': 31.816417694091797, 'learning_rate': 1.944970594974414e-05, 'epoch': 6.11}
{'loss': 0.2643, 'grad_norm': 0.6845220327377319, 'learning_rate': 1.9258764225158482e-05, 'epoch': 6.15}
{'loss': 0.2592, 'grad_norm': 0.3453670144081116, 'learning_rate': 1.9067822500572827e-05, 'epoch': 6.19}
{'loss': 0.2331, 'grad_norm': 1.2410495281219482, 'learning_rate': 1.887688077598717e-05, 'epoch': 6.22}
{'loss': 0.246, 'grad_norm': 0.30783626437187195, 'learning_rate': 1.8685939051401514e-05, 'epoch': 6.26}
{'loss': 0.2556, 'grad_norm': 14.965741157531738, 'learning_rate': 1.849499732681586e-05, 'epoch': 6.3}
{'loss': 0.2738, 'grad_norm': 12.77785587310791, 'learning_rate': 1.83040556022302e-05, 'epoch': 6.34}
{'loss': 0.2513, 'grad_norm': 11.223615646362305, 'learning_rate': 1.8113113877644545e-05, 'epoch': 6.38}
{'loss': 0.2507, 'grad_norm': 9.92819881439209, 'learning_rate': 1.7922172153058887e-05, 'epoch': 6.42}
{'loss': 0.2518, 'grad_norm': 0.7610211968421936, 'learning_rate': 1.7731230428473232e-05, 'epoch': 6.45}
{'loss': 0.2449, 'grad_norm': 0.5058364272117615, 'learning_rate': 1.7540288703887577e-05, 'epoch': 6.49}
{'loss': 0.25, 'grad_norm': 1.2486214637756348, 'learning_rate': 1.734934697930192e-05, 'epoch': 6.53}
{'loss': 0.2633, 'grad_norm': 0.8370065689086914, 'learning_rate': 1.715840525471626e-05, 'epoch': 6.57}
{'loss': 0.2475, 'grad_norm': 39.566036224365234, 'learning_rate': 1.6967463530130605e-05, 'epoch': 6.61}
{'loss': 0.2573, 'grad_norm': 0.3556690216064453, 'learning_rate': 1.6776521805544947e-05, 'epoch': 6.64}
{'loss': 0.2563, 'grad_norm': 21.98523712158203, 'learning_rate': 1.6585580080959292e-05, 'epoch': 6.68}
{'loss': 0.2556, 'grad_norm': 23.627582550048828, 'learning_rate': 1.6394638356373633e-05, 'epoch': 6.72}
{'loss': 0.2693, 'grad_norm': 29.03441619873047, 'learning_rate': 1.620369663178798e-05, 'epoch': 6.76}
{'loss': 0.2399, 'grad_norm': 8.784932136535645, 'learning_rate': 1.6012754907202323e-05, 'epoch': 6.8}
{'loss': 0.2628, 'grad_norm': 0.20375974476337433, 'learning_rate': 1.5821813182616665e-05, 'epoch': 6.84}
{'loss': 0.2777, 'grad_norm': 1.9944132566452026, 'learning_rate': 1.563087145803101e-05, 'epoch': 6.87}
{'loss': 0.2506, 'grad_norm': 11.64150619506836, 'learning_rate': 1.543992973344535e-05, 'epoch': 6.91}
{'loss': 0.2599, 'grad_norm': 15.388120651245117, 'learning_rate': 1.5248988008859697e-05, 'epoch': 6.95}
{'loss': 0.245, 'grad_norm': 19.221769332885742, 'learning_rate': 1.5058046284274042e-05, 'epoch': 6.99}
{'loss': 0.2566, 'grad_norm': 0.2202683389186859, 'learning_rate': 1.4867104559688383e-05, 'epoch': 7.03}
{'loss': 0.2462, 'grad_norm': 0.35131531953811646, 'learning_rate': 1.4676162835102728e-05, 'epoch': 7.06}
{'loss': 0.2432, 'grad_norm': 0.3524852991104126, 'learning_rate': 1.4485221110517072e-05, 'epoch': 7.1}
{'loss': 0.2358, 'grad_norm': 4.706295967102051, 'learning_rate': 1.4294279385931413e-05, 'epoch': 7.14}
{'loss': 0.2304, 'grad_norm': 40.42244338989258, 'learning_rate': 1.4103337661345758e-05, 'epoch': 7.18}
{'loss': 0.259, 'grad_norm': 0.3660713732242584, 'learning_rate': 1.39123959367601e-05, 'epoch': 7.22}
{'loss': 0.2456, 'grad_norm': 23.617265701293945, 'learning_rate': 1.3721454212174445e-05, 'epoch': 7.26}
{'loss': 0.2612, 'grad_norm': 22.1768741607666, 'learning_rate': 1.353051248758879e-05, 'epoch': 7.29}
{'loss': 0.2587, 'grad_norm': 0.09289941936731339, 'learning_rate': 1.3339570763003132e-05, 'epoch': 7.33}
{'loss': 0.2324, 'grad_norm': 7.024740695953369, 'learning_rate': 1.3148629038417477e-05, 'epoch': 7.37}
{'loss': 0.2454, 'grad_norm': 13.771337509155273, 'learning_rate': 1.2957687313831818e-05, 'epoch': 7.41}
{'loss': 0.2507, 'grad_norm': 0.15899640321731567, 'learning_rate': 1.2766745589246163e-05, 'epoch': 7.45}
{'loss': 0.227, 'grad_norm': 3.0604777336120605, 'learning_rate': 1.2575803864660507e-05, 'epoch': 7.48}
{'loss': 0.2451, 'grad_norm': 2.056243658065796, 'learning_rate': 1.238486214007485e-05, 'epoch': 7.52}
{'loss': 0.2581, 'grad_norm': 0.22731666266918182, 'learning_rate': 1.2193920415489193e-05, 'epoch': 7.56}
{'loss': 0.2361, 'grad_norm': 0.868510365486145, 'learning_rate': 1.2002978690903536e-05, 'epoch': 7.6}
{'loss': 0.2391, 'grad_norm': 0.5302019715309143, 'learning_rate': 1.181203696631788e-05, 'epoch': 7.64}
{'loss': 0.2644, 'grad_norm': 7.812943935394287, 'learning_rate': 1.1621095241732225e-05, 'epoch': 7.68}
{'loss': 0.2335, 'grad_norm': 0.20553578436374664, 'learning_rate': 1.1430153517146568e-05, 'epoch': 7.71}
{'loss': 0.2592, 'grad_norm': 1.2908098697662354, 'learning_rate': 1.1239211792560911e-05, 'epoch': 7.75}
{'loss': 0.2588, 'grad_norm': 1.6607424020767212, 'learning_rate': 1.1048270067975255e-05, 'epoch': 7.79}
{'loss': 0.2634, 'grad_norm': 0.11955161392688751, 'learning_rate': 1.0857328343389598e-05, 'epoch': 7.83}
{'loss': 0.2349, 'grad_norm': 9.294825553894043, 'learning_rate': 1.0666386618803941e-05, 'epoch': 7.87}
{'loss': 0.2372, 'grad_norm': 36.57953643798828, 'learning_rate': 1.0475444894218285e-05, 'epoch': 7.9}
{'loss': 0.2468, 'grad_norm': 13.029006004333496, 'learning_rate': 1.0284503169632628e-05, 'epoch': 7.94}
{'loss': 0.2388, 'grad_norm': 0.68025803565979, 'learning_rate': 1.0093561445046971e-05, 'epoch': 7.98}
{'loss': 0.2618, 'grad_norm': 8.688076972961426, 'learning_rate': 9.902619720461316e-06, 'epoch': 8.02}
{'loss': 0.2259, 'grad_norm': 19.132665634155273, 'learning_rate': 9.71167799587566e-06, 'epoch': 8.06}
{'loss': 0.233, 'grad_norm': 6.991322994232178, 'learning_rate': 9.520736271290003e-06, 'epoch': 8.1}
{'loss': 0.2368, 'grad_norm': 25.402162551879883, 'learning_rate': 9.329794546704346e-06, 'epoch': 8.13}
{'loss': 0.2403, 'grad_norm': 48.061485290527344, 'learning_rate': 9.13885282211869e-06, 'epoch': 8.17}
{'loss': 0.2306, 'grad_norm': 20.120817184448242, 'learning_rate': 8.947911097533033e-06, 'epoch': 8.21}
{'loss': 0.2382, 'grad_norm': 0.19249387085437775, 'learning_rate': 8.756969372947376e-06, 'epoch': 8.25}
{'loss': 0.2384, 'grad_norm': 0.7380242347717285, 'learning_rate': 8.56602764836172e-06, 'epoch': 8.29}
{'loss': 0.2316, 'grad_norm': 0.4861539602279663, 'learning_rate': 8.375085923776065e-06, 'epoch': 8.33}
{'loss': 0.2526, 'grad_norm': 0.6669623255729675, 'learning_rate': 8.184144199190408e-06, 'epoch': 8.36}
{'loss': 0.2315, 'grad_norm': 20.2558650970459, 'learning_rate': 7.993202474604751e-06, 'epoch': 8.4}
{'loss': 0.2302, 'grad_norm': 8.348401069641113, 'learning_rate': 7.802260750019095e-06, 'epoch': 8.44}
{'loss': 0.2337, 'grad_norm': 0.538433849811554, 'learning_rate': 7.611319025433437e-06, 'epoch': 8.48}
{'loss': 0.2456, 'grad_norm': 0.47801923751831055, 'learning_rate': 7.420377300847782e-06, 'epoch': 8.52}
{'loss': 0.2581, 'grad_norm': 46.01856231689453, 'learning_rate': 7.2294355762621254e-06, 'epoch': 8.55}
{'loss': 0.2664, 'grad_norm': 0.32954898476600647, 'learning_rate': 7.038493851676469e-06, 'epoch': 8.59}
{'loss': 0.2278, 'grad_norm': 0.14442265033721924, 'learning_rate': 6.847552127090811e-06, 'epoch': 8.63}
{'loss': 0.2467, 'grad_norm': 54.63246154785156, 'learning_rate': 6.656610402505156e-06, 'epoch': 8.67}
{'loss': 0.2405, 'grad_norm': 15.926807403564453, 'learning_rate': 6.4656686779194996e-06, 'epoch': 8.71}
{'loss': 0.2652, 'grad_norm': 8.922300338745117, 'learning_rate': 6.274726953333843e-06, 'epoch': 8.75}
{'loss': 0.2438, 'grad_norm': 23.752300262451172, 'learning_rate': 6.083785228748186e-06, 'epoch': 8.78}
{'loss': 0.228, 'grad_norm': 1.3295265436172485, 'learning_rate': 5.8928435041625295e-06, 'epoch': 8.82}
{'loss': 0.2003, 'grad_norm': 60.28753662109375, 'learning_rate': 5.701901779576874e-06, 'epoch': 8.86}
{'loss': 0.2555, 'grad_norm': 4.515321731567383, 'learning_rate': 5.510960054991217e-06, 'epoch': 8.9}
{'loss': 0.2332, 'grad_norm': 18.682785034179688, 'learning_rate': 5.320018330405561e-06, 'epoch': 8.94}
{'loss': 0.2485, 'grad_norm': 0.2555617094039917, 'learning_rate': 5.129076605819904e-06, 'epoch': 8.97}
{'loss': 0.235, 'grad_norm': 30.2911434173584, 'learning_rate': 4.938134881234248e-06, 'epoch': 9.01}
{'loss': 0.2136, 'grad_norm': 0.23315677046775818, 'learning_rate': 4.747193156648591e-06, 'epoch': 9.05}
{'loss': 0.2454, 'grad_norm': 15.95649528503418, 'learning_rate': 4.5562514320629344e-06, 'epoch': 9.09}
{'loss': 0.2323, 'grad_norm': 29.824710845947266, 'learning_rate': 4.365309707477279e-06, 'epoch': 9.13}
{'loss': 0.2356, 'grad_norm': 0.43394342064857483, 'learning_rate': 4.174367982891621e-06, 'epoch': 9.17}
{'loss': 0.2145, 'grad_norm': 35.335636138916016, 'learning_rate': 3.983426258305965e-06, 'epoch': 9.2}
{'loss': 0.2393, 'grad_norm': 30.485492706298828, 'learning_rate': 3.7924845337203086e-06, 'epoch': 9.24}
{'loss': 0.2437, 'grad_norm': 0.1884792298078537, 'learning_rate': 3.6015428091346523e-06, 'epoch': 9.28}
{'loss': 0.2355, 'grad_norm': 0.21030384302139282, 'learning_rate': 3.4106010845489956e-06, 'epoch': 9.32}
{'loss': 0.2494, 'grad_norm': 0.17684732377529144, 'learning_rate': 3.2196593599633394e-06, 'epoch': 9.36}
{'loss': 0.2446, 'grad_norm': 10.550362586975098, 'learning_rate': 3.0287176353776827e-06, 'epoch': 9.39}
{'loss': 0.2635, 'grad_norm': 4.964612007141113, 'learning_rate': 2.8377759107920264e-06, 'epoch': 9.43}
{'loss': 0.2366, 'grad_norm': 13.844147682189941, 'learning_rate': 2.6468341862063698e-06, 'epoch': 9.47}
{'loss': 0.2396, 'grad_norm': 0.8257298469543457, 'learning_rate': 2.4558924616207135e-06, 'epoch': 9.51}
{'loss': 0.2421, 'grad_norm': 0.21532142162322998, 'learning_rate': 2.264950737035057e-06, 'epoch': 9.55}
{'loss': 0.2364, 'grad_norm': 0.40758001804351807, 'learning_rate': 2.0740090124494006e-06, 'epoch': 9.59}
{'loss': 0.2362, 'grad_norm': 0.6087943315505981, 'learning_rate': 1.883067287863744e-06, 'epoch': 9.62}
{'loss': 0.24, 'grad_norm': 0.35555383563041687, 'learning_rate': 1.6921255632780876e-06, 'epoch': 9.66}
{'loss': 0.2225, 'grad_norm': 37.61552047729492, 'learning_rate': 1.501183838692431e-06, 'epoch': 9.7}
{'loss': 0.2294, 'grad_norm': 8.031669616699219, 'learning_rate': 1.3102421141067747e-06, 'epoch': 9.74}
{'loss': 0.2231, 'grad_norm': 7.946259021759033, 'learning_rate': 1.1193003895211182e-06, 'epoch': 9.78}
{'loss': 0.221, 'grad_norm': 13.569364547729492, 'learning_rate': 9.283586649354618e-07, 'epoch': 9.81}
{'loss': 0.2834, 'grad_norm': 0.3404081165790558, 'learning_rate': 7.374169403498053e-07, 'epoch': 9.85}
{'loss': 0.2287, 'grad_norm': 12.903321266174316, 'learning_rate': 5.464752157641488e-07, 'epoch': 9.89}
{'loss': 0.2465, 'grad_norm': 16.07479476928711, 'learning_rate': 3.5553349117849236e-07, 'epoch': 9.93}
{'loss': 0.2206, 'grad_norm': 8.59829330444336, 'learning_rate': 1.6459176659283587e-07, 'epoch': 9.97}
{'train_runtime': 19937.1482, 'train_samples_per_second': 52.537, 'train_steps_per_second': 6.567, 'train_loss': 0.27781669834049266, 'epoch': 10.0}
***** train metrics *****
  epoch                    =        10.0
  total_flos               = 227717064GF
  train_loss               =      0.2778
  train_runtime            =  5:32:17.14
  train_samples            =      104743
  train_samples_per_second =      52.537
  train_steps_per_second   =       6.567
08/06/2025 14:45:19 - INFO - __main__ - *** Evaluate ***
***** eval metrics *****
  epoch                   =       10.0
  eval_accuracy           =     0.9198
  eval_loss               =     0.3094
  eval_runtime            = 0:00:46.89
  eval_samples            =       5463
  eval_samples_per_second =    116.497
  eval_steps_per_second   =     14.565
08/06/2025 14:46:22 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
08/06/2025 14:46:22 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_revision=None,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
liger_kernel_config=None,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_final/BERT_large/QNLI/InA10/runs/Aug06_14-46-21_n29,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=10.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=output_final/BERT_large/QNLI/InA10/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=output_final/BERT_large/QNLI/InA10/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
08/06/2025 14:46:24 - INFO - datasets.builder - Found cached dataset glue (/home/kangchen/.cache/huggingface/datasets/nyu-mll___glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
The task type of PEFT is not proper!
LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='bert-large-cased', revision=None, task_type=<TaskType.SEQ_CLS: 'SEQ_CLS'>, inference_mode=False, r=4, target_modules={'key', 'query', 'value'}, lora_inhibition=0.1, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
###############
PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): BertForSequenceClassification(
      (bert): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 1024, padding_idx=0)
          (position_embeddings): Embedding(512, 1024)
          (token_type_embeddings): Embedding(2, 1024)
          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0-23): 24 x BertLayer(
              (attention): BertAttention(
                (self): BertSdpaSelfAttention(
                  (query): lora.Linear(
                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=4, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=4, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (key): lora.Linear(
                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=4, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=4, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (value): lora.Linear(
                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=4, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=4, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
          (activation): Tanh()
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (classifier): ModulesToSaveWrapper(
        (original_module): Linear(in_features=1024, out_features=2, bias=True)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=1024, out_features=2, bias=True)
        )
      )
    )
  )
)
08/06/2025 14:46:27 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/kangchen/.cache/huggingface/datasets/nyu-mll___glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-857a5657ab1232c9_*_of_00001.arrow
08/06/2025 14:46:27 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/kangchen/.cache/huggingface/datasets/nyu-mll___glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-32484430d63833d1.arrow
08/06/2025 14:46:28 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/kangchen/.cache/huggingface/datasets/nyu-mll___glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-bc9d8598d5afffcf_*_of_00001.arrow
08/06/2025 14:46:29 - INFO - __main__ - Class distribution in train set:
08/06/2025 14:46:29 - INFO - __main__ -   Label 1: 52366 (49.99%)
08/06/2025 14:46:29 - INFO - __main__ -   Label 0: 52377 (50.01%)
08/06/2025 14:46:29 - INFO - __main__ - Class distribution in validation set:
08/06/2025 14:46:29 - INFO - __main__ -   Label 0: 2702 (49.46%)
08/06/2025 14:46:29 - INFO - __main__ -   Label 1: 2761 (50.54%)
08/06/2025 14:46:29 - INFO - __main__ - Class distribution in test set:
08/06/2025 14:46:29 - INFO - __main__ -   Label -1: 5463 (100.00%)
08/06/2025 14:46:29 - INFO - __main__ - Sample 83810 of the training set: {'question': 'What tactics of the PVA were US and ROK troops not prepared to handle?', 'sentence': "On 27 November at the Korean eastern front, a U.S. 7th Infantry Division Regimental Combat Team (3,000 soldiers) and the U.S. 1st Marine Division (12,000–15,000 marines) were unprepared for the PVA 9th Army Group's three-pronged encirclement tactics at the Battle of Chosin Reservoir, but they managed to escape under Air Force and X Corps support fire—albeit with some 15,000 collective casualties.", 'label': 0, 'idx': 83810, 'input_ids': [101, 1327, 10524, 1104, 1103, 153, 12152, 1127, 1646, 1105, 155, 22027, 2830, 1136, 4029, 1106, 4282, 136, 102, 1212, 1765, 1379, 1120, 1103, 3947, 2638, 1524, 117, 170, 158, 119, 156, 119, 4766, 4155, 1784, 3391, 1348, 10314, 2649, 113, 124, 117, 1288, 2803, 114, 1105, 1103, 158, 119, 156, 119, 2198, 4620, 1784, 113, 1367, 117, 1288, 782, 1405, 117, 1288, 5243, 1116, 114, 1127, 8362, 1643, 1874, 17482, 1174, 1111, 1103, 153, 12152, 5612, 1740, 1990, 112, 188, 1210, 118, 5250, 18288, 4035, 6617, 24490, 1880, 10524, 1120, 1103, 2651, 1104, 22964, 10606, 15220, 117, 1133, 1152, 2374, 1106, 3359, 1223, 1806, 2300, 1105, 161, 3158, 1619, 1783, 783, 12456, 1114, 1199, 1405, 117, 1288, 7764, 8487, 119, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}.
08/06/2025 14:46:29 - INFO - __main__ - Sample 14592 of the training set: {'question': "How did the Egyptian people feel about Nasser's response to the attack?", 'sentence': 'Nasser did not feel that the Egyptian Army was ready for a confrontation and did not retaliate militarily.', 'label': 1, 'idx': 14592, 'input_ids': [101, 1731, 1225, 1103, 6210, 1234, 1631, 1164, 11896, 14607, 112, 188, 2593, 1106, 1103, 2035, 136, 102, 11896, 14607, 1225, 1136, 1631, 1115, 1103, 6210, 1740, 1108, 2407, 1111, 170, 14002, 1105, 1225, 1136, 1231, 6163, 15045, 1940, 12888, 18206, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
08/06/2025 14:46:29 - INFO - __main__ - Sample 3278 of the training set: {'question': 'What kind of aircraft are capable of taking off vertically?', 'sentence': 'Although STOVL aircraft are capable of taking off vertically from a spot on the deck, using the ramp and a running start is far more fuel efficient and permits a heavier launch weight.', 'label': 0, 'idx': 3278, 'input_ids': [101, 1327, 1912, 1104, 2163, 1132, 4451, 1104, 1781, 1228, 22026, 136, 102, 1966, 23676, 2346, 2559, 2162, 2163, 1132, 4451, 1104, 1781, 1228, 22026, 1121, 170, 3205, 1113, 1103, 5579, 117, 1606, 1103, 14207, 1105, 170, 1919, 1838, 1110, 1677, 1167, 4251, 7856, 1105, 15267, 170, 12163, 4286, 2841, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
08/06/2025 14:46:30 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.6255, 'grad_norm': 11.479917526245117, 'learning_rate': 4.980944015886352e-05, 'epoch': 0.04}
{'loss': 0.47, 'grad_norm': 4.063249111175537, 'learning_rate': 4.961849843427786e-05, 'epoch': 0.08}
{'loss': 0.4186, 'grad_norm': 6.076443195343018, 'learning_rate': 4.94275567096922e-05, 'epoch': 0.11}
{'loss': 0.4069, 'grad_norm': 8.877880096435547, 'learning_rate': 4.923661498510655e-05, 'epoch': 0.15}
{'loss': 0.389, 'grad_norm': 8.737967491149902, 'learning_rate': 4.904567326052089e-05, 'epoch': 0.19}
{'loss': 0.3865, 'grad_norm': 14.011225700378418, 'learning_rate': 4.885473153593523e-05, 'epoch': 0.23}
{'loss': 0.3807, 'grad_norm': 5.005514621734619, 'learning_rate': 4.866378981134958e-05, 'epoch': 0.27}
{'loss': 0.3813, 'grad_norm': 8.090361595153809, 'learning_rate': 4.847284808676392e-05, 'epoch': 0.31}
{'loss': 0.3735, 'grad_norm': 6.055410385131836, 'learning_rate': 4.828190636217826e-05, 'epoch': 0.34}
{'loss': 0.3586, 'grad_norm': 7.088924884796143, 'learning_rate': 4.809096463759261e-05, 'epoch': 0.38}
{'loss': 0.3656, 'grad_norm': 2.07759952545166, 'learning_rate': 4.790002291300695e-05, 'epoch': 0.42}
{'loss': 0.3693, 'grad_norm': 3.7118706703186035, 'learning_rate': 4.7709081188421295e-05, 'epoch': 0.46}
{'loss': 0.3639, 'grad_norm': 5.794365406036377, 'learning_rate': 4.7518139463835637e-05, 'epoch': 0.5}
{'loss': 0.3625, 'grad_norm': 2.663973093032837, 'learning_rate': 4.7327197739249985e-05, 'epoch': 0.53}
{'loss': 0.3451, 'grad_norm': 18.439319610595703, 'learning_rate': 4.713625601466433e-05, 'epoch': 0.57}
{'loss': 0.3604, 'grad_norm': 12.853132247924805, 'learning_rate': 4.694531429007867e-05, 'epoch': 0.61}
{'loss': 0.3608, 'grad_norm': 6.535250186920166, 'learning_rate': 4.675437256549302e-05, 'epoch': 0.65}
{'loss': 0.3411, 'grad_norm': 9.07868766784668, 'learning_rate': 4.656343084090736e-05, 'epoch': 0.69}
{'loss': 0.3299, 'grad_norm': 5.583614349365234, 'learning_rate': 4.63724891163217e-05, 'epoch': 0.73}
{'loss': 0.3443, 'grad_norm': 6.036375522613525, 'learning_rate': 4.618154739173605e-05, 'epoch': 0.76}
{'loss': 0.3242, 'grad_norm': 2.825648069381714, 'learning_rate': 4.599060566715039e-05, 'epoch': 0.8}
{'loss': 0.3448, 'grad_norm': 7.05299711227417, 'learning_rate': 4.579966394256473e-05, 'epoch': 0.84}
{'loss': 0.3361, 'grad_norm': 8.637062072753906, 'learning_rate': 4.560872221797908e-05, 'epoch': 0.88}
{'loss': 0.3281, 'grad_norm': 0.5047008395195007, 'learning_rate': 4.541778049339342e-05, 'epoch': 0.92}
{'loss': 0.3476, 'grad_norm': 5.502310752868652, 'learning_rate': 4.522683876880776e-05, 'epoch': 0.95}
{'loss': 0.3273, 'grad_norm': 0.44944119453430176, 'learning_rate': 4.5035897044222105e-05, 'epoch': 0.99}
{'loss': 0.3263, 'grad_norm': 9.77472972869873, 'learning_rate': 4.484495531963645e-05, 'epoch': 1.03}
{'loss': 0.3313, 'grad_norm': 12.123292922973633, 'learning_rate': 4.4654013595050795e-05, 'epoch': 1.07}
{'loss': 0.3222, 'grad_norm': 3.7246716022491455, 'learning_rate': 4.4463071870465137e-05, 'epoch': 1.11}
{'loss': 0.332, 'grad_norm': 5.716081619262695, 'learning_rate': 4.427213014587948e-05, 'epoch': 1.15}
{'loss': 0.3263, 'grad_norm': 12.554161071777344, 'learning_rate': 4.408118842129382e-05, 'epoch': 1.18}
{'loss': 0.3085, 'grad_norm': 2.247711181640625, 'learning_rate': 4.389024669670817e-05, 'epoch': 1.22}
{'loss': 0.3117, 'grad_norm': 1.0816915035247803, 'learning_rate': 4.369930497212251e-05, 'epoch': 1.26}
{'loss': 0.3224, 'grad_norm': 14.901175498962402, 'learning_rate': 4.350836324753685e-05, 'epoch': 1.3}
{'loss': 0.3226, 'grad_norm': 3.456470251083374, 'learning_rate': 4.331742152295119e-05, 'epoch': 1.34}
{'loss': 0.3218, 'grad_norm': 0.8238729238510132, 'learning_rate': 4.312647979836554e-05, 'epoch': 1.37}
{'loss': 0.3244, 'grad_norm': 5.911967754364014, 'learning_rate': 4.293553807377988e-05, 'epoch': 1.41}
{'loss': 0.3032, 'grad_norm': 0.5300803780555725, 'learning_rate': 4.2744596349194225e-05, 'epoch': 1.45}
{'loss': 0.3091, 'grad_norm': 6.139369487762451, 'learning_rate': 4.2553654624608566e-05, 'epoch': 1.49}
{'loss': 0.3087, 'grad_norm': 6.21149206161499, 'learning_rate': 4.2362712900022915e-05, 'epoch': 1.53}
{'loss': 0.3215, 'grad_norm': 6.8691511154174805, 'learning_rate': 4.2171771175437256e-05, 'epoch': 1.57}
{'loss': 0.3322, 'grad_norm': 7.8403143882751465, 'learning_rate': 4.19808294508516e-05, 'epoch': 1.6}
{'loss': 0.315, 'grad_norm': 12.472674369812012, 'learning_rate': 4.1789887726265946e-05, 'epoch': 1.64}
{'loss': 0.3114, 'grad_norm': 10.866687774658203, 'learning_rate': 4.159894600168029e-05, 'epoch': 1.68}
{'loss': 0.3208, 'grad_norm': 10.108475685119629, 'learning_rate': 4.140800427709463e-05, 'epoch': 1.72}
{'loss': 0.3096, 'grad_norm': 2.7234880924224854, 'learning_rate': 4.121706255250898e-05, 'epoch': 1.76}
{'loss': 0.3232, 'grad_norm': 1.8477228879928589, 'learning_rate': 4.102612082792332e-05, 'epoch': 1.79}
{'loss': 0.3134, 'grad_norm': 10.494180679321289, 'learning_rate': 4.083517910333766e-05, 'epoch': 1.83}
{'loss': 0.3125, 'grad_norm': 7.375835418701172, 'learning_rate': 4.064423737875201e-05, 'epoch': 1.87}
{'loss': 0.3037, 'grad_norm': 22.747434616088867, 'learning_rate': 4.045329565416635e-05, 'epoch': 1.91}
{'loss': 0.3082, 'grad_norm': 4.493505954742432, 'learning_rate': 4.026235392958069e-05, 'epoch': 1.95}
{'loss': 0.3036, 'grad_norm': 6.316943168640137, 'learning_rate': 4.0071412204995035e-05, 'epoch': 1.99}
{'loss': 0.3178, 'grad_norm': 10.739480972290039, 'learning_rate': 3.988047048040938e-05, 'epoch': 2.02}
{'loss': 0.2839, 'grad_norm': 17.740543365478516, 'learning_rate': 3.9689528755823725e-05, 'epoch': 2.06}
{'loss': 0.3034, 'grad_norm': 15.855250358581543, 'learning_rate': 3.9498587031238066e-05, 'epoch': 2.1}
{'loss': 0.2962, 'grad_norm': 6.197862148284912, 'learning_rate': 3.9307645306652415e-05, 'epoch': 2.14}
{'loss': 0.3096, 'grad_norm': 17.151241302490234, 'learning_rate': 3.9116703582066756e-05, 'epoch': 2.18}
{'loss': 0.291, 'grad_norm': 1.1142467260360718, 'learning_rate': 3.89257618574811e-05, 'epoch': 2.21}
{'loss': 0.2926, 'grad_norm': 2.7884130477905273, 'learning_rate': 3.8734820132895446e-05, 'epoch': 2.25}
{'loss': 0.2968, 'grad_norm': 10.039198875427246, 'learning_rate': 3.854387840830979e-05, 'epoch': 2.29}
{'loss': 0.2959, 'grad_norm': 20.058338165283203, 'learning_rate': 3.835293668372413e-05, 'epoch': 2.33}
{'loss': 0.2916, 'grad_norm': 1.7427171468734741, 'learning_rate': 3.816199495913848e-05, 'epoch': 2.37}
{'loss': 0.3074, 'grad_norm': 4.682043075561523, 'learning_rate': 3.797105323455282e-05, 'epoch': 2.41}
{'loss': 0.3127, 'grad_norm': 10.980270385742188, 'learning_rate': 3.778011150996716e-05, 'epoch': 2.44}
{'loss': 0.2847, 'grad_norm': 6.444638729095459, 'learning_rate': 3.75891697853815e-05, 'epoch': 2.48}
{'loss': 0.2987, 'grad_norm': 1.4804614782333374, 'learning_rate': 3.739822806079585e-05, 'epoch': 2.52}
{'loss': 0.2936, 'grad_norm': 0.42072612047195435, 'learning_rate': 3.720728633621019e-05, 'epoch': 2.56}
{'loss': 0.2972, 'grad_norm': 12.760251998901367, 'learning_rate': 3.7016344611624534e-05, 'epoch': 2.6}
{'loss': 0.3051, 'grad_norm': 17.141849517822266, 'learning_rate': 3.6825402887038876e-05, 'epoch': 2.63}
{'loss': 0.2991, 'grad_norm': 7.430707931518555, 'learning_rate': 3.663446116245322e-05, 'epoch': 2.67}
{'loss': 0.299, 'grad_norm': 11.95800495147705, 'learning_rate': 3.644351943786756e-05, 'epoch': 2.71}
{'loss': 0.298, 'grad_norm': 19.31920051574707, 'learning_rate': 3.625257771328191e-05, 'epoch': 2.75}
{'loss': 0.2956, 'grad_norm': 11.924211502075195, 'learning_rate': 3.606163598869625e-05, 'epoch': 2.79}
{'loss': 0.2933, 'grad_norm': 13.84641170501709, 'learning_rate': 3.587069426411059e-05, 'epoch': 2.83}
{'loss': 0.2955, 'grad_norm': 1.4324191808700562, 'learning_rate': 3.567975253952494e-05, 'epoch': 2.86}
{'loss': 0.2979, 'grad_norm': 16.496456146240234, 'learning_rate': 3.548881081493928e-05, 'epoch': 2.9}
{'loss': 0.3057, 'grad_norm': 11.092369079589844, 'learning_rate': 3.529786909035362e-05, 'epoch': 2.94}
{'loss': 0.3009, 'grad_norm': 7.267322063446045, 'learning_rate': 3.510692736576797e-05, 'epoch': 2.98}
{'loss': 0.2896, 'grad_norm': 0.5357177257537842, 'learning_rate': 3.491598564118231e-05, 'epoch': 3.02}
{'loss': 0.2932, 'grad_norm': 1.155114769935608, 'learning_rate': 3.4725043916596654e-05, 'epoch': 3.06}
{'loss': 0.285, 'grad_norm': 8.982892036437988, 'learning_rate': 3.4534102192010996e-05, 'epoch': 3.09}
{'loss': 0.3005, 'grad_norm': 12.096739768981934, 'learning_rate': 3.4343160467425344e-05, 'epoch': 3.13}
{'loss': 0.2924, 'grad_norm': 0.784485399723053, 'learning_rate': 3.4152218742839686e-05, 'epoch': 3.17}
{'loss': 0.2869, 'grad_norm': 0.4195365309715271, 'learning_rate': 3.396127701825403e-05, 'epoch': 3.21}
{'loss': 0.2832, 'grad_norm': 7.811360836029053, 'learning_rate': 3.3770335293668376e-05, 'epoch': 3.25}
{'loss': 0.3067, 'grad_norm': 11.713638305664062, 'learning_rate': 3.357939356908272e-05, 'epoch': 3.28}
{'loss': 0.2745, 'grad_norm': 17.278959274291992, 'learning_rate': 3.338845184449706e-05, 'epoch': 3.32}
{'loss': 0.2786, 'grad_norm': 1.324584722518921, 'learning_rate': 3.319751011991141e-05, 'epoch': 3.36}
{'loss': 0.2822, 'grad_norm': 9.455523490905762, 'learning_rate': 3.300656839532575e-05, 'epoch': 3.4}
{'loss': 0.2723, 'grad_norm': 7.415541648864746, 'learning_rate': 3.281562667074009e-05, 'epoch': 3.44}
{'loss': 0.2762, 'grad_norm': 8.517974853515625, 'learning_rate': 3.262468494615444e-05, 'epoch': 3.48}
{'loss': 0.2864, 'grad_norm': 13.677776336669922, 'learning_rate': 3.243374322156878e-05, 'epoch': 3.51}
{'loss': 0.2864, 'grad_norm': 0.65522700548172, 'learning_rate': 3.224280149698312e-05, 'epoch': 3.55}
{'loss': 0.2923, 'grad_norm': 10.037339210510254, 'learning_rate': 3.2051859772397464e-05, 'epoch': 3.59}
{'loss': 0.2945, 'grad_norm': 6.771048069000244, 'learning_rate': 3.186091804781181e-05, 'epoch': 3.63}
{'loss': 0.2801, 'grad_norm': 3.607975959777832, 'learning_rate': 3.1669976323226154e-05, 'epoch': 3.67}
{'loss': 0.2878, 'grad_norm': 15.881536483764648, 'learning_rate': 3.1479034598640496e-05, 'epoch': 3.7}
{'loss': 0.2904, 'grad_norm': 1.156734585762024, 'learning_rate': 3.1288092874054844e-05, 'epoch': 3.74}
{'loss': 0.285, 'grad_norm': 0.6118171811103821, 'learning_rate': 3.1097151149469186e-05, 'epoch': 3.78}
{'loss': 0.2764, 'grad_norm': 1.8113188743591309, 'learning_rate': 3.090620942488353e-05, 'epoch': 3.82}
{'loss': 0.2979, 'grad_norm': 11.482481956481934, 'learning_rate': 3.0715267700297876e-05, 'epoch': 3.86}
{'loss': 0.2947, 'grad_norm': 9.721230506896973, 'learning_rate': 3.052432597571222e-05, 'epoch': 3.9}
{'loss': 0.2746, 'grad_norm': 8.53109073638916, 'learning_rate': 3.0333384251126556e-05, 'epoch': 3.93}
{'loss': 0.2917, 'grad_norm': 9.790496826171875, 'learning_rate': 3.0142442526540904e-05, 'epoch': 3.97}
{'loss': 0.2632, 'grad_norm': 9.370100975036621, 'learning_rate': 2.9951500801955246e-05, 'epoch': 4.01}
{'loss': 0.2694, 'grad_norm': 11.718461990356445, 'learning_rate': 2.9760559077369587e-05, 'epoch': 4.05}
{'loss': 0.2871, 'grad_norm': 19.03695297241211, 'learning_rate': 2.956961735278393e-05, 'epoch': 4.09}
{'loss': 0.2777, 'grad_norm': 32.05430221557617, 'learning_rate': 2.9378675628198277e-05, 'epoch': 4.12}
{'loss': 0.2922, 'grad_norm': 5.773058891296387, 'learning_rate': 2.918773390361262e-05, 'epoch': 4.16}
{'loss': 0.2638, 'grad_norm': 0.40718138217926025, 'learning_rate': 2.899679217902696e-05, 'epoch': 4.2}
{'loss': 0.2867, 'grad_norm': 13.066161155700684, 'learning_rate': 2.880585045444131e-05, 'epoch': 4.24}
{'loss': 0.2762, 'grad_norm': 6.674785614013672, 'learning_rate': 2.861490872985565e-05, 'epoch': 4.28}
{'loss': 0.2909, 'grad_norm': 16.38603401184082, 'learning_rate': 2.8423967005269992e-05, 'epoch': 4.32}
{'loss': 0.2834, 'grad_norm': 0.7691944241523743, 'learning_rate': 2.8233025280684337e-05, 'epoch': 4.35}
{'loss': 0.2793, 'grad_norm': 0.43585050106048584, 'learning_rate': 2.804208355609868e-05, 'epoch': 4.39}
{'loss': 0.2677, 'grad_norm': 33.396419525146484, 'learning_rate': 2.7851141831513024e-05, 'epoch': 4.43}
{'loss': 0.2733, 'grad_norm': 8.177713394165039, 'learning_rate': 2.766020010692737e-05, 'epoch': 4.47}
{'loss': 0.27, 'grad_norm': 10.081765174865723, 'learning_rate': 2.746925838234171e-05, 'epoch': 4.51}
{'loss': 0.2916, 'grad_norm': 14.650464057922363, 'learning_rate': 2.7278316657756052e-05, 'epoch': 4.54}
{'loss': 0.2685, 'grad_norm': 6.256812572479248, 'learning_rate': 2.7087374933170394e-05, 'epoch': 4.58}
{'loss': 0.27, 'grad_norm': 0.9923094511032104, 'learning_rate': 2.6896433208584742e-05, 'epoch': 4.62}
{'loss': 0.2797, 'grad_norm': 17.55318260192871, 'learning_rate': 2.6705491483999084e-05, 'epoch': 4.66}
{'loss': 0.269, 'grad_norm': 10.312139511108398, 'learning_rate': 2.6514549759413426e-05, 'epoch': 4.7}
{'loss': 0.2731, 'grad_norm': 0.3760388195514679, 'learning_rate': 2.6323608034827774e-05, 'epoch': 4.74}
{'loss': 0.2641, 'grad_norm': 0.3794688880443573, 'learning_rate': 2.6132666310242116e-05, 'epoch': 4.77}
{'loss': 0.2569, 'grad_norm': 24.687969207763672, 'learning_rate': 2.5941724585656457e-05, 'epoch': 4.81}
{'loss': 0.2821, 'grad_norm': 0.35477590560913086, 'learning_rate': 2.5750782861070806e-05, 'epoch': 4.85}
{'loss': 0.2594, 'grad_norm': 12.996932983398438, 'learning_rate': 2.5559841136485147e-05, 'epoch': 4.89}
{'loss': 0.2673, 'grad_norm': 26.96327781677246, 'learning_rate': 2.536889941189949e-05, 'epoch': 4.93}
{'loss': 0.2757, 'grad_norm': 8.330635070800781, 'learning_rate': 2.5177957687313837e-05, 'epoch': 4.96}
{'loss': 0.2725, 'grad_norm': 14.076995849609375, 'learning_rate': 2.498701596272818e-05, 'epoch': 5.0}
{'loss': 0.2542, 'grad_norm': 0.25298964977264404, 'learning_rate': 2.479607423814252e-05, 'epoch': 5.04}
{'loss': 0.261, 'grad_norm': 46.578426361083984, 'learning_rate': 2.4605132513556866e-05, 'epoch': 5.08}
{'loss': 0.2851, 'grad_norm': 10.432655334472656, 'learning_rate': 2.4414190788971207e-05, 'epoch': 5.12}
{'loss': 0.2735, 'grad_norm': 4.167952060699463, 'learning_rate': 2.422324906438555e-05, 'epoch': 5.16}
{'loss': 0.2628, 'grad_norm': 0.27316340804100037, 'learning_rate': 2.4032307339799894e-05, 'epoch': 5.19}
{'loss': 0.2683, 'grad_norm': 0.5950897336006165, 'learning_rate': 2.3841365615214235e-05, 'epoch': 5.23}
{'loss': 0.258, 'grad_norm': 34.143428802490234, 'learning_rate': 2.365042389062858e-05, 'epoch': 5.27}
{'loss': 0.2586, 'grad_norm': 0.8260636329650879, 'learning_rate': 2.3459482166042925e-05, 'epoch': 5.31}
{'loss': 0.2679, 'grad_norm': 0.716440737247467, 'learning_rate': 2.3268540441457267e-05, 'epoch': 5.35}
{'loss': 0.276, 'grad_norm': 0.38433006405830383, 'learning_rate': 2.3077598716871612e-05, 'epoch': 5.38}
{'loss': 0.2814, 'grad_norm': 8.556076049804688, 'learning_rate': 2.2886656992285954e-05, 'epoch': 5.42}
{'loss': 0.2599, 'grad_norm': 0.5107830166816711, 'learning_rate': 2.26957152677003e-05, 'epoch': 5.46}
{'loss': 0.2495, 'grad_norm': 1.2700514793395996, 'learning_rate': 2.2504773543114644e-05, 'epoch': 5.5}
{'loss': 0.2665, 'grad_norm': 26.120574951171875, 'learning_rate': 2.2313831818528985e-05, 'epoch': 5.54}
{'loss': 0.2635, 'grad_norm': 2.1999335289001465, 'learning_rate': 2.212289009394333e-05, 'epoch': 5.58}
{'loss': 0.279, 'grad_norm': 14.655807495117188, 'learning_rate': 2.1931948369357675e-05, 'epoch': 5.61}
{'loss': 0.2587, 'grad_norm': 0.479753315448761, 'learning_rate': 2.1741006644772017e-05, 'epoch': 5.65}
{'loss': 0.2728, 'grad_norm': 0.5939254760742188, 'learning_rate': 2.1550064920186362e-05, 'epoch': 5.69}
{'loss': 0.2533, 'grad_norm': 5.450633525848389, 'learning_rate': 2.1359123195600704e-05, 'epoch': 5.73}
{'loss': 0.2711, 'grad_norm': 33.33818054199219, 'learning_rate': 2.116818147101505e-05, 'epoch': 5.77}
{'loss': 0.272, 'grad_norm': 0.6927990913391113, 'learning_rate': 2.097723974642939e-05, 'epoch': 5.8}
{'loss': 0.2731, 'grad_norm': 9.646309852600098, 'learning_rate': 2.0786298021843735e-05, 'epoch': 5.84}
{'loss': 0.2601, 'grad_norm': 5.817882537841797, 'learning_rate': 2.0595356297258077e-05, 'epoch': 5.88}
{'loss': 0.2734, 'grad_norm': 26.427865982055664, 'learning_rate': 2.040441457267242e-05, 'epoch': 5.92}
{'loss': 0.2831, 'grad_norm': 5.949215412139893, 'learning_rate': 2.0213472848086764e-05, 'epoch': 5.96}
{'loss': 0.2526, 'grad_norm': 38.148624420166016, 'learning_rate': 2.002253112350111e-05, 'epoch': 6.0}
{'loss': 0.277, 'grad_norm': 7.834826469421387, 'learning_rate': 1.983158939891545e-05, 'epoch': 6.03}
{'loss': 0.2583, 'grad_norm': 0.4554772973060608, 'learning_rate': 1.9640647674329795e-05, 'epoch': 6.07}
{'loss': 0.2579, 'grad_norm': 17.07042694091797, 'learning_rate': 1.944970594974414e-05, 'epoch': 6.11}
{'loss': 0.2592, 'grad_norm': 2.2619404792785645, 'learning_rate': 1.9258764225158482e-05, 'epoch': 6.15}
{'loss': 0.2634, 'grad_norm': 56.6258659362793, 'learning_rate': 1.9067822500572827e-05, 'epoch': 6.19}
{'loss': 0.2477, 'grad_norm': 12.97281551361084, 'learning_rate': 1.887688077598717e-05, 'epoch': 6.22}
{'loss': 0.2677, 'grad_norm': 16.288732528686523, 'learning_rate': 1.8685939051401514e-05, 'epoch': 6.26}
{'loss': 0.2554, 'grad_norm': 9.395675659179688, 'learning_rate': 1.849499732681586e-05, 'epoch': 6.3}
{'loss': 0.2639, 'grad_norm': 37.248382568359375, 'learning_rate': 1.83040556022302e-05, 'epoch': 6.34}
{'loss': 0.2566, 'grad_norm': 8.459151268005371, 'learning_rate': 1.8113113877644545e-05, 'epoch': 6.38}
{'loss': 0.2674, 'grad_norm': 8.407461166381836, 'learning_rate': 1.7922172153058887e-05, 'epoch': 6.42}
{'loss': 0.2678, 'grad_norm': 19.92375373840332, 'learning_rate': 1.7731230428473232e-05, 'epoch': 6.45}
{'loss': 0.2487, 'grad_norm': 3.2271833419799805, 'learning_rate': 1.7540288703887577e-05, 'epoch': 6.49}
{'loss': 0.264, 'grad_norm': 1.807456612586975, 'learning_rate': 1.734934697930192e-05, 'epoch': 6.53}
{'loss': 0.2681, 'grad_norm': 0.8742185235023499, 'learning_rate': 1.715840525471626e-05, 'epoch': 6.57}
{'loss': 0.2309, 'grad_norm': 0.47028228640556335, 'learning_rate': 1.6967463530130605e-05, 'epoch': 6.61}
{'loss': 0.2631, 'grad_norm': 0.6502563953399658, 'learning_rate': 1.6776521805544947e-05, 'epoch': 6.64}
{'loss': 0.2559, 'grad_norm': 3.7508881092071533, 'learning_rate': 1.6585580080959292e-05, 'epoch': 6.68}
{'loss': 0.2597, 'grad_norm': 14.914066314697266, 'learning_rate': 1.6394638356373633e-05, 'epoch': 6.72}
{'loss': 0.2563, 'grad_norm': 15.901938438415527, 'learning_rate': 1.620369663178798e-05, 'epoch': 6.76}
{'loss': 0.2441, 'grad_norm': 14.233125686645508, 'learning_rate': 1.6012754907202323e-05, 'epoch': 6.8}
{'loss': 0.2716, 'grad_norm': 0.3414585590362549, 'learning_rate': 1.5821813182616665e-05, 'epoch': 6.84}
{'loss': 0.2658, 'grad_norm': 8.390944480895996, 'learning_rate': 1.563087145803101e-05, 'epoch': 6.87}
{'loss': 0.2517, 'grad_norm': 0.49760687351226807, 'learning_rate': 1.543992973344535e-05, 'epoch': 6.91}
{'loss': 0.2565, 'grad_norm': 21.84235382080078, 'learning_rate': 1.5248988008859697e-05, 'epoch': 6.95}
{'loss': 0.2468, 'grad_norm': 0.9249453544616699, 'learning_rate': 1.5058046284274042e-05, 'epoch': 6.99}
{'loss': 0.2605, 'grad_norm': 0.1719362586736679, 'learning_rate': 1.4867104559688383e-05, 'epoch': 7.03}
{'loss': 0.2513, 'grad_norm': 0.2985737919807434, 'learning_rate': 1.4676162835102728e-05, 'epoch': 7.06}
{'loss': 0.2478, 'grad_norm': 0.32206135988235474, 'learning_rate': 1.4485221110517072e-05, 'epoch': 7.1}
{'loss': 0.2496, 'grad_norm': 2.1343390941619873, 'learning_rate': 1.4294279385931413e-05, 'epoch': 7.14}
{'loss': 0.2385, 'grad_norm': 18.07288360595703, 'learning_rate': 1.4103337661345758e-05, 'epoch': 7.18}
{'loss': 0.2465, 'grad_norm': 2.5710365772247314, 'learning_rate': 1.39123959367601e-05, 'epoch': 7.22}
{'loss': 0.2505, 'grad_norm': 6.738758087158203, 'learning_rate': 1.3721454212174445e-05, 'epoch': 7.26}
{'loss': 0.2518, 'grad_norm': 13.017762184143066, 'learning_rate': 1.353051248758879e-05, 'epoch': 7.29}
{'loss': 0.2577, 'grad_norm': 0.1454649418592453, 'learning_rate': 1.3339570763003132e-05, 'epoch': 7.33}
{'loss': 0.2366, 'grad_norm': 11.271116256713867, 'learning_rate': 1.3148629038417477e-05, 'epoch': 7.37}
{'loss': 0.2461, 'grad_norm': 14.102458000183105, 'learning_rate': 1.2957687313831818e-05, 'epoch': 7.41}
{'loss': 0.2707, 'grad_norm': 0.18860508501529694, 'learning_rate': 1.2766745589246163e-05, 'epoch': 7.45}
{'loss': 0.2373, 'grad_norm': 10.16494369506836, 'learning_rate': 1.2575803864660507e-05, 'epoch': 7.48}
{'loss': 0.2441, 'grad_norm': 2.8103532791137695, 'learning_rate': 1.238486214007485e-05, 'epoch': 7.52}
{'loss': 0.271, 'grad_norm': 0.11790781468153, 'learning_rate': 1.2193920415489193e-05, 'epoch': 7.56}
{'loss': 0.2507, 'grad_norm': 0.9925351738929749, 'learning_rate': 1.2002978690903536e-05, 'epoch': 7.6}
{'loss': 0.2489, 'grad_norm': 0.6430020928382874, 'learning_rate': 1.181203696631788e-05, 'epoch': 7.64}
{'loss': 0.2768, 'grad_norm': 1.6207213401794434, 'learning_rate': 1.1621095241732225e-05, 'epoch': 7.68}
{'loss': 0.2399, 'grad_norm': 0.1914842575788498, 'learning_rate': 1.1430153517146568e-05, 'epoch': 7.71}
{'loss': 0.2578, 'grad_norm': 33.76418685913086, 'learning_rate': 1.1239211792560911e-05, 'epoch': 7.75}
{'loss': 0.2745, 'grad_norm': 0.5514010190963745, 'learning_rate': 1.1048270067975255e-05, 'epoch': 7.79}
{'loss': 0.2652, 'grad_norm': 0.24419894814491272, 'learning_rate': 1.0857328343389598e-05, 'epoch': 7.83}
{'loss': 0.2378, 'grad_norm': 21.101341247558594, 'learning_rate': 1.0666386618803941e-05, 'epoch': 7.87}
{'loss': 0.2424, 'grad_norm': 9.419160842895508, 'learning_rate': 1.0475444894218285e-05, 'epoch': 7.9}
{'loss': 0.256, 'grad_norm': 8.504898071289062, 'learning_rate': 1.0284503169632628e-05, 'epoch': 7.94}
{'loss': 0.2582, 'grad_norm': 4.782492637634277, 'learning_rate': 1.0093561445046971e-05, 'epoch': 7.98}
{'loss': 0.2641, 'grad_norm': 17.487850189208984, 'learning_rate': 9.902619720461316e-06, 'epoch': 8.02}
{'loss': 0.2355, 'grad_norm': 16.74688148498535, 'learning_rate': 9.71167799587566e-06, 'epoch': 8.06}
{'loss': 0.231, 'grad_norm': 20.714765548706055, 'learning_rate': 9.520736271290003e-06, 'epoch': 8.1}
{'loss': 0.2396, 'grad_norm': 50.70199966430664, 'learning_rate': 9.329794546704346e-06, 'epoch': 8.13}
{'loss': 0.2463, 'grad_norm': 53.29233169555664, 'learning_rate': 9.13885282211869e-06, 'epoch': 8.17}
{'loss': 0.2298, 'grad_norm': 19.27553939819336, 'learning_rate': 8.947911097533033e-06, 'epoch': 8.21}
{'loss': 0.2373, 'grad_norm': 0.38581547141075134, 'learning_rate': 8.756969372947376e-06, 'epoch': 8.25}
{'loss': 0.2481, 'grad_norm': 17.17620086669922, 'learning_rate': 8.56602764836172e-06, 'epoch': 8.29}
{'loss': 0.2496, 'grad_norm': 5.392579555511475, 'learning_rate': 8.375085923776065e-06, 'epoch': 8.33}
{'loss': 0.2637, 'grad_norm': 41.63316345214844, 'learning_rate': 8.184144199190408e-06, 'epoch': 8.36}
{'loss': 0.2469, 'grad_norm': 7.807092666625977, 'learning_rate': 7.993202474604751e-06, 'epoch': 8.4}
{'loss': 0.2454, 'grad_norm': 11.120017051696777, 'learning_rate': 7.802260750019095e-06, 'epoch': 8.44}
{'loss': 0.2397, 'grad_norm': 1.167780876159668, 'learning_rate': 7.611319025433437e-06, 'epoch': 8.48}
{'loss': 0.2669, 'grad_norm': 0.8853473663330078, 'learning_rate': 7.420377300847782e-06, 'epoch': 8.52}
{'loss': 0.2481, 'grad_norm': 0.1403081715106964, 'learning_rate': 7.2294355762621254e-06, 'epoch': 8.55}
{'loss': 0.2676, 'grad_norm': 0.39849111437797546, 'learning_rate': 7.038493851676469e-06, 'epoch': 8.59}
{'loss': 0.2281, 'grad_norm': 0.23956739902496338, 'learning_rate': 6.847552127090811e-06, 'epoch': 8.63}
{'loss': 0.2477, 'grad_norm': 16.296506881713867, 'learning_rate': 6.656610402505156e-06, 'epoch': 8.67}
{'loss': 0.2551, 'grad_norm': 7.46291446685791, 'learning_rate': 6.4656686779194996e-06, 'epoch': 8.71}
{'loss': 0.2646, 'grad_norm': 5.537749767303467, 'learning_rate': 6.274726953333843e-06, 'epoch': 8.75}
{'loss': 0.2391, 'grad_norm': 14.058670997619629, 'learning_rate': 6.083785228748186e-06, 'epoch': 8.78}
{'loss': 0.2376, 'grad_norm': 0.6137887835502625, 'learning_rate': 5.8928435041625295e-06, 'epoch': 8.82}
{'loss': 0.2248, 'grad_norm': 0.47878482937812805, 'learning_rate': 5.701901779576874e-06, 'epoch': 8.86}
{'loss': 0.2483, 'grad_norm': 0.9939274787902832, 'learning_rate': 5.510960054991217e-06, 'epoch': 8.9}
{'loss': 0.2471, 'grad_norm': 17.498517990112305, 'learning_rate': 5.320018330405561e-06, 'epoch': 8.94}
{'loss': 0.2395, 'grad_norm': 0.3158620595932007, 'learning_rate': 5.129076605819904e-06, 'epoch': 8.97}
{'loss': 0.2478, 'grad_norm': 16.087156295776367, 'learning_rate': 4.938134881234248e-06, 'epoch': 9.01}
{'loss': 0.2126, 'grad_norm': 0.20076848566532135, 'learning_rate': 4.747193156648591e-06, 'epoch': 9.05}
{'loss': 0.2682, 'grad_norm': 18.471643447875977, 'learning_rate': 4.5562514320629344e-06, 'epoch': 9.09}
{'loss': 0.2389, 'grad_norm': 8.81783390045166, 'learning_rate': 4.365309707477279e-06, 'epoch': 9.13}
{'loss': 0.2473, 'grad_norm': 0.23712675273418427, 'learning_rate': 4.174367982891621e-06, 'epoch': 9.17}
{'loss': 0.2333, 'grad_norm': 16.373945236206055, 'learning_rate': 3.983426258305965e-06, 'epoch': 9.2}
{'loss': 0.2537, 'grad_norm': 9.088841438293457, 'learning_rate': 3.7924845337203086e-06, 'epoch': 9.24}
{'loss': 0.2318, 'grad_norm': 0.28555241227149963, 'learning_rate': 3.6015428091346523e-06, 'epoch': 9.28}
{'loss': 0.2312, 'grad_norm': 0.1986660361289978, 'learning_rate': 3.4106010845489956e-06, 'epoch': 9.32}
{'loss': 0.2635, 'grad_norm': 0.18439681828022003, 'learning_rate': 3.2196593599633394e-06, 'epoch': 9.36}
{'loss': 0.2432, 'grad_norm': 13.836210250854492, 'learning_rate': 3.0287176353776827e-06, 'epoch': 9.39}
{'loss': 0.2646, 'grad_norm': 69.16944885253906, 'learning_rate': 2.8377759107920264e-06, 'epoch': 9.43}
{'loss': 0.2617, 'grad_norm': 12.72176456451416, 'learning_rate': 2.6468341862063698e-06, 'epoch': 9.47}
{'loss': 0.2465, 'grad_norm': 0.22945483028888702, 'learning_rate': 2.4558924616207135e-06, 'epoch': 9.51}
{'loss': 0.2535, 'grad_norm': 0.40071621537208557, 'learning_rate': 2.264950737035057e-06, 'epoch': 9.55}
{'loss': 0.2507, 'grad_norm': 0.3415454030036926, 'learning_rate': 2.0740090124494006e-06, 'epoch': 9.59}
{'loss': 0.2292, 'grad_norm': 13.619522094726562, 'learning_rate': 1.883067287863744e-06, 'epoch': 9.62}
{'loss': 0.2347, 'grad_norm': 0.44193190336227417, 'learning_rate': 1.6921255632780876e-06, 'epoch': 9.66}
{'loss': 0.2204, 'grad_norm': 44.76580810546875, 'learning_rate': 1.501183838692431e-06, 'epoch': 9.7}
{'loss': 0.2452, 'grad_norm': 5.926690578460693, 'learning_rate': 1.3102421141067747e-06, 'epoch': 9.74}
{'loss': 0.2151, 'grad_norm': 0.9525282979011536, 'learning_rate': 1.1193003895211182e-06, 'epoch': 9.78}
{'loss': 0.2351, 'grad_norm': 0.7455090880393982, 'learning_rate': 9.283586649354618e-07, 'epoch': 9.81}
{'loss': 0.2745, 'grad_norm': 1.9444575309753418, 'learning_rate': 7.374169403498053e-07, 'epoch': 9.85}
{'loss': 0.2349, 'grad_norm': 52.323150634765625, 'learning_rate': 5.464752157641488e-07, 'epoch': 9.89}
{'loss': 0.2635, 'grad_norm': 8.839405059814453, 'learning_rate': 3.5553349117849236e-07, 'epoch': 9.93}
{'loss': 0.2131, 'grad_norm': 1.1357852220535278, 'learning_rate': 1.6459176659283587e-07, 'epoch': 9.97}
{'train_runtime': 20033.2347, 'train_samples_per_second': 52.285, 'train_steps_per_second': 6.536, 'train_loss': 0.2816662991100624, 'epoch': 10.0}
***** train metrics *****
  epoch                    =        10.0
  total_flos               = 227717064GF
  train_loss               =      0.2817
  train_runtime            =  5:33:53.23
  train_samples            =      104743
  train_samples_per_second =      52.285
  train_steps_per_second   =       6.536
08/06/2025 20:20:36 - INFO - __main__ - *** Evaluate ***
***** eval metrics *****
  epoch                   =       10.0
  eval_accuracy           =     0.9182
  eval_loss               =     0.3165
  eval_runtime            = 0:00:46.30
  eval_samples            =       5463
  eval_samples_per_second =    117.979
  eval_steps_per_second   =      14.75
08/06/2025 20:21:33 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
08/06/2025 20:21:33 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_revision=None,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
liger_kernel_config=None,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_final/BERT_large/QNLI/InA30/runs/Aug06_20-21-33_n29,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=10.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=output_final/BERT_large/QNLI/InA30/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=output_final/BERT_large/QNLI/InA30/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
08/06/2025 20:21:35 - INFO - datasets.builder - Found cached dataset glue (/home/kangchen/.cache/huggingface/datasets/nyu-mll___glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
The task type of PEFT is not proper!
LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='bert-large-cased', revision=None, task_type=<TaskType.SEQ_CLS: 'SEQ_CLS'>, inference_mode=False, r=4, target_modules={'query', 'key', 'value'}, lora_inhibition=0.3, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
###############
PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): BertForSequenceClassification(
      (bert): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 1024, padding_idx=0)
          (position_embeddings): Embedding(512, 1024)
          (token_type_embeddings): Embedding(2, 1024)
          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0-23): 24 x BertLayer(
              (attention): BertAttention(
                (self): BertSdpaSelfAttention(
                  (query): lora.Linear(
                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=4, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=4, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (key): lora.Linear(
                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=4, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=4, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (value): lora.Linear(
                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=4, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=4, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
          (activation): Tanh()
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (classifier): ModulesToSaveWrapper(
        (original_module): Linear(in_features=1024, out_features=2, bias=True)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=1024, out_features=2, bias=True)
        )
      )
    )
  )
)
08/06/2025 20:21:37 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/kangchen/.cache/huggingface/datasets/nyu-mll___glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-857a5657ab1232c9_*_of_00001.arrow
08/06/2025 20:21:37 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/kangchen/.cache/huggingface/datasets/nyu-mll___glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-32484430d63833d1_*_of_00001.arrow
08/06/2025 20:21:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/kangchen/.cache/huggingface/datasets/nyu-mll___glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4b92e6513769a5d9.arrow
08/06/2025 20:21:39 - INFO - __main__ - Class distribution in train set:
08/06/2025 20:21:39 - INFO - __main__ -   Label 1: 52366 (49.99%)
08/06/2025 20:21:39 - INFO - __main__ -   Label 0: 52377 (50.01%)
08/06/2025 20:21:39 - INFO - __main__ - Class distribution in validation set:
08/06/2025 20:21:39 - INFO - __main__ -   Label 0: 2702 (49.46%)
08/06/2025 20:21:39 - INFO - __main__ -   Label 1: 2761 (50.54%)
08/06/2025 20:21:39 - INFO - __main__ - Class distribution in test set:
08/06/2025 20:21:39 - INFO - __main__ -   Label -1: 5463 (100.00%)
08/06/2025 20:21:39 - INFO - __main__ - Sample 83810 of the training set: {'question': 'What tactics of the PVA were US and ROK troops not prepared to handle?', 'sentence': "On 27 November at the Korean eastern front, a U.S. 7th Infantry Division Regimental Combat Team (3,000 soldiers) and the U.S. 1st Marine Division (12,000–15,000 marines) were unprepared for the PVA 9th Army Group's three-pronged encirclement tactics at the Battle of Chosin Reservoir, but they managed to escape under Air Force and X Corps support fire—albeit with some 15,000 collective casualties.", 'label': 0, 'idx': 83810, 'input_ids': [101, 1327, 10524, 1104, 1103, 153, 12152, 1127, 1646, 1105, 155, 22027, 2830, 1136, 4029, 1106, 4282, 136, 102, 1212, 1765, 1379, 1120, 1103, 3947, 2638, 1524, 117, 170, 158, 119, 156, 119, 4766, 4155, 1784, 3391, 1348, 10314, 2649, 113, 124, 117, 1288, 2803, 114, 1105, 1103, 158, 119, 156, 119, 2198, 4620, 1784, 113, 1367, 117, 1288, 782, 1405, 117, 1288, 5243, 1116, 114, 1127, 8362, 1643, 1874, 17482, 1174, 1111, 1103, 153, 12152, 5612, 1740, 1990, 112, 188, 1210, 118, 5250, 18288, 4035, 6617, 24490, 1880, 10524, 1120, 1103, 2651, 1104, 22964, 10606, 15220, 117, 1133, 1152, 2374, 1106, 3359, 1223, 1806, 2300, 1105, 161, 3158, 1619, 1783, 783, 12456, 1114, 1199, 1405, 117, 1288, 7764, 8487, 119, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}.
08/06/2025 20:21:39 - INFO - __main__ - Sample 14592 of the training set: {'question': "How did the Egyptian people feel about Nasser's response to the attack?", 'sentence': 'Nasser did not feel that the Egyptian Army was ready for a confrontation and did not retaliate militarily.', 'label': 1, 'idx': 14592, 'input_ids': [101, 1731, 1225, 1103, 6210, 1234, 1631, 1164, 11896, 14607, 112, 188, 2593, 1106, 1103, 2035, 136, 102, 11896, 14607, 1225, 1136, 1631, 1115, 1103, 6210, 1740, 1108, 2407, 1111, 170, 14002, 1105, 1225, 1136, 1231, 6163, 15045, 1940, 12888, 18206, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
08/06/2025 20:21:39 - INFO - __main__ - Sample 3278 of the training set: {'question': 'What kind of aircraft are capable of taking off vertically?', 'sentence': 'Although STOVL aircraft are capable of taking off vertically from a spot on the deck, using the ramp and a running start is far more fuel efficient and permits a heavier launch weight.', 'label': 0, 'idx': 3278, 'input_ids': [101, 1327, 1912, 1104, 2163, 1132, 4451, 1104, 1781, 1228, 22026, 136, 102, 1966, 23676, 2346, 2559, 2162, 2163, 1132, 4451, 1104, 1781, 1228, 22026, 1121, 170, 3205, 1113, 1103, 5579, 117, 1606, 1103, 14207, 1105, 170, 1919, 1838, 1110, 1677, 1167, 4251, 7856, 1105, 15267, 170, 12163, 4286, 2841, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
08/06/2025 20:21:40 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.6358, 'grad_norm': 10.53084659576416, 'learning_rate': 4.980944015886352e-05, 'epoch': 0.04}
{'loss': 0.483, 'grad_norm': 4.037789344787598, 'learning_rate': 4.961849843427786e-05, 'epoch': 0.08}
{'loss': 0.4262, 'grad_norm': 5.615322589874268, 'learning_rate': 4.94275567096922e-05, 'epoch': 0.11}
{'loss': 0.4139, 'grad_norm': 7.938291549682617, 'learning_rate': 4.923661498510655e-05, 'epoch': 0.15}
{'loss': 0.3947, 'grad_norm': 7.8838725090026855, 'learning_rate': 4.904567326052089e-05, 'epoch': 0.19}
{'loss': 0.3893, 'grad_norm': 12.24278736114502, 'learning_rate': 4.885473153593523e-05, 'epoch': 0.23}
{'loss': 0.3869, 'grad_norm': 4.055706977844238, 'learning_rate': 4.866378981134958e-05, 'epoch': 0.27}
{'loss': 0.3859, 'grad_norm': 4.6896138191223145, 'learning_rate': 4.847284808676392e-05, 'epoch': 0.31}
{'loss': 0.3767, 'grad_norm': 6.07366418838501, 'learning_rate': 4.828190636217826e-05, 'epoch': 0.34}
{'loss': 0.3611, 'grad_norm': 7.1957597732543945, 'learning_rate': 4.809096463759261e-05, 'epoch': 0.38}
{'loss': 0.3657, 'grad_norm': 4.372681140899658, 'learning_rate': 4.790002291300695e-05, 'epoch': 0.42}
{'loss': 0.3697, 'grad_norm': 4.894469261169434, 'learning_rate': 4.7709081188421295e-05, 'epoch': 0.46}
{'loss': 0.3639, 'grad_norm': 5.249190807342529, 'learning_rate': 4.7518139463835637e-05, 'epoch': 0.5}
{'loss': 0.3643, 'grad_norm': 3.7592058181762695, 'learning_rate': 4.7327197739249985e-05, 'epoch': 0.53}
{'loss': 0.3434, 'grad_norm': 11.55576229095459, 'learning_rate': 4.713625601466433e-05, 'epoch': 0.57}
{'loss': 0.3613, 'grad_norm': 11.281378746032715, 'learning_rate': 4.694531429007867e-05, 'epoch': 0.61}
{'loss': 0.3651, 'grad_norm': 8.651056289672852, 'learning_rate': 4.675437256549302e-05, 'epoch': 0.65}
{'loss': 0.3424, 'grad_norm': 14.263465881347656, 'learning_rate': 4.656343084090736e-05, 'epoch': 0.69}
{'loss': 0.3349, 'grad_norm': 6.145023345947266, 'learning_rate': 4.63724891163217e-05, 'epoch': 0.73}
{'loss': 0.3499, 'grad_norm': 8.369251251220703, 'learning_rate': 4.618154739173605e-05, 'epoch': 0.76}
{'loss': 0.329, 'grad_norm': 2.6044199466705322, 'learning_rate': 4.599060566715039e-05, 'epoch': 0.8}
{'loss': 0.3483, 'grad_norm': 5.082296371459961, 'learning_rate': 4.579966394256473e-05, 'epoch': 0.84}
{'loss': 0.341, 'grad_norm': 6.866949558258057, 'learning_rate': 4.560872221797908e-05, 'epoch': 0.88}
{'loss': 0.3285, 'grad_norm': 0.5590705275535583, 'learning_rate': 4.541778049339342e-05, 'epoch': 0.92}
{'loss': 0.3481, 'grad_norm': 3.4409279823303223, 'learning_rate': 4.522683876880776e-05, 'epoch': 0.95}
{'loss': 0.3314, 'grad_norm': 0.7077175378799438, 'learning_rate': 4.5035897044222105e-05, 'epoch': 0.99}
{'loss': 0.3276, 'grad_norm': 9.481780052185059, 'learning_rate': 4.484495531963645e-05, 'epoch': 1.03}
{'loss': 0.33, 'grad_norm': 4.518460750579834, 'learning_rate': 4.4654013595050795e-05, 'epoch': 1.07}
{'loss': 0.3236, 'grad_norm': 3.2599191665649414, 'learning_rate': 4.4463071870465137e-05, 'epoch': 1.11}
{'loss': 0.3332, 'grad_norm': 6.619580268859863, 'learning_rate': 4.427213014587948e-05, 'epoch': 1.15}
{'loss': 0.3262, 'grad_norm': 8.69969654083252, 'learning_rate': 4.408118842129382e-05, 'epoch': 1.18}
{'loss': 0.3062, 'grad_norm': 3.208707571029663, 'learning_rate': 4.389024669670817e-05, 'epoch': 1.22}
{'loss': 0.3112, 'grad_norm': 1.2042815685272217, 'learning_rate': 4.369930497212251e-05, 'epoch': 1.26}
{'loss': 0.3216, 'grad_norm': 10.960898399353027, 'learning_rate': 4.350836324753685e-05, 'epoch': 1.3}
{'loss': 0.3228, 'grad_norm': 4.101078987121582, 'learning_rate': 4.331742152295119e-05, 'epoch': 1.34}
{'loss': 0.3173, 'grad_norm': 0.8215355277061462, 'learning_rate': 4.312647979836554e-05, 'epoch': 1.37}
{'loss': 0.3189, 'grad_norm': 6.508853435516357, 'learning_rate': 4.293553807377988e-05, 'epoch': 1.41}
{'loss': 0.3046, 'grad_norm': 0.5096572637557983, 'learning_rate': 4.2744596349194225e-05, 'epoch': 1.45}
{'loss': 0.3052, 'grad_norm': 5.618605613708496, 'learning_rate': 4.2553654624608566e-05, 'epoch': 1.49}
{'loss': 0.3151, 'grad_norm': 2.7241690158843994, 'learning_rate': 4.2362712900022915e-05, 'epoch': 1.53}
{'loss': 0.3167, 'grad_norm': 6.838850975036621, 'learning_rate': 4.2171771175437256e-05, 'epoch': 1.57}
{'loss': 0.3289, 'grad_norm': 10.08757209777832, 'learning_rate': 4.19808294508516e-05, 'epoch': 1.6}
{'loss': 0.3116, 'grad_norm': 9.738860130310059, 'learning_rate': 4.1789887726265946e-05, 'epoch': 1.64}
{'loss': 0.3071, 'grad_norm': 10.93837833404541, 'learning_rate': 4.159894600168029e-05, 'epoch': 1.68}
{'loss': 0.3191, 'grad_norm': 11.534760475158691, 'learning_rate': 4.140800427709463e-05, 'epoch': 1.72}
{'loss': 0.3134, 'grad_norm': 2.6788032054901123, 'learning_rate': 4.121706255250898e-05, 'epoch': 1.76}
{'loss': 0.3229, 'grad_norm': 1.8172203302383423, 'learning_rate': 4.102612082792332e-05, 'epoch': 1.79}
{'loss': 0.3042, 'grad_norm': 10.609435081481934, 'learning_rate': 4.083517910333766e-05, 'epoch': 1.83}
{'loss': 0.3107, 'grad_norm': 6.8177080154418945, 'learning_rate': 4.064423737875201e-05, 'epoch': 1.87}
{'loss': 0.3052, 'grad_norm': 15.783669471740723, 'learning_rate': 4.045329565416635e-05, 'epoch': 1.91}
{'loss': 0.3076, 'grad_norm': 5.504003524780273, 'learning_rate': 4.026235392958069e-05, 'epoch': 1.95}
{'loss': 0.3015, 'grad_norm': 4.9706244468688965, 'learning_rate': 4.0071412204995035e-05, 'epoch': 1.99}
{'loss': 0.3174, 'grad_norm': 11.615315437316895, 'learning_rate': 3.988047048040938e-05, 'epoch': 2.02}
{'loss': 0.2825, 'grad_norm': 10.66186237335205, 'learning_rate': 3.9689528755823725e-05, 'epoch': 2.06}
{'loss': 0.3, 'grad_norm': 11.343547821044922, 'learning_rate': 3.9498587031238066e-05, 'epoch': 2.1}
{'loss': 0.2989, 'grad_norm': 5.697444915771484, 'learning_rate': 3.9307645306652415e-05, 'epoch': 2.14}
{'loss': 0.3126, 'grad_norm': 16.965978622436523, 'learning_rate': 3.9116703582066756e-05, 'epoch': 2.18}
{'loss': 0.2909, 'grad_norm': 0.9169769883155823, 'learning_rate': 3.89257618574811e-05, 'epoch': 2.21}
{'loss': 0.2868, 'grad_norm': 2.584770917892456, 'learning_rate': 3.8734820132895446e-05, 'epoch': 2.25}
{'loss': 0.2892, 'grad_norm': 9.819944381713867, 'learning_rate': 3.854387840830979e-05, 'epoch': 2.29}
{'loss': 0.2918, 'grad_norm': 20.665124893188477, 'learning_rate': 3.835293668372413e-05, 'epoch': 2.33}
{'loss': 0.2872, 'grad_norm': 3.2396059036254883, 'learning_rate': 3.816199495913848e-05, 'epoch': 2.37}
{'loss': 0.3054, 'grad_norm': 1.3802294731140137, 'learning_rate': 3.797105323455282e-05, 'epoch': 2.41}
{'loss': 0.3092, 'grad_norm': 12.009443283081055, 'learning_rate': 3.778011150996716e-05, 'epoch': 2.44}
{'loss': 0.286, 'grad_norm': 9.856634140014648, 'learning_rate': 3.75891697853815e-05, 'epoch': 2.48}
{'loss': 0.2971, 'grad_norm': 4.873864650726318, 'learning_rate': 3.739822806079585e-05, 'epoch': 2.52}
{'loss': 0.2871, 'grad_norm': 0.5384643077850342, 'learning_rate': 3.720728633621019e-05, 'epoch': 2.56}
{'loss': 0.2965, 'grad_norm': 13.97099494934082, 'learning_rate': 3.7016344611624534e-05, 'epoch': 2.6}
{'loss': 0.3009, 'grad_norm': 14.366978645324707, 'learning_rate': 3.6825402887038876e-05, 'epoch': 2.63}
{'loss': 0.2968, 'grad_norm': 5.831216812133789, 'learning_rate': 3.663446116245322e-05, 'epoch': 2.67}
{'loss': 0.3, 'grad_norm': 10.76602840423584, 'learning_rate': 3.644351943786756e-05, 'epoch': 2.71}
{'loss': 0.298, 'grad_norm': 7.478211879730225, 'learning_rate': 3.625257771328191e-05, 'epoch': 2.75}
{'loss': 0.2965, 'grad_norm': 16.0058650970459, 'learning_rate': 3.606163598869625e-05, 'epoch': 2.79}
{'loss': 0.296, 'grad_norm': 6.614048004150391, 'learning_rate': 3.587069426411059e-05, 'epoch': 2.83}
{'loss': 0.2961, 'grad_norm': 2.081805467605591, 'learning_rate': 3.567975253952494e-05, 'epoch': 2.86}
{'loss': 0.2969, 'grad_norm': 13.506364822387695, 'learning_rate': 3.548881081493928e-05, 'epoch': 2.9}
{'loss': 0.2995, 'grad_norm': 11.186175346374512, 'learning_rate': 3.529786909035362e-05, 'epoch': 2.94}
{'loss': 0.3015, 'grad_norm': 6.27740478515625, 'learning_rate': 3.510692736576797e-05, 'epoch': 2.98}
{'loss': 0.2902, 'grad_norm': 0.5541539788246155, 'learning_rate': 3.491598564118231e-05, 'epoch': 3.02}
{'loss': 0.2913, 'grad_norm': 1.7995586395263672, 'learning_rate': 3.4725043916596654e-05, 'epoch': 3.06}
{'loss': 0.2836, 'grad_norm': 9.075665473937988, 'learning_rate': 3.4534102192010996e-05, 'epoch': 3.09}
{'loss': 0.2965, 'grad_norm': 8.11109733581543, 'learning_rate': 3.4343160467425344e-05, 'epoch': 3.13}
{'loss': 0.2891, 'grad_norm': 0.730556845664978, 'learning_rate': 3.4152218742839686e-05, 'epoch': 3.17}
{'loss': 0.2883, 'grad_norm': 0.4974443316459656, 'learning_rate': 3.396127701825403e-05, 'epoch': 3.21}
{'loss': 0.2855, 'grad_norm': 8.682490348815918, 'learning_rate': 3.3770335293668376e-05, 'epoch': 3.25}
{'loss': 0.2989, 'grad_norm': 11.197065353393555, 'learning_rate': 3.357939356908272e-05, 'epoch': 3.28}
{'loss': 0.2738, 'grad_norm': 14.310378074645996, 'learning_rate': 3.338845184449706e-05, 'epoch': 3.32}
{'loss': 0.2783, 'grad_norm': 1.6761417388916016, 'learning_rate': 3.319751011991141e-05, 'epoch': 3.36}
{'loss': 0.2838, 'grad_norm': 18.89517593383789, 'learning_rate': 3.300656839532575e-05, 'epoch': 3.4}
{'loss': 0.2753, 'grad_norm': 7.051464557647705, 'learning_rate': 3.281562667074009e-05, 'epoch': 3.44}
{'loss': 0.2798, 'grad_norm': 2.351356267929077, 'learning_rate': 3.262468494615444e-05, 'epoch': 3.48}
{'loss': 0.2858, 'grad_norm': 12.30795669555664, 'learning_rate': 3.243374322156878e-05, 'epoch': 3.51}
{'loss': 0.2865, 'grad_norm': 0.5028461217880249, 'learning_rate': 3.224280149698312e-05, 'epoch': 3.55}
{'loss': 0.2952, 'grad_norm': 11.779559135437012, 'learning_rate': 3.2051859772397464e-05, 'epoch': 3.59}
{'loss': 0.2972, 'grad_norm': 5.671133995056152, 'learning_rate': 3.186091804781181e-05, 'epoch': 3.63}
{'loss': 0.2778, 'grad_norm': 8.724223136901855, 'learning_rate': 3.1669976323226154e-05, 'epoch': 3.67}
{'loss': 0.2929, 'grad_norm': 11.480586051940918, 'learning_rate': 3.1479034598640496e-05, 'epoch': 3.7}
{'loss': 0.2882, 'grad_norm': 1.007830023765564, 'learning_rate': 3.1288092874054844e-05, 'epoch': 3.74}
{'loss': 0.284, 'grad_norm': 0.8574655055999756, 'learning_rate': 3.1097151149469186e-05, 'epoch': 3.78}
{'loss': 0.2752, 'grad_norm': 1.5076205730438232, 'learning_rate': 3.090620942488353e-05, 'epoch': 3.82}
{'loss': 0.3022, 'grad_norm': 6.829051971435547, 'learning_rate': 3.0715267700297876e-05, 'epoch': 3.86}
{'loss': 0.2932, 'grad_norm': 6.490123271942139, 'learning_rate': 3.052432597571222e-05, 'epoch': 3.9}
{'loss': 0.2723, 'grad_norm': 10.65611457824707, 'learning_rate': 3.0333384251126556e-05, 'epoch': 3.93}
{'loss': 0.2881, 'grad_norm': 7.077435493469238, 'learning_rate': 3.0142442526540904e-05, 'epoch': 3.97}
{'loss': 0.2647, 'grad_norm': 11.307936668395996, 'learning_rate': 2.9951500801955246e-05, 'epoch': 4.01}
{'loss': 0.2698, 'grad_norm': 10.798332214355469, 'learning_rate': 2.9760559077369587e-05, 'epoch': 4.05}
{'loss': 0.2858, 'grad_norm': 19.23703384399414, 'learning_rate': 2.956961735278393e-05, 'epoch': 4.09}
{'loss': 0.2742, 'grad_norm': 77.78828430175781, 'learning_rate': 2.9378675628198277e-05, 'epoch': 4.12}
{'loss': 0.3, 'grad_norm': 6.048426151275635, 'learning_rate': 2.918773390361262e-05, 'epoch': 4.16}
{'loss': 0.2701, 'grad_norm': 0.38756492733955383, 'learning_rate': 2.899679217902696e-05, 'epoch': 4.2}
{'loss': 0.2846, 'grad_norm': 5.987629413604736, 'learning_rate': 2.880585045444131e-05, 'epoch': 4.24}
{'loss': 0.2805, 'grad_norm': 6.020256519317627, 'learning_rate': 2.861490872985565e-05, 'epoch': 4.28}
{'loss': 0.2882, 'grad_norm': 9.254128456115723, 'learning_rate': 2.8423967005269992e-05, 'epoch': 4.32}
{'loss': 0.2795, 'grad_norm': 1.1680750846862793, 'learning_rate': 2.8233025280684337e-05, 'epoch': 4.35}
{'loss': 0.2868, 'grad_norm': 0.49318403005599976, 'learning_rate': 2.804208355609868e-05, 'epoch': 4.39}
{'loss': 0.2723, 'grad_norm': 6.830478668212891, 'learning_rate': 2.7851141831513024e-05, 'epoch': 4.43}
{'loss': 0.2657, 'grad_norm': 8.532211303710938, 'learning_rate': 2.766020010692737e-05, 'epoch': 4.47}
{'loss': 0.2713, 'grad_norm': 1.0015873908996582, 'learning_rate': 2.746925838234171e-05, 'epoch': 4.51}
{'loss': 0.2898, 'grad_norm': 13.253573417663574, 'learning_rate': 2.7278316657756052e-05, 'epoch': 4.54}
{'loss': 0.2798, 'grad_norm': 6.748828887939453, 'learning_rate': 2.7087374933170394e-05, 'epoch': 4.58}
{'loss': 0.2711, 'grad_norm': 29.831241607666016, 'learning_rate': 2.6896433208584742e-05, 'epoch': 4.62}
{'loss': 0.2797, 'grad_norm': 19.83102798461914, 'learning_rate': 2.6705491483999084e-05, 'epoch': 4.66}
{'loss': 0.266, 'grad_norm': 1.6893765926361084, 'learning_rate': 2.6514549759413426e-05, 'epoch': 4.7}
{'loss': 0.2721, 'grad_norm': 0.3713701069355011, 'learning_rate': 2.6323608034827774e-05, 'epoch': 4.74}
{'loss': 0.2648, 'grad_norm': 0.47038987278938293, 'learning_rate': 2.6132666310242116e-05, 'epoch': 4.77}
{'loss': 0.264, 'grad_norm': 0.5744193196296692, 'learning_rate': 2.5941724585656457e-05, 'epoch': 4.81}
{'loss': 0.2826, 'grad_norm': 0.6531267762184143, 'learning_rate': 2.5750782861070806e-05, 'epoch': 4.85}
{'loss': 0.2695, 'grad_norm': 35.5915412902832, 'learning_rate': 2.5559841136485147e-05, 'epoch': 4.89}
{'loss': 0.2654, 'grad_norm': 14.998799324035645, 'learning_rate': 2.536889941189949e-05, 'epoch': 4.93}
{'loss': 0.2743, 'grad_norm': 7.502220153808594, 'learning_rate': 2.5177957687313837e-05, 'epoch': 4.96}
{'loss': 0.2644, 'grad_norm': 6.822080612182617, 'learning_rate': 2.498701596272818e-05, 'epoch': 5.0}
{'loss': 0.2576, 'grad_norm': 0.2909386157989502, 'learning_rate': 2.479607423814252e-05, 'epoch': 5.04}
{'loss': 0.2554, 'grad_norm': 0.9072278141975403, 'learning_rate': 2.4605132513556866e-05, 'epoch': 5.08}
{'loss': 0.2857, 'grad_norm': 11.183511734008789, 'learning_rate': 2.4414190788971207e-05, 'epoch': 5.12}
{'loss': 0.2696, 'grad_norm': 3.6680197715759277, 'learning_rate': 2.422324906438555e-05, 'epoch': 5.16}
{'loss': 0.2596, 'grad_norm': 0.7566215395927429, 'learning_rate': 2.4032307339799894e-05, 'epoch': 5.19}
{'loss': 0.2728, 'grad_norm': 0.8514333963394165, 'learning_rate': 2.3841365615214235e-05, 'epoch': 5.23}
{'loss': 0.2603, 'grad_norm': 23.86363410949707, 'learning_rate': 2.365042389062858e-05, 'epoch': 5.27}
{'loss': 0.2621, 'grad_norm': 16.624065399169922, 'learning_rate': 2.3459482166042925e-05, 'epoch': 5.31}
{'loss': 0.2646, 'grad_norm': 0.930060625076294, 'learning_rate': 2.3268540441457267e-05, 'epoch': 5.35}
{'loss': 0.2772, 'grad_norm': 0.3907460868358612, 'learning_rate': 2.3077598716871612e-05, 'epoch': 5.38}
{'loss': 0.2819, 'grad_norm': 7.440035820007324, 'learning_rate': 2.2886656992285954e-05, 'epoch': 5.42}
{'loss': 0.2681, 'grad_norm': 0.6544597744941711, 'learning_rate': 2.26957152677003e-05, 'epoch': 5.46}
{'loss': 0.2496, 'grad_norm': 0.7283518314361572, 'learning_rate': 2.2504773543114644e-05, 'epoch': 5.5}
{'loss': 0.2632, 'grad_norm': 21.116409301757812, 'learning_rate': 2.2313831818528985e-05, 'epoch': 5.54}
{'loss': 0.2638, 'grad_norm': 1.4588232040405273, 'learning_rate': 2.212289009394333e-05, 'epoch': 5.58}
{'loss': 0.2759, 'grad_norm': 10.075213432312012, 'learning_rate': 2.1931948369357675e-05, 'epoch': 5.61}
{'loss': 0.2616, 'grad_norm': 0.4755089282989502, 'learning_rate': 2.1741006644772017e-05, 'epoch': 5.65}
{'loss': 0.2721, 'grad_norm': 0.7745149731636047, 'learning_rate': 2.1550064920186362e-05, 'epoch': 5.69}
{'loss': 0.2557, 'grad_norm': 1.6388667821884155, 'learning_rate': 2.1359123195600704e-05, 'epoch': 5.73}
{'loss': 0.2776, 'grad_norm': 7.5517096519470215, 'learning_rate': 2.116818147101505e-05, 'epoch': 5.77}
{'loss': 0.2743, 'grad_norm': 1.433464527130127, 'learning_rate': 2.097723974642939e-05, 'epoch': 5.8}
{'loss': 0.2703, 'grad_norm': 11.849322319030762, 'learning_rate': 2.0786298021843735e-05, 'epoch': 5.84}
{'loss': 0.2639, 'grad_norm': 23.774545669555664, 'learning_rate': 2.0595356297258077e-05, 'epoch': 5.88}
{'loss': 0.2803, 'grad_norm': 21.02369499206543, 'learning_rate': 2.040441457267242e-05, 'epoch': 5.92}
{'loss': 0.2844, 'grad_norm': 14.084775924682617, 'learning_rate': 2.0213472848086764e-05, 'epoch': 5.96}
{'loss': 0.2529, 'grad_norm': 14.803247451782227, 'learning_rate': 2.002253112350111e-05, 'epoch': 6.0}
{'loss': 0.2741, 'grad_norm': 2.3276171684265137, 'learning_rate': 1.983158939891545e-05, 'epoch': 6.03}
{'loss': 0.2528, 'grad_norm': 4.612709999084473, 'learning_rate': 1.9640647674329795e-05, 'epoch': 6.07}
{'loss': 0.2592, 'grad_norm': 14.811664581298828, 'learning_rate': 1.944970594974414e-05, 'epoch': 6.11}
{'loss': 0.2659, 'grad_norm': 23.475570678710938, 'learning_rate': 1.9258764225158482e-05, 'epoch': 6.15}
{'loss': 0.2625, 'grad_norm': 23.92676544189453, 'learning_rate': 1.9067822500572827e-05, 'epoch': 6.19}
{'loss': 0.247, 'grad_norm': 8.78023624420166, 'learning_rate': 1.887688077598717e-05, 'epoch': 6.22}
{'loss': 0.2655, 'grad_norm': 7.776878833770752, 'learning_rate': 1.8685939051401514e-05, 'epoch': 6.26}
{'loss': 0.2552, 'grad_norm': 8.707993507385254, 'learning_rate': 1.849499732681586e-05, 'epoch': 6.3}
{'loss': 0.2701, 'grad_norm': 3.1587865352630615, 'learning_rate': 1.83040556022302e-05, 'epoch': 6.34}
{'loss': 0.2642, 'grad_norm': 6.084921836853027, 'learning_rate': 1.8113113877644545e-05, 'epoch': 6.38}
{'loss': 0.2606, 'grad_norm': 17.81154441833496, 'learning_rate': 1.7922172153058887e-05, 'epoch': 6.42}
{'loss': 0.2759, 'grad_norm': 8.901405334472656, 'learning_rate': 1.7731230428473232e-05, 'epoch': 6.45}
{'loss': 0.2483, 'grad_norm': 1.5383175611495972, 'learning_rate': 1.7540288703887577e-05, 'epoch': 6.49}
{'loss': 0.2669, 'grad_norm': 3.060898780822754, 'learning_rate': 1.734934697930192e-05, 'epoch': 6.53}
{'loss': 0.2666, 'grad_norm': 0.5485920906066895, 'learning_rate': 1.715840525471626e-05, 'epoch': 6.57}
{'loss': 0.2396, 'grad_norm': 0.7110373377799988, 'learning_rate': 1.6967463530130605e-05, 'epoch': 6.61}
{'loss': 0.2611, 'grad_norm': 0.6865261197090149, 'learning_rate': 1.6776521805544947e-05, 'epoch': 6.64}
{'loss': 0.2594, 'grad_norm': 16.342853546142578, 'learning_rate': 1.6585580080959292e-05, 'epoch': 6.68}
{'loss': 0.2722, 'grad_norm': 13.495363235473633, 'learning_rate': 1.6394638356373633e-05, 'epoch': 6.72}
{'loss': 0.2529, 'grad_norm': 15.806913375854492, 'learning_rate': 1.620369663178798e-05, 'epoch': 6.76}
{'loss': 0.237, 'grad_norm': 8.788561820983887, 'learning_rate': 1.6012754907202323e-05, 'epoch': 6.8}
{'loss': 0.2726, 'grad_norm': 0.16245268285274506, 'learning_rate': 1.5821813182616665e-05, 'epoch': 6.84}
{'loss': 0.2708, 'grad_norm': 10.327065467834473, 'learning_rate': 1.563087145803101e-05, 'epoch': 6.87}
{'loss': 0.2624, 'grad_norm': 3.2532734870910645, 'learning_rate': 1.543992973344535e-05, 'epoch': 6.91}
{'loss': 0.2571, 'grad_norm': 19.983203887939453, 'learning_rate': 1.5248988008859697e-05, 'epoch': 6.95}
{'loss': 0.2473, 'grad_norm': 0.5071754455566406, 'learning_rate': 1.5058046284274042e-05, 'epoch': 6.99}
{'loss': 0.2613, 'grad_norm': 0.25237005949020386, 'learning_rate': 1.4867104559688383e-05, 'epoch': 7.03}
{'loss': 0.2458, 'grad_norm': 7.927555084228516, 'learning_rate': 1.4676162835102728e-05, 'epoch': 7.06}
{'loss': 0.2442, 'grad_norm': 0.27665430307388306, 'learning_rate': 1.4485221110517072e-05, 'epoch': 7.1}
{'loss': 0.2522, 'grad_norm': 9.628432273864746, 'learning_rate': 1.4294279385931413e-05, 'epoch': 7.14}
{'loss': 0.2353, 'grad_norm': 17.736785888671875, 'learning_rate': 1.4103337661345758e-05, 'epoch': 7.18}
{'loss': 0.2502, 'grad_norm': 0.3257204294204712, 'learning_rate': 1.39123959367601e-05, 'epoch': 7.22}
{'loss': 0.2501, 'grad_norm': 8.72313117980957, 'learning_rate': 1.3721454212174445e-05, 'epoch': 7.26}
{'loss': 0.2497, 'grad_norm': 22.32418441772461, 'learning_rate': 1.353051248758879e-05, 'epoch': 7.29}
{'loss': 0.2594, 'grad_norm': 0.1810237318277359, 'learning_rate': 1.3339570763003132e-05, 'epoch': 7.33}
{'loss': 0.239, 'grad_norm': 7.246575355529785, 'learning_rate': 1.3148629038417477e-05, 'epoch': 7.37}
{'loss': 0.2442, 'grad_norm': 15.216011047363281, 'learning_rate': 1.2957687313831818e-05, 'epoch': 7.41}
{'loss': 0.2673, 'grad_norm': 0.3825300633907318, 'learning_rate': 1.2766745589246163e-05, 'epoch': 7.45}
{'loss': 0.2419, 'grad_norm': 2.9596152305603027, 'learning_rate': 1.2575803864660507e-05, 'epoch': 7.48}
{'loss': 0.2491, 'grad_norm': 5.901782989501953, 'learning_rate': 1.238486214007485e-05, 'epoch': 7.52}
{'loss': 0.2671, 'grad_norm': 0.18191255629062653, 'learning_rate': 1.2193920415489193e-05, 'epoch': 7.56}
{'loss': 0.2569, 'grad_norm': 0.5515904426574707, 'learning_rate': 1.2002978690903536e-05, 'epoch': 7.6}
{'loss': 0.2491, 'grad_norm': 0.7831212878227234, 'learning_rate': 1.181203696631788e-05, 'epoch': 7.64}
{'loss': 0.2762, 'grad_norm': 2.502971887588501, 'learning_rate': 1.1621095241732225e-05, 'epoch': 7.68}
{'loss': 0.2462, 'grad_norm': 0.2758234739303589, 'learning_rate': 1.1430153517146568e-05, 'epoch': 7.71}
{'loss': 0.2674, 'grad_norm': 5.099149703979492, 'learning_rate': 1.1239211792560911e-05, 'epoch': 7.75}
{'loss': 0.2709, 'grad_norm': 0.7387757301330566, 'learning_rate': 1.1048270067975255e-05, 'epoch': 7.79}
{'loss': 0.2709, 'grad_norm': 0.14821000397205353, 'learning_rate': 1.0857328343389598e-05, 'epoch': 7.83}
{'loss': 0.2353, 'grad_norm': 2.279017448425293, 'learning_rate': 1.0666386618803941e-05, 'epoch': 7.87}
{'loss': 0.251, 'grad_norm': 12.408669471740723, 'learning_rate': 1.0475444894218285e-05, 'epoch': 7.9}
{'loss': 0.2491, 'grad_norm': 0.6347109079360962, 'learning_rate': 1.0284503169632628e-05, 'epoch': 7.94}
{'loss': 0.2641, 'grad_norm': 1.2214909791946411, 'learning_rate': 1.0093561445046971e-05, 'epoch': 7.98}
{'loss': 0.2667, 'grad_norm': 10.686234474182129, 'learning_rate': 9.902619720461316e-06, 'epoch': 8.02}
{'loss': 0.2332, 'grad_norm': 24.197954177856445, 'learning_rate': 9.71167799587566e-06, 'epoch': 8.06}
{'loss': 0.2419, 'grad_norm': 23.34371566772461, 'learning_rate': 9.520736271290003e-06, 'epoch': 8.1}
{'loss': 0.2362, 'grad_norm': 22.76991844177246, 'learning_rate': 9.329794546704346e-06, 'epoch': 8.13}
{'loss': 0.2489, 'grad_norm': 19.1013240814209, 'learning_rate': 9.13885282211869e-06, 'epoch': 8.17}
{'loss': 0.2295, 'grad_norm': 10.437552452087402, 'learning_rate': 8.947911097533033e-06, 'epoch': 8.21}
{'loss': 0.2457, 'grad_norm': 0.577852725982666, 'learning_rate': 8.756969372947376e-06, 'epoch': 8.25}
{'loss': 0.2481, 'grad_norm': 2.0015132427215576, 'learning_rate': 8.56602764836172e-06, 'epoch': 8.29}
{'loss': 0.2498, 'grad_norm': 8.707850456237793, 'learning_rate': 8.375085923776065e-06, 'epoch': 8.33}
{'loss': 0.2531, 'grad_norm': 20.52143669128418, 'learning_rate': 8.184144199190408e-06, 'epoch': 8.36}
{'loss': 0.2526, 'grad_norm': 6.061394691467285, 'learning_rate': 7.993202474604751e-06, 'epoch': 8.4}
{'loss': 0.2479, 'grad_norm': 9.667651176452637, 'learning_rate': 7.802260750019095e-06, 'epoch': 8.44}
{'loss': 0.2374, 'grad_norm': 1.9141350984573364, 'learning_rate': 7.611319025433437e-06, 'epoch': 8.48}
{'loss': 0.2589, 'grad_norm': 24.073055267333984, 'learning_rate': 7.420377300847782e-06, 'epoch': 8.52}
{'loss': 0.2481, 'grad_norm': 0.17763416469097137, 'learning_rate': 7.2294355762621254e-06, 'epoch': 8.55}
{'loss': 0.2575, 'grad_norm': 0.495648592710495, 'learning_rate': 7.038493851676469e-06, 'epoch': 8.59}
{'loss': 0.2281, 'grad_norm': 0.2598142921924591, 'learning_rate': 6.847552127090811e-06, 'epoch': 8.63}
{'loss': 0.2549, 'grad_norm': 32.1843147277832, 'learning_rate': 6.656610402505156e-06, 'epoch': 8.67}
{'loss': 0.2535, 'grad_norm': 6.908284664154053, 'learning_rate': 6.4656686779194996e-06, 'epoch': 8.71}
{'loss': 0.2674, 'grad_norm': 16.176288604736328, 'learning_rate': 6.274726953333843e-06, 'epoch': 8.75}
{'loss': 0.2411, 'grad_norm': 9.31972885131836, 'learning_rate': 6.083785228748186e-06, 'epoch': 8.78}
{'loss': 0.2401, 'grad_norm': 0.7511228322982788, 'learning_rate': 5.8928435041625295e-06, 'epoch': 8.82}
{'loss': 0.2266, 'grad_norm': 0.5531994104385376, 'learning_rate': 5.701901779576874e-06, 'epoch': 8.86}
{'loss': 0.2443, 'grad_norm': 0.46248698234558105, 'learning_rate': 5.510960054991217e-06, 'epoch': 8.9}
{'loss': 0.2515, 'grad_norm': 8.603412628173828, 'learning_rate': 5.320018330405561e-06, 'epoch': 8.94}
{'loss': 0.2487, 'grad_norm': 0.5210448503494263, 'learning_rate': 5.129076605819904e-06, 'epoch': 8.97}
{'loss': 0.2455, 'grad_norm': 13.059374809265137, 'learning_rate': 4.938134881234248e-06, 'epoch': 9.01}
{'loss': 0.2211, 'grad_norm': 0.21990182995796204, 'learning_rate': 4.747193156648591e-06, 'epoch': 9.05}
{'loss': 0.2661, 'grad_norm': 17.909042358398438, 'learning_rate': 4.5562514320629344e-06, 'epoch': 9.09}
{'loss': 0.2348, 'grad_norm': 6.518310070037842, 'learning_rate': 4.365309707477279e-06, 'epoch': 9.13}
{'loss': 0.246, 'grad_norm': 0.22904658317565918, 'learning_rate': 4.174367982891621e-06, 'epoch': 9.17}
{'loss': 0.2325, 'grad_norm': 29.384719848632812, 'learning_rate': 3.983426258305965e-06, 'epoch': 9.2}
{'loss': 0.2584, 'grad_norm': 6.292439937591553, 'learning_rate': 3.7924845337203086e-06, 'epoch': 9.24}
{'loss': 0.2424, 'grad_norm': 0.28315114974975586, 'learning_rate': 3.6015428091346523e-06, 'epoch': 9.28}
{'loss': 0.2341, 'grad_norm': 0.668682336807251, 'learning_rate': 3.4106010845489956e-06, 'epoch': 9.32}
{'loss': 0.2598, 'grad_norm': 0.11372780054807663, 'learning_rate': 3.2196593599633394e-06, 'epoch': 9.36}
{'loss': 0.2417, 'grad_norm': 11.242597579956055, 'learning_rate': 3.0287176353776827e-06, 'epoch': 9.39}
{'loss': 0.2636, 'grad_norm': 0.6659205555915833, 'learning_rate': 2.8377759107920264e-06, 'epoch': 9.43}
{'loss': 0.258, 'grad_norm': 12.812040328979492, 'learning_rate': 2.6468341862063698e-06, 'epoch': 9.47}
{'loss': 0.2428, 'grad_norm': 0.230610191822052, 'learning_rate': 2.4558924616207135e-06, 'epoch': 9.51}
{'loss': 0.2622, 'grad_norm': 0.4154537618160248, 'learning_rate': 2.264950737035057e-06, 'epoch': 9.55}
{'loss': 0.2497, 'grad_norm': 0.44310805201530457, 'learning_rate': 2.0740090124494006e-06, 'epoch': 9.59}
{'loss': 0.2312, 'grad_norm': 12.139286041259766, 'learning_rate': 1.883067287863744e-06, 'epoch': 9.62}
{'loss': 0.2247, 'grad_norm': 0.5629482269287109, 'learning_rate': 1.6921255632780876e-06, 'epoch': 9.66}
{'loss': 0.2228, 'grad_norm': 29.848526000976562, 'learning_rate': 1.501183838692431e-06, 'epoch': 9.7}
{'loss': 0.2481, 'grad_norm': 9.266294479370117, 'learning_rate': 1.3102421141067747e-06, 'epoch': 9.74}
{'loss': 0.2224, 'grad_norm': 17.08319854736328, 'learning_rate': 1.1193003895211182e-06, 'epoch': 9.78}
{'loss': 0.2354, 'grad_norm': 24.7939510345459, 'learning_rate': 9.283586649354618e-07, 'epoch': 9.81}
{'loss': 0.2804, 'grad_norm': 0.6001126766204834, 'learning_rate': 7.374169403498053e-07, 'epoch': 9.85}
{'loss': 0.2382, 'grad_norm': 104.6762466430664, 'learning_rate': 5.464752157641488e-07, 'epoch': 9.89}
{'loss': 0.2576, 'grad_norm': 8.571264266967773, 'learning_rate': 3.5553349117849236e-07, 'epoch': 9.93}
{'loss': 0.2199, 'grad_norm': 30.01688003540039, 'learning_rate': 1.6459176659283587e-07, 'epoch': 9.97}
{'train_runtime': 20052.9214, 'train_samples_per_second': 52.233, 'train_steps_per_second': 6.529, 'train_loss': 0.28238448816979894, 'epoch': 10.0}
***** train metrics *****
  epoch                    =        10.0
  total_flos               = 227717064GF
  train_loss               =      0.2824
  train_runtime            =  5:34:12.92
  train_samples            =      104743
  train_samples_per_second =      52.233
  train_steps_per_second   =       6.529
08/07/2025 01:55:57 - INFO - __main__ - *** Evaluate ***
***** eval metrics *****
  epoch                   =       10.0
  eval_accuracy           =     0.9218
  eval_loss               =     0.3041
  eval_runtime            = 0:00:46.30
  eval_samples            =       5463
  eval_samples_per_second =    117.981
  eval_steps_per_second   =      14.75
08/07/2025 01:56:59 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
08/07/2025 01:56:59 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_revision=None,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
liger_kernel_config=None,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_final/BERT_large/QNLI/InA90/runs/Aug07_01-56-59_n29,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=10.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=output_final/BERT_large/QNLI/InA90/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=output_final/BERT_large/QNLI/InA90/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
08/07/2025 01:57:01 - INFO - datasets.builder - Found cached dataset glue (/home/kangchen/.cache/huggingface/datasets/nyu-mll___glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
The task type of PEFT is not proper!
LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='bert-large-cased', revision=None, task_type=<TaskType.SEQ_CLS: 'SEQ_CLS'>, inference_mode=False, r=4, target_modules={'value', 'key', 'query'}, lora_inhibition=0.9, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
###############
PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): BertForSequenceClassification(
      (bert): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 1024, padding_idx=0)
          (position_embeddings): Embedding(512, 1024)
          (token_type_embeddings): Embedding(2, 1024)
          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0-23): 24 x BertLayer(
              (attention): BertAttention(
                (self): BertSdpaSelfAttention(
                  (query): lora.Linear(
                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=4, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=4, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (key): lora.Linear(
                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=4, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=4, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (value): lora.Linear(
                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.1, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=1024, out_features=4, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=4, out_features=1024, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=1024, out_features=1024, bias=True)
                  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=1024, out_features=4096, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): BertOutput(
                (dense): Linear(in_features=4096, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=1024, out_features=1024, bias=True)
          (activation): Tanh()
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (classifier): ModulesToSaveWrapper(
        (original_module): Linear(in_features=1024, out_features=2, bias=True)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=1024, out_features=2, bias=True)
        )
      )
    )
  )
)
08/07/2025 01:57:04 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/kangchen/.cache/huggingface/datasets/nyu-mll___glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-857a5657ab1232c9_*_of_00001.arrow
08/07/2025 01:57:04 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/kangchen/.cache/huggingface/datasets/nyu-mll___glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-32484430d63833d1_*_of_00001.arrow
08/07/2025 01:57:04 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/kangchen/.cache/huggingface/datasets/nyu-mll___glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4b92e6513769a5d9_*_of_00001.arrow
08/07/2025 01:57:05 - INFO - __main__ - Class distribution in train set:
08/07/2025 01:57:05 - INFO - __main__ -   Label 1: 52366 (49.99%)
08/07/2025 01:57:05 - INFO - __main__ -   Label 0: 52377 (50.01%)
08/07/2025 01:57:05 - INFO - __main__ - Class distribution in validation set:
08/07/2025 01:57:05 - INFO - __main__ -   Label 0: 2702 (49.46%)
08/07/2025 01:57:05 - INFO - __main__ -   Label 1: 2761 (50.54%)
08/07/2025 01:57:05 - INFO - __main__ - Class distribution in test set:
08/07/2025 01:57:05 - INFO - __main__ -   Label -1: 5463 (100.00%)
08/07/2025 01:57:05 - INFO - __main__ - Sample 83810 of the training set: {'question': 'What tactics of the PVA were US and ROK troops not prepared to handle?', 'sentence': "On 27 November at the Korean eastern front, a U.S. 7th Infantry Division Regimental Combat Team (3,000 soldiers) and the U.S. 1st Marine Division (12,000–15,000 marines) were unprepared for the PVA 9th Army Group's three-pronged encirclement tactics at the Battle of Chosin Reservoir, but they managed to escape under Air Force and X Corps support fire—albeit with some 15,000 collective casualties.", 'label': 0, 'idx': 83810, 'input_ids': [101, 1327, 10524, 1104, 1103, 153, 12152, 1127, 1646, 1105, 155, 22027, 2830, 1136, 4029, 1106, 4282, 136, 102, 1212, 1765, 1379, 1120, 1103, 3947, 2638, 1524, 117, 170, 158, 119, 156, 119, 4766, 4155, 1784, 3391, 1348, 10314, 2649, 113, 124, 117, 1288, 2803, 114, 1105, 1103, 158, 119, 156, 119, 2198, 4620, 1784, 113, 1367, 117, 1288, 782, 1405, 117, 1288, 5243, 1116, 114, 1127, 8362, 1643, 1874, 17482, 1174, 1111, 1103, 153, 12152, 5612, 1740, 1990, 112, 188, 1210, 118, 5250, 18288, 4035, 6617, 24490, 1880, 10524, 1120, 1103, 2651, 1104, 22964, 10606, 15220, 117, 1133, 1152, 2374, 1106, 3359, 1223, 1806, 2300, 1105, 161, 3158, 1619, 1783, 783, 12456, 1114, 1199, 1405, 117, 1288, 7764, 8487, 119, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}.
08/07/2025 01:57:05 - INFO - __main__ - Sample 14592 of the training set: {'question': "How did the Egyptian people feel about Nasser's response to the attack?", 'sentence': 'Nasser did not feel that the Egyptian Army was ready for a confrontation and did not retaliate militarily.', 'label': 1, 'idx': 14592, 'input_ids': [101, 1731, 1225, 1103, 6210, 1234, 1631, 1164, 11896, 14607, 112, 188, 2593, 1106, 1103, 2035, 136, 102, 11896, 14607, 1225, 1136, 1631, 1115, 1103, 6210, 1740, 1108, 2407, 1111, 170, 14002, 1105, 1225, 1136, 1231, 6163, 15045, 1940, 12888, 18206, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
08/07/2025 01:57:05 - INFO - __main__ - Sample 3278 of the training set: {'question': 'What kind of aircraft are capable of taking off vertically?', 'sentence': 'Although STOVL aircraft are capable of taking off vertically from a spot on the deck, using the ramp and a running start is far more fuel efficient and permits a heavier launch weight.', 'label': 0, 'idx': 3278, 'input_ids': [101, 1327, 1912, 1104, 2163, 1132, 4451, 1104, 1781, 1228, 22026, 136, 102, 1966, 23676, 2346, 2559, 2162, 2163, 1132, 4451, 1104, 1781, 1228, 22026, 1121, 170, 3205, 1113, 1103, 5579, 117, 1606, 1103, 14207, 1105, 170, 1919, 1838, 1110, 1677, 1167, 4251, 7856, 1105, 15267, 170, 12163, 4286, 2841, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
08/07/2025 01:57:06 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.663, 'grad_norm': 9.949535369873047, 'learning_rate': 4.980944015886352e-05, 'epoch': 0.04}
{'loss': 0.6166, 'grad_norm': 2.516079902648926, 'learning_rate': 4.961849843427786e-05, 'epoch': 0.08}
{'loss': 0.5696, 'grad_norm': 3.8968822956085205, 'learning_rate': 4.94275567096922e-05, 'epoch': 0.11}
{'loss': 0.5182, 'grad_norm': 11.839781761169434, 'learning_rate': 4.923661498510655e-05, 'epoch': 0.15}
{'loss': 0.478, 'grad_norm': 6.640523433685303, 'learning_rate': 4.904567326052089e-05, 'epoch': 0.19}
{'loss': 0.4684, 'grad_norm': 6.885077476501465, 'learning_rate': 4.885473153593523e-05, 'epoch': 0.23}
{'loss': 0.4463, 'grad_norm': 1.1206886768341064, 'learning_rate': 4.866378981134958e-05, 'epoch': 0.27}
{'loss': 0.4404, 'grad_norm': 5.353808403015137, 'learning_rate': 4.847284808676392e-05, 'epoch': 0.31}
{'loss': 0.4272, 'grad_norm': 4.173153400421143, 'learning_rate': 4.828190636217826e-05, 'epoch': 0.34}
{'loss': 0.4231, 'grad_norm': 4.18876838684082, 'learning_rate': 4.809096463759261e-05, 'epoch': 0.38}
{'loss': 0.4145, 'grad_norm': 1.1701053380966187, 'learning_rate': 4.790002291300695e-05, 'epoch': 0.42}
{'loss': 0.4282, 'grad_norm': 4.161426544189453, 'learning_rate': 4.7709081188421295e-05, 'epoch': 0.46}
{'loss': 0.413, 'grad_norm': 2.539112091064453, 'learning_rate': 4.7518139463835637e-05, 'epoch': 0.5}
{'loss': 0.4117, 'grad_norm': 2.629514217376709, 'learning_rate': 4.7327197739249985e-05, 'epoch': 0.53}
{'loss': 0.4017, 'grad_norm': 3.740987539291382, 'learning_rate': 4.713625601466433e-05, 'epoch': 0.57}
{'loss': 0.4078, 'grad_norm': 5.879226207733154, 'learning_rate': 4.694531429007867e-05, 'epoch': 0.61}
{'loss': 0.4131, 'grad_norm': 5.121507167816162, 'learning_rate': 4.675437256549302e-05, 'epoch': 0.65}
{'loss': 0.3976, 'grad_norm': 4.009472370147705, 'learning_rate': 4.656343084090736e-05, 'epoch': 0.69}
{'loss': 0.3858, 'grad_norm': 2.691352128982544, 'learning_rate': 4.63724891163217e-05, 'epoch': 0.73}
{'loss': 0.3877, 'grad_norm': 6.846001148223877, 'learning_rate': 4.618154739173605e-05, 'epoch': 0.76}
{'loss': 0.3873, 'grad_norm': 8.1541748046875, 'learning_rate': 4.599060566715039e-05, 'epoch': 0.8}
{'loss': 0.3822, 'grad_norm': 2.5346784591674805, 'learning_rate': 4.579966394256473e-05, 'epoch': 0.84}
{'loss': 0.3742, 'grad_norm': 12.674993515014648, 'learning_rate': 4.560872221797908e-05, 'epoch': 0.88}
{'loss': 0.379, 'grad_norm': 2.418828010559082, 'learning_rate': 4.541778049339342e-05, 'epoch': 0.92}
{'loss': 0.3874, 'grad_norm': 4.13533353805542, 'learning_rate': 4.522683876880776e-05, 'epoch': 0.95}
{'loss': 0.3823, 'grad_norm': 0.8415079712867737, 'learning_rate': 4.5035897044222105e-05, 'epoch': 0.99}
{'loss': 0.3739, 'grad_norm': 4.321774005889893, 'learning_rate': 4.484495531963645e-05, 'epoch': 1.03}
{'loss': 0.3809, 'grad_norm': 5.012912273406982, 'learning_rate': 4.4654013595050795e-05, 'epoch': 1.07}
{'loss': 0.3729, 'grad_norm': 6.443814277648926, 'learning_rate': 4.4463071870465137e-05, 'epoch': 1.11}
{'loss': 0.3674, 'grad_norm': 6.755543231964111, 'learning_rate': 4.427213014587948e-05, 'epoch': 1.15}
{'loss': 0.3689, 'grad_norm': 4.646345615386963, 'learning_rate': 4.408118842129382e-05, 'epoch': 1.18}
{'loss': 0.3474, 'grad_norm': 3.759371280670166, 'learning_rate': 4.389024669670817e-05, 'epoch': 1.22}
{'loss': 0.354, 'grad_norm': 1.0179038047790527, 'learning_rate': 4.369930497212251e-05, 'epoch': 1.26}
{'loss': 0.3706, 'grad_norm': 4.706476211547852, 'learning_rate': 4.350836324753685e-05, 'epoch': 1.3}
{'loss': 0.3682, 'grad_norm': 5.905998706817627, 'learning_rate': 4.331742152295119e-05, 'epoch': 1.34}
{'loss': 0.3634, 'grad_norm': 1.1783790588378906, 'learning_rate': 4.312647979836554e-05, 'epoch': 1.37}
{'loss': 0.3539, 'grad_norm': 4.629815578460693, 'learning_rate': 4.293553807377988e-05, 'epoch': 1.41}
{'loss': 0.3497, 'grad_norm': 0.612770676612854, 'learning_rate': 4.2744596349194225e-05, 'epoch': 1.45}
{'loss': 0.3403, 'grad_norm': 4.745299339294434, 'learning_rate': 4.2553654624608566e-05, 'epoch': 1.49}
{'loss': 0.3599, 'grad_norm': 4.0224809646606445, 'learning_rate': 4.2362712900022915e-05, 'epoch': 1.53}
{'loss': 0.3535, 'grad_norm': 2.1696994304656982, 'learning_rate': 4.2171771175437256e-05, 'epoch': 1.57}
{'loss': 0.3667, 'grad_norm': 5.241518974304199, 'learning_rate': 4.19808294508516e-05, 'epoch': 1.6}
{'loss': 0.3463, 'grad_norm': 5.956056118011475, 'learning_rate': 4.1789887726265946e-05, 'epoch': 1.64}
{'loss': 0.347, 'grad_norm': 8.432271957397461, 'learning_rate': 4.159894600168029e-05, 'epoch': 1.68}
{'loss': 0.3533, 'grad_norm': 3.981483221054077, 'learning_rate': 4.140800427709463e-05, 'epoch': 1.72}
{'loss': 0.352, 'grad_norm': 3.928776502609253, 'learning_rate': 4.121706255250898e-05, 'epoch': 1.76}
{'loss': 0.3472, 'grad_norm': 1.6580673456192017, 'learning_rate': 4.102612082792332e-05, 'epoch': 1.79}
{'loss': 0.3447, 'grad_norm': 16.058053970336914, 'learning_rate': 4.083517910333766e-05, 'epoch': 1.83}
{'loss': 0.3465, 'grad_norm': 5.435301780700684, 'learning_rate': 4.064423737875201e-05, 'epoch': 1.87}
{'loss': 0.3427, 'grad_norm': 12.630341529846191, 'learning_rate': 4.045329565416635e-05, 'epoch': 1.91}
{'loss': 0.3404, 'grad_norm': 6.982361316680908, 'learning_rate': 4.026235392958069e-05, 'epoch': 1.95}
{'loss': 0.3333, 'grad_norm': 2.354762554168701, 'learning_rate': 4.0071412204995035e-05, 'epoch': 1.99}
{'loss': 0.3531, 'grad_norm': 4.29294490814209, 'learning_rate': 3.988047048040938e-05, 'epoch': 2.02}
{'loss': 0.321, 'grad_norm': 11.352030754089355, 'learning_rate': 3.9689528755823725e-05, 'epoch': 2.06}
{'loss': 0.3397, 'grad_norm': 4.7919182777404785, 'learning_rate': 3.9498587031238066e-05, 'epoch': 2.1}
{'loss': 0.3421, 'grad_norm': 5.161870002746582, 'learning_rate': 3.9307645306652415e-05, 'epoch': 2.14}
{'loss': 0.3405, 'grad_norm': 12.826974868774414, 'learning_rate': 3.9116703582066756e-05, 'epoch': 2.18}
{'loss': 0.3302, 'grad_norm': 4.332046031951904, 'learning_rate': 3.89257618574811e-05, 'epoch': 2.21}
{'loss': 0.3291, 'grad_norm': 1.1299570798873901, 'learning_rate': 3.8734820132895446e-05, 'epoch': 2.25}
{'loss': 0.3123, 'grad_norm': 7.238608360290527, 'learning_rate': 3.854387840830979e-05, 'epoch': 2.29}
{'loss': 0.3403, 'grad_norm': 5.297704696655273, 'learning_rate': 3.835293668372413e-05, 'epoch': 2.33}
{'loss': 0.3202, 'grad_norm': 3.9493842124938965, 'learning_rate': 3.816199495913848e-05, 'epoch': 2.37}
{'loss': 0.3358, 'grad_norm': 2.371035099029541, 'learning_rate': 3.797105323455282e-05, 'epoch': 2.41}
{'loss': 0.3533, 'grad_norm': 7.308608531951904, 'learning_rate': 3.778011150996716e-05, 'epoch': 2.44}
{'loss': 0.3262, 'grad_norm': 4.075193405151367, 'learning_rate': 3.75891697853815e-05, 'epoch': 2.48}
{'loss': 0.3383, 'grad_norm': 8.797531127929688, 'learning_rate': 3.739822806079585e-05, 'epoch': 2.52}
{'loss': 0.3122, 'grad_norm': 1.5847524404525757, 'learning_rate': 3.720728633621019e-05, 'epoch': 2.56}
{'loss': 0.3445, 'grad_norm': 8.510970115661621, 'learning_rate': 3.7016344611624534e-05, 'epoch': 2.6}
{'loss': 0.3381, 'grad_norm': 4.826423645019531, 'learning_rate': 3.6825402887038876e-05, 'epoch': 2.63}
{'loss': 0.3273, 'grad_norm': 9.054593086242676, 'learning_rate': 3.663446116245322e-05, 'epoch': 2.67}
{'loss': 0.3334, 'grad_norm': 8.667019844055176, 'learning_rate': 3.644351943786756e-05, 'epoch': 2.71}
{'loss': 0.3362, 'grad_norm': 6.7379631996154785, 'learning_rate': 3.625257771328191e-05, 'epoch': 2.75}
{'loss': 0.3343, 'grad_norm': 6.624180316925049, 'learning_rate': 3.606163598869625e-05, 'epoch': 2.79}
{'loss': 0.3251, 'grad_norm': 11.94194221496582, 'learning_rate': 3.587069426411059e-05, 'epoch': 2.83}
{'loss': 0.3368, 'grad_norm': 3.102813482284546, 'learning_rate': 3.567975253952494e-05, 'epoch': 2.86}
{'loss': 0.3269, 'grad_norm': 10.842575073242188, 'learning_rate': 3.548881081493928e-05, 'epoch': 2.9}
{'loss': 0.3325, 'grad_norm': 8.727618217468262, 'learning_rate': 3.529786909035362e-05, 'epoch': 2.94}
{'loss': 0.3255, 'grad_norm': 3.3137190341949463, 'learning_rate': 3.510692736576797e-05, 'epoch': 2.98}
{'loss': 0.3424, 'grad_norm': 1.5349363088607788, 'learning_rate': 3.491598564118231e-05, 'epoch': 3.02}
{'loss': 0.3279, 'grad_norm': 1.6797438859939575, 'learning_rate': 3.4725043916596654e-05, 'epoch': 3.06}
{'loss': 0.3323, 'grad_norm': 5.136842250823975, 'learning_rate': 3.4534102192010996e-05, 'epoch': 3.09}
{'loss': 0.3215, 'grad_norm': 8.994490623474121, 'learning_rate': 3.4343160467425344e-05, 'epoch': 3.13}
{'loss': 0.343, 'grad_norm': 2.1333088874816895, 'learning_rate': 3.4152218742839686e-05, 'epoch': 3.17}
{'loss': 0.3131, 'grad_norm': 0.8457027077674866, 'learning_rate': 3.396127701825403e-05, 'epoch': 3.21}
{'loss': 0.3137, 'grad_norm': 8.931843757629395, 'learning_rate': 3.3770335293668376e-05, 'epoch': 3.25}
{'loss': 0.3273, 'grad_norm': 9.17138957977295, 'learning_rate': 3.357939356908272e-05, 'epoch': 3.28}
{'loss': 0.3145, 'grad_norm': 14.429389953613281, 'learning_rate': 3.338845184449706e-05, 'epoch': 3.32}
{'loss': 0.3239, 'grad_norm': 4.151413440704346, 'learning_rate': 3.319751011991141e-05, 'epoch': 3.36}
{'loss': 0.3238, 'grad_norm': 8.100569725036621, 'learning_rate': 3.300656839532575e-05, 'epoch': 3.4}
{'loss': 0.3063, 'grad_norm': 5.404658317565918, 'learning_rate': 3.281562667074009e-05, 'epoch': 3.44}
{'loss': 0.3155, 'grad_norm': 0.944146454334259, 'learning_rate': 3.262468494615444e-05, 'epoch': 3.48}
{'loss': 0.3197, 'grad_norm': 7.272260665893555, 'learning_rate': 3.243374322156878e-05, 'epoch': 3.51}
{'loss': 0.3243, 'grad_norm': 1.3460116386413574, 'learning_rate': 3.224280149698312e-05, 'epoch': 3.55}
{'loss': 0.3288, 'grad_norm': 6.68721866607666, 'learning_rate': 3.2051859772397464e-05, 'epoch': 3.59}
{'loss': 0.3311, 'grad_norm': 7.454747200012207, 'learning_rate': 3.186091804781181e-05, 'epoch': 3.63}
{'loss': 0.3048, 'grad_norm': 1.0401543378829956, 'learning_rate': 3.1669976323226154e-05, 'epoch': 3.67}
{'loss': 0.3203, 'grad_norm': 7.2841410636901855, 'learning_rate': 3.1479034598640496e-05, 'epoch': 3.7}
{'loss': 0.3181, 'grad_norm': 10.196657180786133, 'learning_rate': 3.1288092874054844e-05, 'epoch': 3.74}
{'loss': 0.3207, 'grad_norm': 1.0446057319641113, 'learning_rate': 3.1097151149469186e-05, 'epoch': 3.78}
{'loss': 0.3239, 'grad_norm': 5.6081438064575195, 'learning_rate': 3.090620942488353e-05, 'epoch': 3.82}
{'loss': 0.3281, 'grad_norm': 6.836927890777588, 'learning_rate': 3.0715267700297876e-05, 'epoch': 3.86}
{'loss': 0.3352, 'grad_norm': 4.588394641876221, 'learning_rate': 3.052432597571222e-05, 'epoch': 3.9}
{'loss': 0.3007, 'grad_norm': 5.530942916870117, 'learning_rate': 3.0333384251126556e-05, 'epoch': 3.93}
{'loss': 0.3165, 'grad_norm': 7.364265441894531, 'learning_rate': 3.0142442526540904e-05, 'epoch': 3.97}
{'loss': 0.2945, 'grad_norm': 6.68869686126709, 'learning_rate': 2.9951500801955246e-05, 'epoch': 4.01}
{'loss': 0.313, 'grad_norm': 3.205108880996704, 'learning_rate': 2.9760559077369587e-05, 'epoch': 4.05}
{'loss': 0.3265, 'grad_norm': 8.344220161437988, 'learning_rate': 2.956961735278393e-05, 'epoch': 4.09}
{'loss': 0.3124, 'grad_norm': 11.553081512451172, 'learning_rate': 2.9378675628198277e-05, 'epoch': 4.12}
{'loss': 0.3272, 'grad_norm': 4.898372650146484, 'learning_rate': 2.918773390361262e-05, 'epoch': 4.16}
{'loss': 0.291, 'grad_norm': 10.123993873596191, 'learning_rate': 2.899679217902696e-05, 'epoch': 4.2}
{'loss': 0.331, 'grad_norm': 3.197312355041504, 'learning_rate': 2.880585045444131e-05, 'epoch': 4.24}
{'loss': 0.3094, 'grad_norm': 4.677900314331055, 'learning_rate': 2.861490872985565e-05, 'epoch': 4.28}
{'loss': 0.3181, 'grad_norm': 8.409811973571777, 'learning_rate': 2.8423967005269992e-05, 'epoch': 4.32}
{'loss': 0.3356, 'grad_norm': 4.802584171295166, 'learning_rate': 2.8233025280684337e-05, 'epoch': 4.35}
{'loss': 0.3235, 'grad_norm': 0.8703061938285828, 'learning_rate': 2.804208355609868e-05, 'epoch': 4.39}
{'loss': 0.2979, 'grad_norm': 5.27825927734375, 'learning_rate': 2.7851141831513024e-05, 'epoch': 4.43}
{'loss': 0.2953, 'grad_norm': 5.501797676086426, 'learning_rate': 2.766020010692737e-05, 'epoch': 4.47}
{'loss': 0.3103, 'grad_norm': 8.261566162109375, 'learning_rate': 2.746925838234171e-05, 'epoch': 4.51}
{'loss': 0.3231, 'grad_norm': 13.326375007629395, 'learning_rate': 2.7278316657756052e-05, 'epoch': 4.54}
{'loss': 0.3005, 'grad_norm': 4.1786298751831055, 'learning_rate': 2.7087374933170394e-05, 'epoch': 4.58}
{'loss': 0.3059, 'grad_norm': 2.9035542011260986, 'learning_rate': 2.6896433208584742e-05, 'epoch': 4.62}
{'loss': 0.3117, 'grad_norm': 7.716691493988037, 'learning_rate': 2.6705491483999084e-05, 'epoch': 4.66}
{'loss': 0.3093, 'grad_norm': 5.183434009552002, 'learning_rate': 2.6514549759413426e-05, 'epoch': 4.7}
{'loss': 0.306, 'grad_norm': 0.9080737829208374, 'learning_rate': 2.6323608034827774e-05, 'epoch': 4.74}
{'loss': 0.2957, 'grad_norm': 1.1182013750076294, 'learning_rate': 2.6132666310242116e-05, 'epoch': 4.77}
{'loss': 0.3001, 'grad_norm': 4.169547080993652, 'learning_rate': 2.5941724585656457e-05, 'epoch': 4.81}
{'loss': 0.3077, 'grad_norm': 1.3549491167068481, 'learning_rate': 2.5750782861070806e-05, 'epoch': 4.85}
{'loss': 0.2971, 'grad_norm': 9.07935905456543, 'learning_rate': 2.5559841136485147e-05, 'epoch': 4.89}
{'loss': 0.2919, 'grad_norm': 5.150195598602295, 'learning_rate': 2.536889941189949e-05, 'epoch': 4.93}
{'loss': 0.3122, 'grad_norm': 6.598069667816162, 'learning_rate': 2.5177957687313837e-05, 'epoch': 4.96}
{'loss': 0.2934, 'grad_norm': 7.93243932723999, 'learning_rate': 2.498701596272818e-05, 'epoch': 5.0}
{'loss': 0.3109, 'grad_norm': 0.6917842030525208, 'learning_rate': 2.479607423814252e-05, 'epoch': 5.04}
{'loss': 0.303, 'grad_norm': 11.39120864868164, 'learning_rate': 2.4605132513556866e-05, 'epoch': 5.08}
{'loss': 0.3261, 'grad_norm': 4.579926490783691, 'learning_rate': 2.4414190788971207e-05, 'epoch': 5.12}
{'loss': 0.3085, 'grad_norm': 5.116159439086914, 'learning_rate': 2.422324906438555e-05, 'epoch': 5.16}
{'loss': 0.299, 'grad_norm': 3.877734661102295, 'learning_rate': 2.4032307339799894e-05, 'epoch': 5.19}
{'loss': 0.303, 'grad_norm': 1.3029069900512695, 'learning_rate': 2.3841365615214235e-05, 'epoch': 5.23}
{'loss': 0.2968, 'grad_norm': 7.371504306793213, 'learning_rate': 2.365042389062858e-05, 'epoch': 5.27}
{'loss': 0.3088, 'grad_norm': 0.9687303304672241, 'learning_rate': 2.3459482166042925e-05, 'epoch': 5.31}
{'loss': 0.296, 'grad_norm': 6.211594104766846, 'learning_rate': 2.3268540441457267e-05, 'epoch': 5.35}
{'loss': 0.3131, 'grad_norm': 0.8960598707199097, 'learning_rate': 2.3077598716871612e-05, 'epoch': 5.38}
{'loss': 0.313, 'grad_norm': 5.5377302169799805, 'learning_rate': 2.2886656992285954e-05, 'epoch': 5.42}
{'loss': 0.2996, 'grad_norm': 4.843117713928223, 'learning_rate': 2.26957152677003e-05, 'epoch': 5.46}
{'loss': 0.2947, 'grad_norm': 0.8328416347503662, 'learning_rate': 2.2504773543114644e-05, 'epoch': 5.5}
{'loss': 0.299, 'grad_norm': 7.357219219207764, 'learning_rate': 2.2313831818528985e-05, 'epoch': 5.54}
{'loss': 0.2987, 'grad_norm': 1.1003167629241943, 'learning_rate': 2.212289009394333e-05, 'epoch': 5.58}
{'loss': 0.3158, 'grad_norm': 4.80298376083374, 'learning_rate': 2.1931948369357675e-05, 'epoch': 5.61}
{'loss': 0.3159, 'grad_norm': 1.0065211057662964, 'learning_rate': 2.1741006644772017e-05, 'epoch': 5.65}
{'loss': 0.3007, 'grad_norm': 3.5976459980010986, 'learning_rate': 2.1550064920186362e-05, 'epoch': 5.69}
{'loss': 0.2823, 'grad_norm': 9.15860366821289, 'learning_rate': 2.1359123195600704e-05, 'epoch': 5.73}
{'loss': 0.3134, 'grad_norm': 8.92285442352295, 'learning_rate': 2.116818147101505e-05, 'epoch': 5.77}
{'loss': 0.311, 'grad_norm': 1.5882797241210938, 'learning_rate': 2.097723974642939e-05, 'epoch': 5.8}
{'loss': 0.3001, 'grad_norm': 6.396711826324463, 'learning_rate': 2.0786298021843735e-05, 'epoch': 5.84}
{'loss': 0.2871, 'grad_norm': 4.903302192687988, 'learning_rate': 2.0595356297258077e-05, 'epoch': 5.88}
{'loss': 0.3088, 'grad_norm': 13.866966247558594, 'learning_rate': 2.040441457267242e-05, 'epoch': 5.92}
{'loss': 0.3144, 'grad_norm': 6.376019477844238, 'learning_rate': 2.0213472848086764e-05, 'epoch': 5.96}
{'loss': 0.2846, 'grad_norm': 12.099715232849121, 'learning_rate': 2.002253112350111e-05, 'epoch': 6.0}
{'loss': 0.315, 'grad_norm': 4.8461456298828125, 'learning_rate': 1.983158939891545e-05, 'epoch': 6.03}
{'loss': 0.2944, 'grad_norm': 6.220055103302002, 'learning_rate': 1.9640647674329795e-05, 'epoch': 6.07}
{'loss': 0.2918, 'grad_norm': 12.4039306640625, 'learning_rate': 1.944970594974414e-05, 'epoch': 6.11}
{'loss': 0.3033, 'grad_norm': 3.0065455436706543, 'learning_rate': 1.9258764225158482e-05, 'epoch': 6.15}
{'loss': 0.3114, 'grad_norm': 3.6589555740356445, 'learning_rate': 1.9067822500572827e-05, 'epoch': 6.19}
{'loss': 0.2748, 'grad_norm': 2.1584672927856445, 'learning_rate': 1.887688077598717e-05, 'epoch': 6.22}
{'loss': 0.2993, 'grad_norm': 6.067856311798096, 'learning_rate': 1.8685939051401514e-05, 'epoch': 6.26}
{'loss': 0.2965, 'grad_norm': 5.400592803955078, 'learning_rate': 1.849499732681586e-05, 'epoch': 6.3}
{'loss': 0.3063, 'grad_norm': 7.292181491851807, 'learning_rate': 1.83040556022302e-05, 'epoch': 6.34}
{'loss': 0.3051, 'grad_norm': 7.199634552001953, 'learning_rate': 1.8113113877644545e-05, 'epoch': 6.38}
{'loss': 0.301, 'grad_norm': 14.05126953125, 'learning_rate': 1.7922172153058887e-05, 'epoch': 6.42}
{'loss': 0.2985, 'grad_norm': 8.688016891479492, 'learning_rate': 1.7731230428473232e-05, 'epoch': 6.45}
{'loss': 0.2868, 'grad_norm': 1.2778218984603882, 'learning_rate': 1.7540288703887577e-05, 'epoch': 6.49}
{'loss': 0.3002, 'grad_norm': 6.270583152770996, 'learning_rate': 1.734934697930192e-05, 'epoch': 6.53}
{'loss': 0.3124, 'grad_norm': 1.3351253271102905, 'learning_rate': 1.715840525471626e-05, 'epoch': 6.57}
{'loss': 0.282, 'grad_norm': 0.7679943442344666, 'learning_rate': 1.6967463530130605e-05, 'epoch': 6.61}
{'loss': 0.296, 'grad_norm': 0.9891488552093506, 'learning_rate': 1.6776521805544947e-05, 'epoch': 6.64}
{'loss': 0.2964, 'grad_norm': 5.792471408843994, 'learning_rate': 1.6585580080959292e-05, 'epoch': 6.68}
{'loss': 0.3041, 'grad_norm': 8.692513465881348, 'learning_rate': 1.6394638356373633e-05, 'epoch': 6.72}
{'loss': 0.301, 'grad_norm': 16.01938247680664, 'learning_rate': 1.620369663178798e-05, 'epoch': 6.76}
{'loss': 0.2819, 'grad_norm': 5.085179328918457, 'learning_rate': 1.6012754907202323e-05, 'epoch': 6.8}
{'loss': 0.3013, 'grad_norm': 0.46184098720550537, 'learning_rate': 1.5821813182616665e-05, 'epoch': 6.84}
{'loss': 0.3114, 'grad_norm': 5.778287410736084, 'learning_rate': 1.563087145803101e-05, 'epoch': 6.87}
{'loss': 0.3159, 'grad_norm': 1.3193073272705078, 'learning_rate': 1.543992973344535e-05, 'epoch': 6.91}
{'loss': 0.3023, 'grad_norm': 11.222740173339844, 'learning_rate': 1.5248988008859697e-05, 'epoch': 6.95}
{'loss': 0.2883, 'grad_norm': 3.493286609649658, 'learning_rate': 1.5058046284274042e-05, 'epoch': 6.99}
{'loss': 0.3017, 'grad_norm': 0.867214024066925, 'learning_rate': 1.4867104559688383e-05, 'epoch': 7.03}
{'loss': 0.2934, 'grad_norm': 3.15506649017334, 'learning_rate': 1.4676162835102728e-05, 'epoch': 7.06}
{'loss': 0.2946, 'grad_norm': 4.025750637054443, 'learning_rate': 1.4485221110517072e-05, 'epoch': 7.1}
{'loss': 0.2919, 'grad_norm': 12.331196784973145, 'learning_rate': 1.4294279385931413e-05, 'epoch': 7.14}
{'loss': 0.2834, 'grad_norm': 8.935169219970703, 'learning_rate': 1.4103337661345758e-05, 'epoch': 7.18}
{'loss': 0.3027, 'grad_norm': 0.7081635594367981, 'learning_rate': 1.39123959367601e-05, 'epoch': 7.22}
{'loss': 0.2853, 'grad_norm': 7.162354469299316, 'learning_rate': 1.3721454212174445e-05, 'epoch': 7.26}
{'loss': 0.2947, 'grad_norm': 7.536683082580566, 'learning_rate': 1.353051248758879e-05, 'epoch': 7.29}
{'loss': 0.2965, 'grad_norm': 0.9845448136329651, 'learning_rate': 1.3339570763003132e-05, 'epoch': 7.33}
{'loss': 0.2844, 'grad_norm': 7.092708110809326, 'learning_rate': 1.3148629038417477e-05, 'epoch': 7.37}
{'loss': 0.2782, 'grad_norm': 10.263493537902832, 'learning_rate': 1.2957687313831818e-05, 'epoch': 7.41}
{'loss': 0.3111, 'grad_norm': 1.3007735013961792, 'learning_rate': 1.2766745589246163e-05, 'epoch': 7.45}
{'loss': 0.2889, 'grad_norm': 4.799861431121826, 'learning_rate': 1.2575803864660507e-05, 'epoch': 7.48}
{'loss': 0.2813, 'grad_norm': 9.08496379852295, 'learning_rate': 1.238486214007485e-05, 'epoch': 7.52}
{'loss': 0.3103, 'grad_norm': 0.5923462510108948, 'learning_rate': 1.2193920415489193e-05, 'epoch': 7.56}
{'loss': 0.3008, 'grad_norm': 1.557008147239685, 'learning_rate': 1.2002978690903536e-05, 'epoch': 7.6}
{'loss': 0.2899, 'grad_norm': 10.266047477722168, 'learning_rate': 1.181203696631788e-05, 'epoch': 7.64}
{'loss': 0.3192, 'grad_norm': 1.3031977415084839, 'learning_rate': 1.1621095241732225e-05, 'epoch': 7.68}
{'loss': 0.2987, 'grad_norm': 1.0234571695327759, 'learning_rate': 1.1430153517146568e-05, 'epoch': 7.71}
{'loss': 0.3056, 'grad_norm': 2.3600361347198486, 'learning_rate': 1.1239211792560911e-05, 'epoch': 7.75}
{'loss': 0.3059, 'grad_norm': 0.7855092287063599, 'learning_rate': 1.1048270067975255e-05, 'epoch': 7.79}
{'loss': 0.3159, 'grad_norm': 0.5287765860557556, 'learning_rate': 1.0857328343389598e-05, 'epoch': 7.83}
{'loss': 0.2724, 'grad_norm': 2.6977882385253906, 'learning_rate': 1.0666386618803941e-05, 'epoch': 7.87}
{'loss': 0.2892, 'grad_norm': 9.427961349487305, 'learning_rate': 1.0475444894218285e-05, 'epoch': 7.9}
{'loss': 0.304, 'grad_norm': 5.831766605377197, 'learning_rate': 1.0284503169632628e-05, 'epoch': 7.94}
{'loss': 0.3, 'grad_norm': 1.0361089706420898, 'learning_rate': 1.0093561445046971e-05, 'epoch': 7.98}
{'loss': 0.2996, 'grad_norm': 10.343498229980469, 'learning_rate': 9.902619720461316e-06, 'epoch': 8.02}
{'loss': 0.2935, 'grad_norm': 9.656902313232422, 'learning_rate': 9.71167799587566e-06, 'epoch': 8.06}
{'loss': 0.2884, 'grad_norm': 7.862234592437744, 'learning_rate': 9.520736271290003e-06, 'epoch': 8.1}
{'loss': 0.2778, 'grad_norm': 19.05902099609375, 'learning_rate': 9.329794546704346e-06, 'epoch': 8.13}
{'loss': 0.2956, 'grad_norm': 19.455942153930664, 'learning_rate': 9.13885282211869e-06, 'epoch': 8.17}
{'loss': 0.2817, 'grad_norm': 10.747884750366211, 'learning_rate': 8.947911097533033e-06, 'epoch': 8.21}
{'loss': 0.2926, 'grad_norm': 3.194430351257324, 'learning_rate': 8.756969372947376e-06, 'epoch': 8.25}
{'loss': 0.2853, 'grad_norm': 5.501270771026611, 'learning_rate': 8.56602764836172e-06, 'epoch': 8.29}
{'loss': 0.2847, 'grad_norm': 4.7704291343688965, 'learning_rate': 8.375085923776065e-06, 'epoch': 8.33}
{'loss': 0.2952, 'grad_norm': 12.53687572479248, 'learning_rate': 8.184144199190408e-06, 'epoch': 8.36}
{'loss': 0.2927, 'grad_norm': 6.735651016235352, 'learning_rate': 7.993202474604751e-06, 'epoch': 8.4}
{'loss': 0.29, 'grad_norm': 6.205052852630615, 'learning_rate': 7.802260750019095e-06, 'epoch': 8.44}
{'loss': 0.2743, 'grad_norm': 0.8268216848373413, 'learning_rate': 7.611319025433437e-06, 'epoch': 8.48}
{'loss': 0.2931, 'grad_norm': 8.162919998168945, 'learning_rate': 7.420377300847782e-06, 'epoch': 8.52}
{'loss': 0.2926, 'grad_norm': 0.5739408731460571, 'learning_rate': 7.2294355762621254e-06, 'epoch': 8.55}
{'loss': 0.3196, 'grad_norm': 1.520267128944397, 'learning_rate': 7.038493851676469e-06, 'epoch': 8.59}
{'loss': 0.2782, 'grad_norm': 0.7827140092849731, 'learning_rate': 6.847552127090811e-06, 'epoch': 8.63}
{'loss': 0.2978, 'grad_norm': 10.384861946105957, 'learning_rate': 6.656610402505156e-06, 'epoch': 8.67}
{'loss': 0.308, 'grad_norm': 5.371720314025879, 'learning_rate': 6.4656686779194996e-06, 'epoch': 8.71}
{'loss': 0.3007, 'grad_norm': 4.0015058517456055, 'learning_rate': 6.274726953333843e-06, 'epoch': 8.75}
{'loss': 0.2929, 'grad_norm': 9.170289039611816, 'learning_rate': 6.083785228748186e-06, 'epoch': 8.78}
{'loss': 0.2721, 'grad_norm': 1.413697600364685, 'learning_rate': 5.8928435041625295e-06, 'epoch': 8.82}
{'loss': 0.2758, 'grad_norm': 7.948320388793945, 'learning_rate': 5.701901779576874e-06, 'epoch': 8.86}
{'loss': 0.2949, 'grad_norm': 6.068889141082764, 'learning_rate': 5.510960054991217e-06, 'epoch': 8.9}
{'loss': 0.3022, 'grad_norm': 4.384580135345459, 'learning_rate': 5.320018330405561e-06, 'epoch': 8.94}
{'loss': 0.2863, 'grad_norm': 4.915485858917236, 'learning_rate': 5.129076605819904e-06, 'epoch': 8.97}
{'loss': 0.2886, 'grad_norm': 8.741601943969727, 'learning_rate': 4.938134881234248e-06, 'epoch': 9.01}
{'loss': 0.2776, 'grad_norm': 0.9410789608955383, 'learning_rate': 4.747193156648591e-06, 'epoch': 9.05}
{'loss': 0.3053, 'grad_norm': 8.582465171813965, 'learning_rate': 4.5562514320629344e-06, 'epoch': 9.09}
{'loss': 0.287, 'grad_norm': 6.61831521987915, 'learning_rate': 4.365309707477279e-06, 'epoch': 9.13}
{'loss': 0.2951, 'grad_norm': 0.880193293094635, 'learning_rate': 4.174367982891621e-06, 'epoch': 9.17}
{'loss': 0.2807, 'grad_norm': 9.84064769744873, 'learning_rate': 3.983426258305965e-06, 'epoch': 9.2}
{'loss': 0.2922, 'grad_norm': 5.244017124176025, 'learning_rate': 3.7924845337203086e-06, 'epoch': 9.24}
{'loss': 0.2892, 'grad_norm': 0.8293601274490356, 'learning_rate': 3.6015428091346523e-06, 'epoch': 9.28}
{'loss': 0.2812, 'grad_norm': 0.4033152461051941, 'learning_rate': 3.4106010845489956e-06, 'epoch': 9.32}
{'loss': 0.3055, 'grad_norm': 0.45363131165504456, 'learning_rate': 3.2196593599633394e-06, 'epoch': 9.36}
{'loss': 0.2807, 'grad_norm': 6.099381923675537, 'learning_rate': 3.0287176353776827e-06, 'epoch': 9.39}
{'loss': 0.3207, 'grad_norm': 7.0820112228393555, 'learning_rate': 2.8377759107920264e-06, 'epoch': 9.43}
{'loss': 0.2779, 'grad_norm': 11.36723804473877, 'learning_rate': 2.6468341862063698e-06, 'epoch': 9.47}
{'loss': 0.2913, 'grad_norm': 2.099149703979492, 'learning_rate': 2.4558924616207135e-06, 'epoch': 9.51}
{'loss': 0.3054, 'grad_norm': 1.3303396701812744, 'learning_rate': 2.264950737035057e-06, 'epoch': 9.55}
{'loss': 0.3023, 'grad_norm': 6.661375045776367, 'learning_rate': 2.0740090124494006e-06, 'epoch': 9.59}
{'loss': 0.2896, 'grad_norm': 7.787227153778076, 'learning_rate': 1.883067287863744e-06, 'epoch': 9.62}
{'loss': 0.2812, 'grad_norm': 1.0795390605926514, 'learning_rate': 1.6921255632780876e-06, 'epoch': 9.66}
{'loss': 0.2728, 'grad_norm': 8.347513198852539, 'learning_rate': 1.501183838692431e-06, 'epoch': 9.7}
{'loss': 0.2795, 'grad_norm': 5.8591790199279785, 'learning_rate': 1.3102421141067747e-06, 'epoch': 9.74}
{'loss': 0.2706, 'grad_norm': 10.994630813598633, 'learning_rate': 1.1193003895211182e-06, 'epoch': 9.78}
{'loss': 0.2952, 'grad_norm': 15.484472274780273, 'learning_rate': 9.283586649354618e-07, 'epoch': 9.81}
{'loss': 0.3235, 'grad_norm': 10.064146041870117, 'learning_rate': 7.374169403498053e-07, 'epoch': 9.85}
{'loss': 0.2677, 'grad_norm': 11.702432632446289, 'learning_rate': 5.464752157641488e-07, 'epoch': 9.89}
{'loss': 0.3088, 'grad_norm': 6.6747965812683105, 'learning_rate': 3.5553349117849236e-07, 'epoch': 9.93}
{'loss': 0.2733, 'grad_norm': 9.074546813964844, 'learning_rate': 1.6459176659283587e-07, 'epoch': 9.97}
{'train_runtime': 20028.5826, 'train_samples_per_second': 52.297, 'train_steps_per_second': 6.537, 'train_loss': 0.3236683373453782, 'epoch': 10.0}
***** train metrics *****
  epoch                    =        10.0
  total_flos               = 227717064GF
  train_loss               =      0.3237
  train_runtime            =  5:33:48.58
  train_samples            =      104743
  train_samples_per_second =      52.297
  train_steps_per_second   =       6.537
08/07/2025 07:31:02 - INFO - __main__ - *** Evaluate ***
***** eval metrics *****
  epoch                   =       10.0
  eval_accuracy           =     0.9109
  eval_loss               =     0.2725
  eval_runtime            = 0:00:46.05
  eval_samples            =       5463
  eval_samples_per_second =    118.609
  eval_steps_per_second   =     14.829
