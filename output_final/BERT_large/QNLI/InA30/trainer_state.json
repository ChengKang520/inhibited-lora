{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 130930,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.038188344917131294,
      "grad_norm": 10.53084659576416,
      "learning_rate": 4.980944015886352e-05,
      "loss": 0.6358,
      "step": 500
    },
    {
      "epoch": 0.07637668983426259,
      "grad_norm": 4.037789344787598,
      "learning_rate": 4.961849843427786e-05,
      "loss": 0.483,
      "step": 1000
    },
    {
      "epoch": 0.11456503475139387,
      "grad_norm": 5.615322589874268,
      "learning_rate": 4.94275567096922e-05,
      "loss": 0.4262,
      "step": 1500
    },
    {
      "epoch": 0.15275337966852517,
      "grad_norm": 7.938291549682617,
      "learning_rate": 4.923661498510655e-05,
      "loss": 0.4139,
      "step": 2000
    },
    {
      "epoch": 0.19094172458565645,
      "grad_norm": 7.8838725090026855,
      "learning_rate": 4.904567326052089e-05,
      "loss": 0.3947,
      "step": 2500
    },
    {
      "epoch": 0.22913006950278775,
      "grad_norm": 12.24278736114502,
      "learning_rate": 4.885473153593523e-05,
      "loss": 0.3893,
      "step": 3000
    },
    {
      "epoch": 0.26731841441991905,
      "grad_norm": 4.055706977844238,
      "learning_rate": 4.866378981134958e-05,
      "loss": 0.3869,
      "step": 3500
    },
    {
      "epoch": 0.30550675933705035,
      "grad_norm": 4.6896138191223145,
      "learning_rate": 4.847284808676392e-05,
      "loss": 0.3859,
      "step": 4000
    },
    {
      "epoch": 0.34369510425418165,
      "grad_norm": 6.07366418838501,
      "learning_rate": 4.828190636217826e-05,
      "loss": 0.3767,
      "step": 4500
    },
    {
      "epoch": 0.3818834491713129,
      "grad_norm": 7.1957597732543945,
      "learning_rate": 4.809096463759261e-05,
      "loss": 0.3611,
      "step": 5000
    },
    {
      "epoch": 0.4200717940884442,
      "grad_norm": 4.372681140899658,
      "learning_rate": 4.790002291300695e-05,
      "loss": 0.3657,
      "step": 5500
    },
    {
      "epoch": 0.4582601390055755,
      "grad_norm": 4.894469261169434,
      "learning_rate": 4.7709081188421295e-05,
      "loss": 0.3697,
      "step": 6000
    },
    {
      "epoch": 0.4964484839227068,
      "grad_norm": 5.249190807342529,
      "learning_rate": 4.7518139463835637e-05,
      "loss": 0.3639,
      "step": 6500
    },
    {
      "epoch": 0.5346368288398381,
      "grad_norm": 3.7592058181762695,
      "learning_rate": 4.7327197739249985e-05,
      "loss": 0.3643,
      "step": 7000
    },
    {
      "epoch": 0.5728251737569694,
      "grad_norm": 11.55576229095459,
      "learning_rate": 4.713625601466433e-05,
      "loss": 0.3434,
      "step": 7500
    },
    {
      "epoch": 0.6110135186741007,
      "grad_norm": 11.281378746032715,
      "learning_rate": 4.694531429007867e-05,
      "loss": 0.3613,
      "step": 8000
    },
    {
      "epoch": 0.649201863591232,
      "grad_norm": 8.651056289672852,
      "learning_rate": 4.675437256549302e-05,
      "loss": 0.3651,
      "step": 8500
    },
    {
      "epoch": 0.6873902085083633,
      "grad_norm": 14.263465881347656,
      "learning_rate": 4.656343084090736e-05,
      "loss": 0.3424,
      "step": 9000
    },
    {
      "epoch": 0.7255785534254945,
      "grad_norm": 6.145023345947266,
      "learning_rate": 4.63724891163217e-05,
      "loss": 0.3349,
      "step": 9500
    },
    {
      "epoch": 0.7637668983426258,
      "grad_norm": 8.369251251220703,
      "learning_rate": 4.618154739173605e-05,
      "loss": 0.3499,
      "step": 10000
    },
    {
      "epoch": 0.8019552432597571,
      "grad_norm": 2.6044199466705322,
      "learning_rate": 4.599060566715039e-05,
      "loss": 0.329,
      "step": 10500
    },
    {
      "epoch": 0.8401435881768884,
      "grad_norm": 5.082296371459961,
      "learning_rate": 4.579966394256473e-05,
      "loss": 0.3483,
      "step": 11000
    },
    {
      "epoch": 0.8783319330940197,
      "grad_norm": 6.866949558258057,
      "learning_rate": 4.560872221797908e-05,
      "loss": 0.341,
      "step": 11500
    },
    {
      "epoch": 0.916520278011151,
      "grad_norm": 0.5590705275535583,
      "learning_rate": 4.541778049339342e-05,
      "loss": 0.3285,
      "step": 12000
    },
    {
      "epoch": 0.9547086229282823,
      "grad_norm": 3.4409279823303223,
      "learning_rate": 4.522683876880776e-05,
      "loss": 0.3481,
      "step": 12500
    },
    {
      "epoch": 0.9928969678454136,
      "grad_norm": 0.7077175378799438,
      "learning_rate": 4.5035897044222105e-05,
      "loss": 0.3314,
      "step": 13000
    },
    {
      "epoch": 1.0310853127625448,
      "grad_norm": 9.481780052185059,
      "learning_rate": 4.484495531963645e-05,
      "loss": 0.3276,
      "step": 13500
    },
    {
      "epoch": 1.0692736576796762,
      "grad_norm": 4.518460750579834,
      "learning_rate": 4.4654013595050795e-05,
      "loss": 0.33,
      "step": 14000
    },
    {
      "epoch": 1.1074620025968074,
      "grad_norm": 3.2599191665649414,
      "learning_rate": 4.4463071870465137e-05,
      "loss": 0.3236,
      "step": 14500
    },
    {
      "epoch": 1.1456503475139388,
      "grad_norm": 6.619580268859863,
      "learning_rate": 4.427213014587948e-05,
      "loss": 0.3332,
      "step": 15000
    },
    {
      "epoch": 1.18383869243107,
      "grad_norm": 8.69969654083252,
      "learning_rate": 4.408118842129382e-05,
      "loss": 0.3262,
      "step": 15500
    },
    {
      "epoch": 1.2220270373482014,
      "grad_norm": 3.208707571029663,
      "learning_rate": 4.389024669670817e-05,
      "loss": 0.3062,
      "step": 16000
    },
    {
      "epoch": 1.2602153822653326,
      "grad_norm": 1.2042815685272217,
      "learning_rate": 4.369930497212251e-05,
      "loss": 0.3112,
      "step": 16500
    },
    {
      "epoch": 1.298403727182464,
      "grad_norm": 10.960898399353027,
      "learning_rate": 4.350836324753685e-05,
      "loss": 0.3216,
      "step": 17000
    },
    {
      "epoch": 1.3365920720995952,
      "grad_norm": 4.101078987121582,
      "learning_rate": 4.331742152295119e-05,
      "loss": 0.3228,
      "step": 17500
    },
    {
      "epoch": 1.3747804170167264,
      "grad_norm": 0.8215355277061462,
      "learning_rate": 4.312647979836554e-05,
      "loss": 0.3173,
      "step": 18000
    },
    {
      "epoch": 1.4129687619338578,
      "grad_norm": 6.508853435516357,
      "learning_rate": 4.293553807377988e-05,
      "loss": 0.3189,
      "step": 18500
    },
    {
      "epoch": 1.4511571068509892,
      "grad_norm": 0.5096572637557983,
      "learning_rate": 4.2744596349194225e-05,
      "loss": 0.3046,
      "step": 19000
    },
    {
      "epoch": 1.4893454517681204,
      "grad_norm": 5.618605613708496,
      "learning_rate": 4.2553654624608566e-05,
      "loss": 0.3052,
      "step": 19500
    },
    {
      "epoch": 1.5275337966852516,
      "grad_norm": 2.7241690158843994,
      "learning_rate": 4.2362712900022915e-05,
      "loss": 0.3151,
      "step": 20000
    },
    {
      "epoch": 1.565722141602383,
      "grad_norm": 6.838850975036621,
      "learning_rate": 4.2171771175437256e-05,
      "loss": 0.3167,
      "step": 20500
    },
    {
      "epoch": 1.6039104865195144,
      "grad_norm": 10.08757209777832,
      "learning_rate": 4.19808294508516e-05,
      "loss": 0.3289,
      "step": 21000
    },
    {
      "epoch": 1.6420988314366456,
      "grad_norm": 9.738860130310059,
      "learning_rate": 4.1789887726265946e-05,
      "loss": 0.3116,
      "step": 21500
    },
    {
      "epoch": 1.6802871763537768,
      "grad_norm": 10.93837833404541,
      "learning_rate": 4.159894600168029e-05,
      "loss": 0.3071,
      "step": 22000
    },
    {
      "epoch": 1.718475521270908,
      "grad_norm": 11.534760475158691,
      "learning_rate": 4.140800427709463e-05,
      "loss": 0.3191,
      "step": 22500
    },
    {
      "epoch": 1.7566638661880394,
      "grad_norm": 2.6788032054901123,
      "learning_rate": 4.121706255250898e-05,
      "loss": 0.3134,
      "step": 23000
    },
    {
      "epoch": 1.7948522111051708,
      "grad_norm": 1.8172203302383423,
      "learning_rate": 4.102612082792332e-05,
      "loss": 0.3229,
      "step": 23500
    },
    {
      "epoch": 1.833040556022302,
      "grad_norm": 10.609435081481934,
      "learning_rate": 4.083517910333766e-05,
      "loss": 0.3042,
      "step": 24000
    },
    {
      "epoch": 1.8712289009394332,
      "grad_norm": 6.8177080154418945,
      "learning_rate": 4.064423737875201e-05,
      "loss": 0.3107,
      "step": 24500
    },
    {
      "epoch": 1.9094172458565646,
      "grad_norm": 15.783669471740723,
      "learning_rate": 4.045329565416635e-05,
      "loss": 0.3052,
      "step": 25000
    },
    {
      "epoch": 1.947605590773696,
      "grad_norm": 5.504003524780273,
      "learning_rate": 4.026235392958069e-05,
      "loss": 0.3076,
      "step": 25500
    },
    {
      "epoch": 1.9857939356908272,
      "grad_norm": 4.9706244468688965,
      "learning_rate": 4.0071412204995035e-05,
      "loss": 0.3015,
      "step": 26000
    },
    {
      "epoch": 2.0239822806079584,
      "grad_norm": 11.615315437316895,
      "learning_rate": 3.988047048040938e-05,
      "loss": 0.3174,
      "step": 26500
    },
    {
      "epoch": 2.0621706255250896,
      "grad_norm": 10.66186237335205,
      "learning_rate": 3.9689528755823725e-05,
      "loss": 0.2825,
      "step": 27000
    },
    {
      "epoch": 2.100358970442221,
      "grad_norm": 11.343547821044922,
      "learning_rate": 3.9498587031238066e-05,
      "loss": 0.3,
      "step": 27500
    },
    {
      "epoch": 2.1385473153593524,
      "grad_norm": 5.697444915771484,
      "learning_rate": 3.9307645306652415e-05,
      "loss": 0.2989,
      "step": 28000
    },
    {
      "epoch": 2.1767356602764836,
      "grad_norm": 16.965978622436523,
      "learning_rate": 3.9116703582066756e-05,
      "loss": 0.3126,
      "step": 28500
    },
    {
      "epoch": 2.2149240051936148,
      "grad_norm": 0.9169769883155823,
      "learning_rate": 3.89257618574811e-05,
      "loss": 0.2909,
      "step": 29000
    },
    {
      "epoch": 2.2531123501107464,
      "grad_norm": 2.584770917892456,
      "learning_rate": 3.8734820132895446e-05,
      "loss": 0.2868,
      "step": 29500
    },
    {
      "epoch": 2.2913006950278776,
      "grad_norm": 9.819944381713867,
      "learning_rate": 3.854387840830979e-05,
      "loss": 0.2892,
      "step": 30000
    },
    {
      "epoch": 2.3294890399450088,
      "grad_norm": 20.665124893188477,
      "learning_rate": 3.835293668372413e-05,
      "loss": 0.2918,
      "step": 30500
    },
    {
      "epoch": 2.36767738486214,
      "grad_norm": 3.2396059036254883,
      "learning_rate": 3.816199495913848e-05,
      "loss": 0.2872,
      "step": 31000
    },
    {
      "epoch": 2.405865729779271,
      "grad_norm": 1.3802294731140137,
      "learning_rate": 3.797105323455282e-05,
      "loss": 0.3054,
      "step": 31500
    },
    {
      "epoch": 2.444054074696403,
      "grad_norm": 12.009443283081055,
      "learning_rate": 3.778011150996716e-05,
      "loss": 0.3092,
      "step": 32000
    },
    {
      "epoch": 2.482242419613534,
      "grad_norm": 9.856634140014648,
      "learning_rate": 3.75891697853815e-05,
      "loss": 0.286,
      "step": 32500
    },
    {
      "epoch": 2.520430764530665,
      "grad_norm": 4.873864650726318,
      "learning_rate": 3.739822806079585e-05,
      "loss": 0.2971,
      "step": 33000
    },
    {
      "epoch": 2.558619109447797,
      "grad_norm": 0.5384643077850342,
      "learning_rate": 3.720728633621019e-05,
      "loss": 0.2871,
      "step": 33500
    },
    {
      "epoch": 2.596807454364928,
      "grad_norm": 13.97099494934082,
      "learning_rate": 3.7016344611624534e-05,
      "loss": 0.2965,
      "step": 34000
    },
    {
      "epoch": 2.634995799282059,
      "grad_norm": 14.366978645324707,
      "learning_rate": 3.6825402887038876e-05,
      "loss": 0.3009,
      "step": 34500
    },
    {
      "epoch": 2.6731841441991904,
      "grad_norm": 5.831216812133789,
      "learning_rate": 3.663446116245322e-05,
      "loss": 0.2968,
      "step": 35000
    },
    {
      "epoch": 2.7113724891163216,
      "grad_norm": 10.76602840423584,
      "learning_rate": 3.644351943786756e-05,
      "loss": 0.3,
      "step": 35500
    },
    {
      "epoch": 2.7495608340334527,
      "grad_norm": 7.478211879730225,
      "learning_rate": 3.625257771328191e-05,
      "loss": 0.298,
      "step": 36000
    },
    {
      "epoch": 2.7877491789505844,
      "grad_norm": 16.0058650970459,
      "learning_rate": 3.606163598869625e-05,
      "loss": 0.2965,
      "step": 36500
    },
    {
      "epoch": 2.8259375238677156,
      "grad_norm": 6.614048004150391,
      "learning_rate": 3.587069426411059e-05,
      "loss": 0.296,
      "step": 37000
    },
    {
      "epoch": 2.8641258687848468,
      "grad_norm": 2.081805467605591,
      "learning_rate": 3.567975253952494e-05,
      "loss": 0.2961,
      "step": 37500
    },
    {
      "epoch": 2.9023142137019784,
      "grad_norm": 13.506364822387695,
      "learning_rate": 3.548881081493928e-05,
      "loss": 0.2969,
      "step": 38000
    },
    {
      "epoch": 2.9405025586191096,
      "grad_norm": 11.186175346374512,
      "learning_rate": 3.529786909035362e-05,
      "loss": 0.2995,
      "step": 38500
    },
    {
      "epoch": 2.9786909035362408,
      "grad_norm": 6.27740478515625,
      "learning_rate": 3.510692736576797e-05,
      "loss": 0.3015,
      "step": 39000
    },
    {
      "epoch": 3.016879248453372,
      "grad_norm": 0.5541539788246155,
      "learning_rate": 3.491598564118231e-05,
      "loss": 0.2902,
      "step": 39500
    },
    {
      "epoch": 3.055067593370503,
      "grad_norm": 1.7995586395263672,
      "learning_rate": 3.4725043916596654e-05,
      "loss": 0.2913,
      "step": 40000
    },
    {
      "epoch": 3.093255938287635,
      "grad_norm": 9.075665473937988,
      "learning_rate": 3.4534102192010996e-05,
      "loss": 0.2836,
      "step": 40500
    },
    {
      "epoch": 3.131444283204766,
      "grad_norm": 8.11109733581543,
      "learning_rate": 3.4343160467425344e-05,
      "loss": 0.2965,
      "step": 41000
    },
    {
      "epoch": 3.169632628121897,
      "grad_norm": 0.730556845664978,
      "learning_rate": 3.4152218742839686e-05,
      "loss": 0.2891,
      "step": 41500
    },
    {
      "epoch": 3.2078209730390284,
      "grad_norm": 0.4974443316459656,
      "learning_rate": 3.396127701825403e-05,
      "loss": 0.2883,
      "step": 42000
    },
    {
      "epoch": 3.24600931795616,
      "grad_norm": 8.682490348815918,
      "learning_rate": 3.3770335293668376e-05,
      "loss": 0.2855,
      "step": 42500
    },
    {
      "epoch": 3.284197662873291,
      "grad_norm": 11.197065353393555,
      "learning_rate": 3.357939356908272e-05,
      "loss": 0.2989,
      "step": 43000
    },
    {
      "epoch": 3.3223860077904224,
      "grad_norm": 14.310378074645996,
      "learning_rate": 3.338845184449706e-05,
      "loss": 0.2738,
      "step": 43500
    },
    {
      "epoch": 3.3605743527075536,
      "grad_norm": 1.6761417388916016,
      "learning_rate": 3.319751011991141e-05,
      "loss": 0.2783,
      "step": 44000
    },
    {
      "epoch": 3.3987626976246847,
      "grad_norm": 18.89517593383789,
      "learning_rate": 3.300656839532575e-05,
      "loss": 0.2838,
      "step": 44500
    },
    {
      "epoch": 3.4369510425418164,
      "grad_norm": 7.051464557647705,
      "learning_rate": 3.281562667074009e-05,
      "loss": 0.2753,
      "step": 45000
    },
    {
      "epoch": 3.4751393874589476,
      "grad_norm": 2.351356267929077,
      "learning_rate": 3.262468494615444e-05,
      "loss": 0.2798,
      "step": 45500
    },
    {
      "epoch": 3.5133277323760788,
      "grad_norm": 12.30795669555664,
      "learning_rate": 3.243374322156878e-05,
      "loss": 0.2858,
      "step": 46000
    },
    {
      "epoch": 3.55151607729321,
      "grad_norm": 0.5028461217880249,
      "learning_rate": 3.224280149698312e-05,
      "loss": 0.2865,
      "step": 46500
    },
    {
      "epoch": 3.5897044222103416,
      "grad_norm": 11.779559135437012,
      "learning_rate": 3.2051859772397464e-05,
      "loss": 0.2952,
      "step": 47000
    },
    {
      "epoch": 3.6278927671274728,
      "grad_norm": 5.671133995056152,
      "learning_rate": 3.186091804781181e-05,
      "loss": 0.2972,
      "step": 47500
    },
    {
      "epoch": 3.666081112044604,
      "grad_norm": 8.724223136901855,
      "learning_rate": 3.1669976323226154e-05,
      "loss": 0.2778,
      "step": 48000
    },
    {
      "epoch": 3.704269456961735,
      "grad_norm": 11.480586051940918,
      "learning_rate": 3.1479034598640496e-05,
      "loss": 0.2929,
      "step": 48500
    },
    {
      "epoch": 3.7424578018788663,
      "grad_norm": 1.007830023765564,
      "learning_rate": 3.1288092874054844e-05,
      "loss": 0.2882,
      "step": 49000
    },
    {
      "epoch": 3.780646146795998,
      "grad_norm": 0.8574655055999756,
      "learning_rate": 3.1097151149469186e-05,
      "loss": 0.284,
      "step": 49500
    },
    {
      "epoch": 3.818834491713129,
      "grad_norm": 1.5076205730438232,
      "learning_rate": 3.090620942488353e-05,
      "loss": 0.2752,
      "step": 50000
    },
    {
      "epoch": 3.8570228366302604,
      "grad_norm": 6.829051971435547,
      "learning_rate": 3.0715267700297876e-05,
      "loss": 0.3022,
      "step": 50500
    },
    {
      "epoch": 3.895211181547392,
      "grad_norm": 6.490123271942139,
      "learning_rate": 3.052432597571222e-05,
      "loss": 0.2932,
      "step": 51000
    },
    {
      "epoch": 3.933399526464523,
      "grad_norm": 10.65611457824707,
      "learning_rate": 3.0333384251126556e-05,
      "loss": 0.2723,
      "step": 51500
    },
    {
      "epoch": 3.9715878713816544,
      "grad_norm": 7.077435493469238,
      "learning_rate": 3.0142442526540904e-05,
      "loss": 0.2881,
      "step": 52000
    },
    {
      "epoch": 4.009776216298786,
      "grad_norm": 11.307936668395996,
      "learning_rate": 2.9951500801955246e-05,
      "loss": 0.2647,
      "step": 52500
    },
    {
      "epoch": 4.047964561215917,
      "grad_norm": 10.798332214355469,
      "learning_rate": 2.9760559077369587e-05,
      "loss": 0.2698,
      "step": 53000
    },
    {
      "epoch": 4.086152906133048,
      "grad_norm": 19.23703384399414,
      "learning_rate": 2.956961735278393e-05,
      "loss": 0.2858,
      "step": 53500
    },
    {
      "epoch": 4.124341251050179,
      "grad_norm": 77.78828430175781,
      "learning_rate": 2.9378675628198277e-05,
      "loss": 0.2742,
      "step": 54000
    },
    {
      "epoch": 4.162529595967311,
      "grad_norm": 6.048426151275635,
      "learning_rate": 2.918773390361262e-05,
      "loss": 0.3,
      "step": 54500
    },
    {
      "epoch": 4.200717940884442,
      "grad_norm": 0.38756492733955383,
      "learning_rate": 2.899679217902696e-05,
      "loss": 0.2701,
      "step": 55000
    },
    {
      "epoch": 4.238906285801574,
      "grad_norm": 5.987629413604736,
      "learning_rate": 2.880585045444131e-05,
      "loss": 0.2846,
      "step": 55500
    },
    {
      "epoch": 4.277094630718705,
      "grad_norm": 6.020256519317627,
      "learning_rate": 2.861490872985565e-05,
      "loss": 0.2805,
      "step": 56000
    },
    {
      "epoch": 4.315282975635836,
      "grad_norm": 9.254128456115723,
      "learning_rate": 2.8423967005269992e-05,
      "loss": 0.2882,
      "step": 56500
    },
    {
      "epoch": 4.353471320552967,
      "grad_norm": 1.1680750846862793,
      "learning_rate": 2.8233025280684337e-05,
      "loss": 0.2795,
      "step": 57000
    },
    {
      "epoch": 4.391659665470098,
      "grad_norm": 0.49318403005599976,
      "learning_rate": 2.804208355609868e-05,
      "loss": 0.2868,
      "step": 57500
    },
    {
      "epoch": 4.4298480103872295,
      "grad_norm": 6.830478668212891,
      "learning_rate": 2.7851141831513024e-05,
      "loss": 0.2723,
      "step": 58000
    },
    {
      "epoch": 4.468036355304361,
      "grad_norm": 8.532211303710938,
      "learning_rate": 2.766020010692737e-05,
      "loss": 0.2657,
      "step": 58500
    },
    {
      "epoch": 4.506224700221493,
      "grad_norm": 1.0015873908996582,
      "learning_rate": 2.746925838234171e-05,
      "loss": 0.2713,
      "step": 59000
    },
    {
      "epoch": 4.544413045138624,
      "grad_norm": 13.253573417663574,
      "learning_rate": 2.7278316657756052e-05,
      "loss": 0.2898,
      "step": 59500
    },
    {
      "epoch": 4.582601390055755,
      "grad_norm": 6.748828887939453,
      "learning_rate": 2.7087374933170394e-05,
      "loss": 0.2798,
      "step": 60000
    },
    {
      "epoch": 4.620789734972886,
      "grad_norm": 29.831241607666016,
      "learning_rate": 2.6896433208584742e-05,
      "loss": 0.2711,
      "step": 60500
    },
    {
      "epoch": 4.6589780798900176,
      "grad_norm": 19.83102798461914,
      "learning_rate": 2.6705491483999084e-05,
      "loss": 0.2797,
      "step": 61000
    },
    {
      "epoch": 4.697166424807149,
      "grad_norm": 1.6893765926361084,
      "learning_rate": 2.6514549759413426e-05,
      "loss": 0.266,
      "step": 61500
    },
    {
      "epoch": 4.73535476972428,
      "grad_norm": 0.3713701069355011,
      "learning_rate": 2.6323608034827774e-05,
      "loss": 0.2721,
      "step": 62000
    },
    {
      "epoch": 4.773543114641411,
      "grad_norm": 0.47038987278938293,
      "learning_rate": 2.6132666310242116e-05,
      "loss": 0.2648,
      "step": 62500
    },
    {
      "epoch": 4.811731459558542,
      "grad_norm": 0.5744193196296692,
      "learning_rate": 2.5941724585656457e-05,
      "loss": 0.264,
      "step": 63000
    },
    {
      "epoch": 4.849919804475674,
      "grad_norm": 0.6531267762184143,
      "learning_rate": 2.5750782861070806e-05,
      "loss": 0.2826,
      "step": 63500
    },
    {
      "epoch": 4.888108149392806,
      "grad_norm": 35.5915412902832,
      "learning_rate": 2.5559841136485147e-05,
      "loss": 0.2695,
      "step": 64000
    },
    {
      "epoch": 4.926296494309937,
      "grad_norm": 14.998799324035645,
      "learning_rate": 2.536889941189949e-05,
      "loss": 0.2654,
      "step": 64500
    },
    {
      "epoch": 4.964484839227068,
      "grad_norm": 7.502220153808594,
      "learning_rate": 2.5177957687313837e-05,
      "loss": 0.2743,
      "step": 65000
    },
    {
      "epoch": 5.002673184144199,
      "grad_norm": 6.822080612182617,
      "learning_rate": 2.498701596272818e-05,
      "loss": 0.2644,
      "step": 65500
    },
    {
      "epoch": 5.04086152906133,
      "grad_norm": 0.2909386157989502,
      "learning_rate": 2.479607423814252e-05,
      "loss": 0.2576,
      "step": 66000
    },
    {
      "epoch": 5.0790498739784615,
      "grad_norm": 0.9072278141975403,
      "learning_rate": 2.4605132513556866e-05,
      "loss": 0.2554,
      "step": 66500
    },
    {
      "epoch": 5.117238218895593,
      "grad_norm": 11.183511734008789,
      "learning_rate": 2.4414190788971207e-05,
      "loss": 0.2857,
      "step": 67000
    },
    {
      "epoch": 5.155426563812725,
      "grad_norm": 3.6680197715759277,
      "learning_rate": 2.422324906438555e-05,
      "loss": 0.2696,
      "step": 67500
    },
    {
      "epoch": 5.193614908729856,
      "grad_norm": 0.7566215395927429,
      "learning_rate": 2.4032307339799894e-05,
      "loss": 0.2596,
      "step": 68000
    },
    {
      "epoch": 5.231803253646987,
      "grad_norm": 0.8514333963394165,
      "learning_rate": 2.3841365615214235e-05,
      "loss": 0.2728,
      "step": 68500
    },
    {
      "epoch": 5.269991598564118,
      "grad_norm": 23.86363410949707,
      "learning_rate": 2.365042389062858e-05,
      "loss": 0.2603,
      "step": 69000
    },
    {
      "epoch": 5.3081799434812496,
      "grad_norm": 16.624065399169922,
      "learning_rate": 2.3459482166042925e-05,
      "loss": 0.2621,
      "step": 69500
    },
    {
      "epoch": 5.346368288398381,
      "grad_norm": 0.930060625076294,
      "learning_rate": 2.3268540441457267e-05,
      "loss": 0.2646,
      "step": 70000
    },
    {
      "epoch": 5.384556633315512,
      "grad_norm": 0.3907460868358612,
      "learning_rate": 2.3077598716871612e-05,
      "loss": 0.2772,
      "step": 70500
    },
    {
      "epoch": 5.422744978232643,
      "grad_norm": 7.440035820007324,
      "learning_rate": 2.2886656992285954e-05,
      "loss": 0.2819,
      "step": 71000
    },
    {
      "epoch": 5.460933323149774,
      "grad_norm": 0.6544597744941711,
      "learning_rate": 2.26957152677003e-05,
      "loss": 0.2681,
      "step": 71500
    },
    {
      "epoch": 5.499121668066906,
      "grad_norm": 0.7283518314361572,
      "learning_rate": 2.2504773543114644e-05,
      "loss": 0.2496,
      "step": 72000
    },
    {
      "epoch": 5.537310012984038,
      "grad_norm": 21.116409301757812,
      "learning_rate": 2.2313831818528985e-05,
      "loss": 0.2632,
      "step": 72500
    },
    {
      "epoch": 5.575498357901169,
      "grad_norm": 1.4588232040405273,
      "learning_rate": 2.212289009394333e-05,
      "loss": 0.2638,
      "step": 73000
    },
    {
      "epoch": 5.6136867028183,
      "grad_norm": 10.075213432312012,
      "learning_rate": 2.1931948369357675e-05,
      "loss": 0.2759,
      "step": 73500
    },
    {
      "epoch": 5.651875047735431,
      "grad_norm": 0.4755089282989502,
      "learning_rate": 2.1741006644772017e-05,
      "loss": 0.2616,
      "step": 74000
    },
    {
      "epoch": 5.690063392652562,
      "grad_norm": 0.7745149731636047,
      "learning_rate": 2.1550064920186362e-05,
      "loss": 0.2721,
      "step": 74500
    },
    {
      "epoch": 5.7282517375696935,
      "grad_norm": 1.6388667821884155,
      "learning_rate": 2.1359123195600704e-05,
      "loss": 0.2557,
      "step": 75000
    },
    {
      "epoch": 5.766440082486825,
      "grad_norm": 7.5517096519470215,
      "learning_rate": 2.116818147101505e-05,
      "loss": 0.2776,
      "step": 75500
    },
    {
      "epoch": 5.804628427403957,
      "grad_norm": 1.433464527130127,
      "learning_rate": 2.097723974642939e-05,
      "loss": 0.2743,
      "step": 76000
    },
    {
      "epoch": 5.842816772321088,
      "grad_norm": 11.849322319030762,
      "learning_rate": 2.0786298021843735e-05,
      "loss": 0.2703,
      "step": 76500
    },
    {
      "epoch": 5.881005117238219,
      "grad_norm": 23.774545669555664,
      "learning_rate": 2.0595356297258077e-05,
      "loss": 0.2639,
      "step": 77000
    },
    {
      "epoch": 5.91919346215535,
      "grad_norm": 21.02369499206543,
      "learning_rate": 2.040441457267242e-05,
      "loss": 0.2803,
      "step": 77500
    },
    {
      "epoch": 5.9573818070724815,
      "grad_norm": 14.084775924682617,
      "learning_rate": 2.0213472848086764e-05,
      "loss": 0.2844,
      "step": 78000
    },
    {
      "epoch": 5.995570151989613,
      "grad_norm": 14.803247451782227,
      "learning_rate": 2.002253112350111e-05,
      "loss": 0.2529,
      "step": 78500
    },
    {
      "epoch": 6.033758496906744,
      "grad_norm": 2.3276171684265137,
      "learning_rate": 1.983158939891545e-05,
      "loss": 0.2741,
      "step": 79000
    },
    {
      "epoch": 6.071946841823875,
      "grad_norm": 4.612709999084473,
      "learning_rate": 1.9640647674329795e-05,
      "loss": 0.2528,
      "step": 79500
    },
    {
      "epoch": 6.110135186741006,
      "grad_norm": 14.811664581298828,
      "learning_rate": 1.944970594974414e-05,
      "loss": 0.2592,
      "step": 80000
    },
    {
      "epoch": 6.1483235316581375,
      "grad_norm": 23.475570678710938,
      "learning_rate": 1.9258764225158482e-05,
      "loss": 0.2659,
      "step": 80500
    },
    {
      "epoch": 6.18651187657527,
      "grad_norm": 23.92676544189453,
      "learning_rate": 1.9067822500572827e-05,
      "loss": 0.2625,
      "step": 81000
    },
    {
      "epoch": 6.224700221492401,
      "grad_norm": 8.78023624420166,
      "learning_rate": 1.887688077598717e-05,
      "loss": 0.247,
      "step": 81500
    },
    {
      "epoch": 6.262888566409532,
      "grad_norm": 7.776878833770752,
      "learning_rate": 1.8685939051401514e-05,
      "loss": 0.2655,
      "step": 82000
    },
    {
      "epoch": 6.301076911326663,
      "grad_norm": 8.707993507385254,
      "learning_rate": 1.849499732681586e-05,
      "loss": 0.2552,
      "step": 82500
    },
    {
      "epoch": 6.339265256243794,
      "grad_norm": 3.1587865352630615,
      "learning_rate": 1.83040556022302e-05,
      "loss": 0.2701,
      "step": 83000
    },
    {
      "epoch": 6.3774536011609255,
      "grad_norm": 6.084921836853027,
      "learning_rate": 1.8113113877644545e-05,
      "loss": 0.2642,
      "step": 83500
    },
    {
      "epoch": 6.415641946078057,
      "grad_norm": 17.81154441833496,
      "learning_rate": 1.7922172153058887e-05,
      "loss": 0.2606,
      "step": 84000
    },
    {
      "epoch": 6.453830290995188,
      "grad_norm": 8.901405334472656,
      "learning_rate": 1.7731230428473232e-05,
      "loss": 0.2759,
      "step": 84500
    },
    {
      "epoch": 6.49201863591232,
      "grad_norm": 1.5383175611495972,
      "learning_rate": 1.7540288703887577e-05,
      "loss": 0.2483,
      "step": 85000
    },
    {
      "epoch": 6.530206980829451,
      "grad_norm": 3.060898780822754,
      "learning_rate": 1.734934697930192e-05,
      "loss": 0.2669,
      "step": 85500
    },
    {
      "epoch": 6.568395325746582,
      "grad_norm": 0.5485920906066895,
      "learning_rate": 1.715840525471626e-05,
      "loss": 0.2666,
      "step": 86000
    },
    {
      "epoch": 6.6065836706637135,
      "grad_norm": 0.7110373377799988,
      "learning_rate": 1.6967463530130605e-05,
      "loss": 0.2396,
      "step": 86500
    },
    {
      "epoch": 6.644772015580845,
      "grad_norm": 0.6865261197090149,
      "learning_rate": 1.6776521805544947e-05,
      "loss": 0.2611,
      "step": 87000
    },
    {
      "epoch": 6.682960360497976,
      "grad_norm": 16.342853546142578,
      "learning_rate": 1.6585580080959292e-05,
      "loss": 0.2594,
      "step": 87500
    },
    {
      "epoch": 6.721148705415107,
      "grad_norm": 13.495363235473633,
      "learning_rate": 1.6394638356373633e-05,
      "loss": 0.2722,
      "step": 88000
    },
    {
      "epoch": 6.759337050332238,
      "grad_norm": 15.806913375854492,
      "learning_rate": 1.620369663178798e-05,
      "loss": 0.2529,
      "step": 88500
    },
    {
      "epoch": 6.7975253952493695,
      "grad_norm": 8.788561820983887,
      "learning_rate": 1.6012754907202323e-05,
      "loss": 0.237,
      "step": 89000
    },
    {
      "epoch": 6.835713740166501,
      "grad_norm": 0.16245268285274506,
      "learning_rate": 1.5821813182616665e-05,
      "loss": 0.2726,
      "step": 89500
    },
    {
      "epoch": 6.873902085083633,
      "grad_norm": 10.327065467834473,
      "learning_rate": 1.563087145803101e-05,
      "loss": 0.2708,
      "step": 90000
    },
    {
      "epoch": 6.912090430000764,
      "grad_norm": 3.2532734870910645,
      "learning_rate": 1.543992973344535e-05,
      "loss": 0.2624,
      "step": 90500
    },
    {
      "epoch": 6.950278774917895,
      "grad_norm": 19.983203887939453,
      "learning_rate": 1.5248988008859697e-05,
      "loss": 0.2571,
      "step": 91000
    },
    {
      "epoch": 6.988467119835026,
      "grad_norm": 0.5071754455566406,
      "learning_rate": 1.5058046284274042e-05,
      "loss": 0.2473,
      "step": 91500
    },
    {
      "epoch": 7.0266554647521575,
      "grad_norm": 0.25237005949020386,
      "learning_rate": 1.4867104559688383e-05,
      "loss": 0.2613,
      "step": 92000
    },
    {
      "epoch": 7.064843809669289,
      "grad_norm": 7.927555084228516,
      "learning_rate": 1.4676162835102728e-05,
      "loss": 0.2458,
      "step": 92500
    },
    {
      "epoch": 7.10303215458642,
      "grad_norm": 0.27665430307388306,
      "learning_rate": 1.4485221110517072e-05,
      "loss": 0.2442,
      "step": 93000
    },
    {
      "epoch": 7.141220499503551,
      "grad_norm": 9.628432273864746,
      "learning_rate": 1.4294279385931413e-05,
      "loss": 0.2522,
      "step": 93500
    },
    {
      "epoch": 7.179408844420683,
      "grad_norm": 17.736785888671875,
      "learning_rate": 1.4103337661345758e-05,
      "loss": 0.2353,
      "step": 94000
    },
    {
      "epoch": 7.217597189337814,
      "grad_norm": 0.3257204294204712,
      "learning_rate": 1.39123959367601e-05,
      "loss": 0.2502,
      "step": 94500
    },
    {
      "epoch": 7.2557855342549455,
      "grad_norm": 8.72313117980957,
      "learning_rate": 1.3721454212174445e-05,
      "loss": 0.2501,
      "step": 95000
    },
    {
      "epoch": 7.293973879172077,
      "grad_norm": 22.32418441772461,
      "learning_rate": 1.353051248758879e-05,
      "loss": 0.2497,
      "step": 95500
    },
    {
      "epoch": 7.332162224089208,
      "grad_norm": 0.1810237318277359,
      "learning_rate": 1.3339570763003132e-05,
      "loss": 0.2594,
      "step": 96000
    },
    {
      "epoch": 7.370350569006339,
      "grad_norm": 7.246575355529785,
      "learning_rate": 1.3148629038417477e-05,
      "loss": 0.239,
      "step": 96500
    },
    {
      "epoch": 7.40853891392347,
      "grad_norm": 15.216011047363281,
      "learning_rate": 1.2957687313831818e-05,
      "loss": 0.2442,
      "step": 97000
    },
    {
      "epoch": 7.4467272588406015,
      "grad_norm": 0.3825300633907318,
      "learning_rate": 1.2766745589246163e-05,
      "loss": 0.2673,
      "step": 97500
    },
    {
      "epoch": 7.484915603757733,
      "grad_norm": 2.9596152305603027,
      "learning_rate": 1.2575803864660507e-05,
      "loss": 0.2419,
      "step": 98000
    },
    {
      "epoch": 7.523103948674865,
      "grad_norm": 5.901782989501953,
      "learning_rate": 1.238486214007485e-05,
      "loss": 0.2491,
      "step": 98500
    },
    {
      "epoch": 7.561292293591996,
      "grad_norm": 0.18191255629062653,
      "learning_rate": 1.2193920415489193e-05,
      "loss": 0.2671,
      "step": 99000
    },
    {
      "epoch": 7.599480638509127,
      "grad_norm": 0.5515904426574707,
      "learning_rate": 1.2002978690903536e-05,
      "loss": 0.2569,
      "step": 99500
    },
    {
      "epoch": 7.637668983426258,
      "grad_norm": 0.7831212878227234,
      "learning_rate": 1.181203696631788e-05,
      "loss": 0.2491,
      "step": 100000
    },
    {
      "epoch": 7.6758573283433895,
      "grad_norm": 2.502971887588501,
      "learning_rate": 1.1621095241732225e-05,
      "loss": 0.2762,
      "step": 100500
    },
    {
      "epoch": 7.714045673260521,
      "grad_norm": 0.2758234739303589,
      "learning_rate": 1.1430153517146568e-05,
      "loss": 0.2462,
      "step": 101000
    },
    {
      "epoch": 7.752234018177652,
      "grad_norm": 5.099149703979492,
      "learning_rate": 1.1239211792560911e-05,
      "loss": 0.2674,
      "step": 101500
    },
    {
      "epoch": 7.790422363094783,
      "grad_norm": 0.7387757301330566,
      "learning_rate": 1.1048270067975255e-05,
      "loss": 0.2709,
      "step": 102000
    },
    {
      "epoch": 7.828610708011915,
      "grad_norm": 0.14821000397205353,
      "learning_rate": 1.0857328343389598e-05,
      "loss": 0.2709,
      "step": 102500
    },
    {
      "epoch": 7.866799052929046,
      "grad_norm": 2.279017448425293,
      "learning_rate": 1.0666386618803941e-05,
      "loss": 0.2353,
      "step": 103000
    },
    {
      "epoch": 7.9049873978461775,
      "grad_norm": 12.408669471740723,
      "learning_rate": 1.0475444894218285e-05,
      "loss": 0.251,
      "step": 103500
    },
    {
      "epoch": 7.943175742763309,
      "grad_norm": 0.6347109079360962,
      "learning_rate": 1.0284503169632628e-05,
      "loss": 0.2491,
      "step": 104000
    },
    {
      "epoch": 7.98136408768044,
      "grad_norm": 1.2214909791946411,
      "learning_rate": 1.0093561445046971e-05,
      "loss": 0.2641,
      "step": 104500
    },
    {
      "epoch": 8.019552432597571,
      "grad_norm": 10.686234474182129,
      "learning_rate": 9.902619720461316e-06,
      "loss": 0.2667,
      "step": 105000
    },
    {
      "epoch": 8.057740777514702,
      "grad_norm": 24.197954177856445,
      "learning_rate": 9.71167799587566e-06,
      "loss": 0.2332,
      "step": 105500
    },
    {
      "epoch": 8.095929122431833,
      "grad_norm": 23.34371566772461,
      "learning_rate": 9.520736271290003e-06,
      "loss": 0.2419,
      "step": 106000
    },
    {
      "epoch": 8.134117467348965,
      "grad_norm": 22.76991844177246,
      "learning_rate": 9.329794546704346e-06,
      "loss": 0.2362,
      "step": 106500
    },
    {
      "epoch": 8.172305812266096,
      "grad_norm": 19.1013240814209,
      "learning_rate": 9.13885282211869e-06,
      "loss": 0.2489,
      "step": 107000
    },
    {
      "epoch": 8.210494157183227,
      "grad_norm": 10.437552452087402,
      "learning_rate": 8.947911097533033e-06,
      "loss": 0.2295,
      "step": 107500
    },
    {
      "epoch": 8.248682502100358,
      "grad_norm": 0.577852725982666,
      "learning_rate": 8.756969372947376e-06,
      "loss": 0.2457,
      "step": 108000
    },
    {
      "epoch": 8.28687084701749,
      "grad_norm": 2.0015132427215576,
      "learning_rate": 8.56602764836172e-06,
      "loss": 0.2481,
      "step": 108500
    },
    {
      "epoch": 8.325059191934622,
      "grad_norm": 8.707850456237793,
      "learning_rate": 8.375085923776065e-06,
      "loss": 0.2498,
      "step": 109000
    },
    {
      "epoch": 8.363247536851754,
      "grad_norm": 20.52143669128418,
      "learning_rate": 8.184144199190408e-06,
      "loss": 0.2531,
      "step": 109500
    },
    {
      "epoch": 8.401435881768885,
      "grad_norm": 6.061394691467285,
      "learning_rate": 7.993202474604751e-06,
      "loss": 0.2526,
      "step": 110000
    },
    {
      "epoch": 8.439624226686016,
      "grad_norm": 9.667651176452637,
      "learning_rate": 7.802260750019095e-06,
      "loss": 0.2479,
      "step": 110500
    },
    {
      "epoch": 8.477812571603147,
      "grad_norm": 1.9141350984573364,
      "learning_rate": 7.611319025433437e-06,
      "loss": 0.2374,
      "step": 111000
    },
    {
      "epoch": 8.516000916520278,
      "grad_norm": 24.073055267333984,
      "learning_rate": 7.420377300847782e-06,
      "loss": 0.2589,
      "step": 111500
    },
    {
      "epoch": 8.55418926143741,
      "grad_norm": 0.17763416469097137,
      "learning_rate": 7.2294355762621254e-06,
      "loss": 0.2481,
      "step": 112000
    },
    {
      "epoch": 8.59237760635454,
      "grad_norm": 0.495648592710495,
      "learning_rate": 7.038493851676469e-06,
      "loss": 0.2575,
      "step": 112500
    },
    {
      "epoch": 8.630565951271672,
      "grad_norm": 0.2598142921924591,
      "learning_rate": 6.847552127090811e-06,
      "loss": 0.2281,
      "step": 113000
    },
    {
      "epoch": 8.668754296188803,
      "grad_norm": 32.1843147277832,
      "learning_rate": 6.656610402505156e-06,
      "loss": 0.2549,
      "step": 113500
    },
    {
      "epoch": 8.706942641105934,
      "grad_norm": 6.908284664154053,
      "learning_rate": 6.4656686779194996e-06,
      "loss": 0.2535,
      "step": 114000
    },
    {
      "epoch": 8.745130986023065,
      "grad_norm": 16.176288604736328,
      "learning_rate": 6.274726953333843e-06,
      "loss": 0.2674,
      "step": 114500
    },
    {
      "epoch": 8.783319330940197,
      "grad_norm": 9.31972885131836,
      "learning_rate": 6.083785228748186e-06,
      "loss": 0.2411,
      "step": 115000
    },
    {
      "epoch": 8.821507675857328,
      "grad_norm": 0.7511228322982788,
      "learning_rate": 5.8928435041625295e-06,
      "loss": 0.2401,
      "step": 115500
    },
    {
      "epoch": 8.859696020774459,
      "grad_norm": 0.5531994104385376,
      "learning_rate": 5.701901779576874e-06,
      "loss": 0.2266,
      "step": 116000
    },
    {
      "epoch": 8.89788436569159,
      "grad_norm": 0.46248698234558105,
      "learning_rate": 5.510960054991217e-06,
      "loss": 0.2443,
      "step": 116500
    },
    {
      "epoch": 8.936072710608721,
      "grad_norm": 8.603412628173828,
      "learning_rate": 5.320018330405561e-06,
      "loss": 0.2515,
      "step": 117000
    },
    {
      "epoch": 8.974261055525854,
      "grad_norm": 0.5210448503494263,
      "learning_rate": 5.129076605819904e-06,
      "loss": 0.2487,
      "step": 117500
    },
    {
      "epoch": 9.012449400442986,
      "grad_norm": 13.059374809265137,
      "learning_rate": 4.938134881234248e-06,
      "loss": 0.2455,
      "step": 118000
    },
    {
      "epoch": 9.050637745360117,
      "grad_norm": 0.21990182995796204,
      "learning_rate": 4.747193156648591e-06,
      "loss": 0.2211,
      "step": 118500
    },
    {
      "epoch": 9.088826090277248,
      "grad_norm": 17.909042358398438,
      "learning_rate": 4.5562514320629344e-06,
      "loss": 0.2661,
      "step": 119000
    },
    {
      "epoch": 9.12701443519438,
      "grad_norm": 6.518310070037842,
      "learning_rate": 4.365309707477279e-06,
      "loss": 0.2348,
      "step": 119500
    },
    {
      "epoch": 9.16520278011151,
      "grad_norm": 0.22904658317565918,
      "learning_rate": 4.174367982891621e-06,
      "loss": 0.246,
      "step": 120000
    },
    {
      "epoch": 9.203391125028642,
      "grad_norm": 29.384719848632812,
      "learning_rate": 3.983426258305965e-06,
      "loss": 0.2325,
      "step": 120500
    },
    {
      "epoch": 9.241579469945773,
      "grad_norm": 6.292439937591553,
      "learning_rate": 3.7924845337203086e-06,
      "loss": 0.2584,
      "step": 121000
    },
    {
      "epoch": 9.279767814862904,
      "grad_norm": 0.28315114974975586,
      "learning_rate": 3.6015428091346523e-06,
      "loss": 0.2424,
      "step": 121500
    },
    {
      "epoch": 9.317956159780035,
      "grad_norm": 0.668682336807251,
      "learning_rate": 3.4106010845489956e-06,
      "loss": 0.2341,
      "step": 122000
    },
    {
      "epoch": 9.356144504697166,
      "grad_norm": 0.11372780054807663,
      "learning_rate": 3.2196593599633394e-06,
      "loss": 0.2598,
      "step": 122500
    },
    {
      "epoch": 9.394332849614297,
      "grad_norm": 11.242597579956055,
      "learning_rate": 3.0287176353776827e-06,
      "loss": 0.2417,
      "step": 123000
    },
    {
      "epoch": 9.432521194531429,
      "grad_norm": 0.6659205555915833,
      "learning_rate": 2.8377759107920264e-06,
      "loss": 0.2636,
      "step": 123500
    },
    {
      "epoch": 9.47070953944856,
      "grad_norm": 12.812040328979492,
      "learning_rate": 2.6468341862063698e-06,
      "loss": 0.258,
      "step": 124000
    },
    {
      "epoch": 9.508897884365691,
      "grad_norm": 0.230610191822052,
      "learning_rate": 2.4558924616207135e-06,
      "loss": 0.2428,
      "step": 124500
    },
    {
      "epoch": 9.547086229282822,
      "grad_norm": 0.4154537618160248,
      "learning_rate": 2.264950737035057e-06,
      "loss": 0.2622,
      "step": 125000
    },
    {
      "epoch": 9.585274574199953,
      "grad_norm": 0.44310805201530457,
      "learning_rate": 2.0740090124494006e-06,
      "loss": 0.2497,
      "step": 125500
    },
    {
      "epoch": 9.623462919117085,
      "grad_norm": 12.139286041259766,
      "learning_rate": 1.883067287863744e-06,
      "loss": 0.2312,
      "step": 126000
    },
    {
      "epoch": 9.661651264034216,
      "grad_norm": 0.5629482269287109,
      "learning_rate": 1.6921255632780876e-06,
      "loss": 0.2247,
      "step": 126500
    },
    {
      "epoch": 9.699839608951349,
      "grad_norm": 29.848526000976562,
      "learning_rate": 1.501183838692431e-06,
      "loss": 0.2228,
      "step": 127000
    },
    {
      "epoch": 9.73802795386848,
      "grad_norm": 9.266294479370117,
      "learning_rate": 1.3102421141067747e-06,
      "loss": 0.2481,
      "step": 127500
    },
    {
      "epoch": 9.776216298785611,
      "grad_norm": 17.08319854736328,
      "learning_rate": 1.1193003895211182e-06,
      "loss": 0.2224,
      "step": 128000
    },
    {
      "epoch": 9.814404643702742,
      "grad_norm": 24.7939510345459,
      "learning_rate": 9.283586649354618e-07,
      "loss": 0.2354,
      "step": 128500
    },
    {
      "epoch": 9.852592988619874,
      "grad_norm": 0.6001126766204834,
      "learning_rate": 7.374169403498053e-07,
      "loss": 0.2804,
      "step": 129000
    },
    {
      "epoch": 9.890781333537005,
      "grad_norm": 104.6762466430664,
      "learning_rate": 5.464752157641488e-07,
      "loss": 0.2382,
      "step": 129500
    },
    {
      "epoch": 9.928969678454136,
      "grad_norm": 8.571264266967773,
      "learning_rate": 3.5553349117849236e-07,
      "loss": 0.2576,
      "step": 130000
    },
    {
      "epoch": 9.967158023371267,
      "grad_norm": 30.01688003540039,
      "learning_rate": 1.6459176659283587e-07,
      "loss": 0.2199,
      "step": 130500
    },
    {
      "epoch": 10.0,
      "step": 130930,
      "total_flos": 2.4450933594903552e+17,
      "train_loss": 0.28238448816979894,
      "train_runtime": 20052.9214,
      "train_samples_per_second": 52.233,
      "train_steps_per_second": 6.529
    }
  ],
  "logging_steps": 500,
  "max_steps": 130930,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.4450933594903552e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
