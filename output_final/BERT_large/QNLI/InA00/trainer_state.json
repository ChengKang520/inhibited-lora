{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 130930,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.038188344917131294,
      "grad_norm": 18.360958099365234,
      "learning_rate": 4.980944015886352e-05,
      "loss": 0.6184,
      "step": 500
    },
    {
      "epoch": 0.07637668983426259,
      "grad_norm": 5.784853458404541,
      "learning_rate": 4.961849843427786e-05,
      "loss": 0.459,
      "step": 1000
    },
    {
      "epoch": 0.11456503475139387,
      "grad_norm": 2.51664137840271,
      "learning_rate": 4.94275567096922e-05,
      "loss": 0.4174,
      "step": 1500
    },
    {
      "epoch": 0.15275337966852517,
      "grad_norm": 10.682733535766602,
      "learning_rate": 4.923661498510655e-05,
      "loss": 0.398,
      "step": 2000
    },
    {
      "epoch": 0.19094172458565645,
      "grad_norm": 15.745842933654785,
      "learning_rate": 4.904567326052089e-05,
      "loss": 0.3894,
      "step": 2500
    },
    {
      "epoch": 0.22913006950278775,
      "grad_norm": 12.938901901245117,
      "learning_rate": 4.885473153593523e-05,
      "loss": 0.3903,
      "step": 3000
    },
    {
      "epoch": 0.26731841441991905,
      "grad_norm": 4.350727558135986,
      "learning_rate": 4.866378981134958e-05,
      "loss": 0.3823,
      "step": 3500
    },
    {
      "epoch": 0.30550675933705035,
      "grad_norm": 7.401341915130615,
      "learning_rate": 4.847284808676392e-05,
      "loss": 0.3678,
      "step": 4000
    },
    {
      "epoch": 0.34369510425418165,
      "grad_norm": 6.7927045822143555,
      "learning_rate": 4.828190636217826e-05,
      "loss": 0.3682,
      "step": 4500
    },
    {
      "epoch": 0.3818834491713129,
      "grad_norm": 5.334386825561523,
      "learning_rate": 4.809096463759261e-05,
      "loss": 0.3676,
      "step": 5000
    },
    {
      "epoch": 0.4200717940884442,
      "grad_norm": 1.8467634916305542,
      "learning_rate": 4.790002291300695e-05,
      "loss": 0.367,
      "step": 5500
    },
    {
      "epoch": 0.4582601390055755,
      "grad_norm": 3.8947274684906006,
      "learning_rate": 4.7709081188421295e-05,
      "loss": 0.3636,
      "step": 6000
    },
    {
      "epoch": 0.4964484839227068,
      "grad_norm": 9.365954399108887,
      "learning_rate": 4.7518139463835637e-05,
      "loss": 0.3627,
      "step": 6500
    },
    {
      "epoch": 0.5346368288398381,
      "grad_norm": 8.389982223510742,
      "learning_rate": 4.7327197739249985e-05,
      "loss": 0.3575,
      "step": 7000
    },
    {
      "epoch": 0.5728251737569694,
      "grad_norm": 3.5220565795898438,
      "learning_rate": 4.713625601466433e-05,
      "loss": 0.3491,
      "step": 7500
    },
    {
      "epoch": 0.6110135186741007,
      "grad_norm": 8.75337028503418,
      "learning_rate": 4.694531429007867e-05,
      "loss": 0.3472,
      "step": 8000
    },
    {
      "epoch": 0.649201863591232,
      "grad_norm": 10.88392162322998,
      "learning_rate": 4.675437256549302e-05,
      "loss": 0.3541,
      "step": 8500
    },
    {
      "epoch": 0.6873902085083633,
      "grad_norm": 9.188912391662598,
      "learning_rate": 4.656343084090736e-05,
      "loss": 0.3415,
      "step": 9000
    },
    {
      "epoch": 0.7255785534254945,
      "grad_norm": 12.384138107299805,
      "learning_rate": 4.63724891163217e-05,
      "loss": 0.3306,
      "step": 9500
    },
    {
      "epoch": 0.7637668983426258,
      "grad_norm": 3.4579851627349854,
      "learning_rate": 4.618154739173605e-05,
      "loss": 0.3483,
      "step": 10000
    },
    {
      "epoch": 0.8019552432597571,
      "grad_norm": 2.281789541244507,
      "learning_rate": 4.599060566715039e-05,
      "loss": 0.3282,
      "step": 10500
    },
    {
      "epoch": 0.8401435881768884,
      "grad_norm": 9.428650856018066,
      "learning_rate": 4.579966394256473e-05,
      "loss": 0.3339,
      "step": 11000
    },
    {
      "epoch": 0.8783319330940197,
      "grad_norm": 8.268836975097656,
      "learning_rate": 4.560872221797908e-05,
      "loss": 0.3296,
      "step": 11500
    },
    {
      "epoch": 0.916520278011151,
      "grad_norm": 0.4855998158454895,
      "learning_rate": 4.541778049339342e-05,
      "loss": 0.3282,
      "step": 12000
    },
    {
      "epoch": 0.9547086229282823,
      "grad_norm": 2.2222981452941895,
      "learning_rate": 4.522683876880776e-05,
      "loss": 0.3386,
      "step": 12500
    },
    {
      "epoch": 0.9928969678454136,
      "grad_norm": 0.8433657288551331,
      "learning_rate": 4.5035897044222105e-05,
      "loss": 0.3348,
      "step": 13000
    },
    {
      "epoch": 1.0310853127625448,
      "grad_norm": 6.2412285804748535,
      "learning_rate": 4.484495531963645e-05,
      "loss": 0.3212,
      "step": 13500
    },
    {
      "epoch": 1.0692736576796762,
      "grad_norm": 2.8477301597595215,
      "learning_rate": 4.4654013595050795e-05,
      "loss": 0.3232,
      "step": 14000
    },
    {
      "epoch": 1.1074620025968074,
      "grad_norm": 5.843940734863281,
      "learning_rate": 4.4463071870465137e-05,
      "loss": 0.3258,
      "step": 14500
    },
    {
      "epoch": 1.1456503475139388,
      "grad_norm": 9.273938179016113,
      "learning_rate": 4.427213014587948e-05,
      "loss": 0.333,
      "step": 15000
    },
    {
      "epoch": 1.18383869243107,
      "grad_norm": 4.433589458465576,
      "learning_rate": 4.408118842129382e-05,
      "loss": 0.3242,
      "step": 15500
    },
    {
      "epoch": 1.2220270373482014,
      "grad_norm": 10.67694091796875,
      "learning_rate": 4.389024669670817e-05,
      "loss": 0.3084,
      "step": 16000
    },
    {
      "epoch": 1.2602153822653326,
      "grad_norm": 1.1691218614578247,
      "learning_rate": 4.369930497212251e-05,
      "loss": 0.3214,
      "step": 16500
    },
    {
      "epoch": 1.298403727182464,
      "grad_norm": 17.590309143066406,
      "learning_rate": 4.350836324753685e-05,
      "loss": 0.3164,
      "step": 17000
    },
    {
      "epoch": 1.3365920720995952,
      "grad_norm": 10.901020050048828,
      "learning_rate": 4.331742152295119e-05,
      "loss": 0.3222,
      "step": 17500
    },
    {
      "epoch": 1.3747804170167264,
      "grad_norm": 1.4415346384048462,
      "learning_rate": 4.312647979836554e-05,
      "loss": 0.3198,
      "step": 18000
    },
    {
      "epoch": 1.4129687619338578,
      "grad_norm": 7.912034034729004,
      "learning_rate": 4.293553807377988e-05,
      "loss": 0.3128,
      "step": 18500
    },
    {
      "epoch": 1.4511571068509892,
      "grad_norm": 0.6350243091583252,
      "learning_rate": 4.2744596349194225e-05,
      "loss": 0.3088,
      "step": 19000
    },
    {
      "epoch": 1.4893454517681204,
      "grad_norm": 8.652159690856934,
      "learning_rate": 4.2553654624608566e-05,
      "loss": 0.2991,
      "step": 19500
    },
    {
      "epoch": 1.5275337966852516,
      "grad_norm": 5.342833518981934,
      "learning_rate": 4.2362712900022915e-05,
      "loss": 0.3065,
      "step": 20000
    },
    {
      "epoch": 1.565722141602383,
      "grad_norm": 1.5920817852020264,
      "learning_rate": 4.2171771175437256e-05,
      "loss": 0.3192,
      "step": 20500
    },
    {
      "epoch": 1.6039104865195144,
      "grad_norm": 8.042941093444824,
      "learning_rate": 4.19808294508516e-05,
      "loss": 0.3219,
      "step": 21000
    },
    {
      "epoch": 1.6420988314366456,
      "grad_norm": 7.042696475982666,
      "learning_rate": 4.1789887726265946e-05,
      "loss": 0.3043,
      "step": 21500
    },
    {
      "epoch": 1.6802871763537768,
      "grad_norm": 11.454668998718262,
      "learning_rate": 4.159894600168029e-05,
      "loss": 0.3092,
      "step": 22000
    },
    {
      "epoch": 1.718475521270908,
      "grad_norm": 8.244353294372559,
      "learning_rate": 4.140800427709463e-05,
      "loss": 0.3163,
      "step": 22500
    },
    {
      "epoch": 1.7566638661880394,
      "grad_norm": 2.3262381553649902,
      "learning_rate": 4.121706255250898e-05,
      "loss": 0.3175,
      "step": 23000
    },
    {
      "epoch": 1.7948522111051708,
      "grad_norm": 4.908867359161377,
      "learning_rate": 4.102612082792332e-05,
      "loss": 0.3189,
      "step": 23500
    },
    {
      "epoch": 1.833040556022302,
      "grad_norm": 7.171950340270996,
      "learning_rate": 4.083517910333766e-05,
      "loss": 0.3141,
      "step": 24000
    },
    {
      "epoch": 1.8712289009394332,
      "grad_norm": 10.330987930297852,
      "learning_rate": 4.064423737875201e-05,
      "loss": 0.3165,
      "step": 24500
    },
    {
      "epoch": 1.9094172458565646,
      "grad_norm": 15.190910339355469,
      "learning_rate": 4.045329565416635e-05,
      "loss": 0.3044,
      "step": 25000
    },
    {
      "epoch": 1.947605590773696,
      "grad_norm": 3.767911672592163,
      "learning_rate": 4.026235392958069e-05,
      "loss": 0.3137,
      "step": 25500
    },
    {
      "epoch": 1.9857939356908272,
      "grad_norm": 0.4239515960216522,
      "learning_rate": 4.0071412204995035e-05,
      "loss": 0.3011,
      "step": 26000
    },
    {
      "epoch": 2.0239822806079584,
      "grad_norm": 11.946000099182129,
      "learning_rate": 3.988047048040938e-05,
      "loss": 0.316,
      "step": 26500
    },
    {
      "epoch": 2.0621706255250896,
      "grad_norm": 10.278236389160156,
      "learning_rate": 3.9689528755823725e-05,
      "loss": 0.2748,
      "step": 27000
    },
    {
      "epoch": 2.100358970442221,
      "grad_norm": 22.435665130615234,
      "learning_rate": 3.9498587031238066e-05,
      "loss": 0.3062,
      "step": 27500
    },
    {
      "epoch": 2.1385473153593524,
      "grad_norm": 7.453586578369141,
      "learning_rate": 3.9307645306652415e-05,
      "loss": 0.2984,
      "step": 28000
    },
    {
      "epoch": 2.1767356602764836,
      "grad_norm": 14.602716445922852,
      "learning_rate": 3.9116703582066756e-05,
      "loss": 0.3033,
      "step": 28500
    },
    {
      "epoch": 2.2149240051936148,
      "grad_norm": 0.3789125084877014,
      "learning_rate": 3.89257618574811e-05,
      "loss": 0.2907,
      "step": 29000
    },
    {
      "epoch": 2.2531123501107464,
      "grad_norm": 0.6781274676322937,
      "learning_rate": 3.8734820132895446e-05,
      "loss": 0.2977,
      "step": 29500
    },
    {
      "epoch": 2.2913006950278776,
      "grad_norm": 3.4225807189941406,
      "learning_rate": 3.854387840830979e-05,
      "loss": 0.2895,
      "step": 30000
    },
    {
      "epoch": 2.3294890399450088,
      "grad_norm": 15.407108306884766,
      "learning_rate": 3.835293668372413e-05,
      "loss": 0.3053,
      "step": 30500
    },
    {
      "epoch": 2.36767738486214,
      "grad_norm": 2.92846417427063,
      "learning_rate": 3.816199495913848e-05,
      "loss": 0.2882,
      "step": 31000
    },
    {
      "epoch": 2.405865729779271,
      "grad_norm": 6.330750465393066,
      "learning_rate": 3.797105323455282e-05,
      "loss": 0.3008,
      "step": 31500
    },
    {
      "epoch": 2.444054074696403,
      "grad_norm": 12.26751708984375,
      "learning_rate": 3.778011150996716e-05,
      "loss": 0.308,
      "step": 32000
    },
    {
      "epoch": 2.482242419613534,
      "grad_norm": 15.937216758728027,
      "learning_rate": 3.75891697853815e-05,
      "loss": 0.2856,
      "step": 32500
    },
    {
      "epoch": 2.520430764530665,
      "grad_norm": 7.195240497589111,
      "learning_rate": 3.739822806079585e-05,
      "loss": 0.3017,
      "step": 33000
    },
    {
      "epoch": 2.558619109447797,
      "grad_norm": 0.5617009401321411,
      "learning_rate": 3.720728633621019e-05,
      "loss": 0.2895,
      "step": 33500
    },
    {
      "epoch": 2.596807454364928,
      "grad_norm": 17.842260360717773,
      "learning_rate": 3.7016344611624534e-05,
      "loss": 0.3025,
      "step": 34000
    },
    {
      "epoch": 2.634995799282059,
      "grad_norm": 8.97860050201416,
      "learning_rate": 3.6825402887038876e-05,
      "loss": 0.3004,
      "step": 34500
    },
    {
      "epoch": 2.6731841441991904,
      "grad_norm": 12.784257888793945,
      "learning_rate": 3.663446116245322e-05,
      "loss": 0.3108,
      "step": 35000
    },
    {
      "epoch": 2.7113724891163216,
      "grad_norm": 25.577070236206055,
      "learning_rate": 3.644351943786756e-05,
      "loss": 0.3029,
      "step": 35500
    },
    {
      "epoch": 2.7495608340334527,
      "grad_norm": 0.9964712858200073,
      "learning_rate": 3.625257771328191e-05,
      "loss": 0.2983,
      "step": 36000
    },
    {
      "epoch": 2.7877491789505844,
      "grad_norm": 13.49460506439209,
      "learning_rate": 3.606163598869625e-05,
      "loss": 0.3055,
      "step": 36500
    },
    {
      "epoch": 2.8259375238677156,
      "grad_norm": 7.361420631408691,
      "learning_rate": 3.587069426411059e-05,
      "loss": 0.2848,
      "step": 37000
    },
    {
      "epoch": 2.8641258687848468,
      "grad_norm": 1.112704873085022,
      "learning_rate": 3.567975253952494e-05,
      "loss": 0.2963,
      "step": 37500
    },
    {
      "epoch": 2.9023142137019784,
      "grad_norm": 7.998281955718994,
      "learning_rate": 3.548881081493928e-05,
      "loss": 0.3023,
      "step": 38000
    },
    {
      "epoch": 2.9405025586191096,
      "grad_norm": 11.416357040405273,
      "learning_rate": 3.529786909035362e-05,
      "loss": 0.3027,
      "step": 38500
    },
    {
      "epoch": 2.9786909035362408,
      "grad_norm": 21.185508728027344,
      "learning_rate": 3.510692736576797e-05,
      "loss": 0.3024,
      "step": 39000
    },
    {
      "epoch": 3.016879248453372,
      "grad_norm": 0.5628984570503235,
      "learning_rate": 3.491598564118231e-05,
      "loss": 0.2792,
      "step": 39500
    },
    {
      "epoch": 3.055067593370503,
      "grad_norm": 0.3315107524394989,
      "learning_rate": 3.4725043916596654e-05,
      "loss": 0.2875,
      "step": 40000
    },
    {
      "epoch": 3.093255938287635,
      "grad_norm": 1.0509530305862427,
      "learning_rate": 3.4534102192010996e-05,
      "loss": 0.2777,
      "step": 40500
    },
    {
      "epoch": 3.131444283204766,
      "grad_norm": 12.508663177490234,
      "learning_rate": 3.4343160467425344e-05,
      "loss": 0.2946,
      "step": 41000
    },
    {
      "epoch": 3.169632628121897,
      "grad_norm": 0.3088963031768799,
      "learning_rate": 3.4152218742839686e-05,
      "loss": 0.277,
      "step": 41500
    },
    {
      "epoch": 3.2078209730390284,
      "grad_norm": 0.6955169439315796,
      "learning_rate": 3.396127701825403e-05,
      "loss": 0.286,
      "step": 42000
    },
    {
      "epoch": 3.24600931795616,
      "grad_norm": 24.983659744262695,
      "learning_rate": 3.3770335293668376e-05,
      "loss": 0.273,
      "step": 42500
    },
    {
      "epoch": 3.284197662873291,
      "grad_norm": 15.029561996459961,
      "learning_rate": 3.357939356908272e-05,
      "loss": 0.2966,
      "step": 43000
    },
    {
      "epoch": 3.3223860077904224,
      "grad_norm": 13.966164588928223,
      "learning_rate": 3.338845184449706e-05,
      "loss": 0.2783,
      "step": 43500
    },
    {
      "epoch": 3.3605743527075536,
      "grad_norm": 0.6207005381584167,
      "learning_rate": 3.319751011991141e-05,
      "loss": 0.2879,
      "step": 44000
    },
    {
      "epoch": 3.3987626976246847,
      "grad_norm": 12.50955581665039,
      "learning_rate": 3.300656839532575e-05,
      "loss": 0.2807,
      "step": 44500
    },
    {
      "epoch": 3.4369510425418164,
      "grad_norm": 9.484294891357422,
      "learning_rate": 3.281562667074009e-05,
      "loss": 0.267,
      "step": 45000
    },
    {
      "epoch": 3.4751393874589476,
      "grad_norm": 0.2596403658390045,
      "learning_rate": 3.262468494615444e-05,
      "loss": 0.276,
      "step": 45500
    },
    {
      "epoch": 3.5133277323760788,
      "grad_norm": 13.48850154876709,
      "learning_rate": 3.243374322156878e-05,
      "loss": 0.2828,
      "step": 46000
    },
    {
      "epoch": 3.55151607729321,
      "grad_norm": 0.6291555762290955,
      "learning_rate": 3.224280149698312e-05,
      "loss": 0.2929,
      "step": 46500
    },
    {
      "epoch": 3.5897044222103416,
      "grad_norm": 7.284631252288818,
      "learning_rate": 3.2051859772397464e-05,
      "loss": 0.2755,
      "step": 47000
    },
    {
      "epoch": 3.6278927671274728,
      "grad_norm": 14.974239349365234,
      "learning_rate": 3.186091804781181e-05,
      "loss": 0.295,
      "step": 47500
    },
    {
      "epoch": 3.666081112044604,
      "grad_norm": 1.2842857837677002,
      "learning_rate": 3.1669976323226154e-05,
      "loss": 0.2761,
      "step": 48000
    },
    {
      "epoch": 3.704269456961735,
      "grad_norm": 7.948650360107422,
      "learning_rate": 3.1479034598640496e-05,
      "loss": 0.2795,
      "step": 48500
    },
    {
      "epoch": 3.7424578018788663,
      "grad_norm": 18.784486770629883,
      "learning_rate": 3.1288092874054844e-05,
      "loss": 0.2762,
      "step": 49000
    },
    {
      "epoch": 3.780646146795998,
      "grad_norm": 0.7772231101989746,
      "learning_rate": 3.1097151149469186e-05,
      "loss": 0.282,
      "step": 49500
    },
    {
      "epoch": 3.818834491713129,
      "grad_norm": 1.2413908243179321,
      "learning_rate": 3.090620942488353e-05,
      "loss": 0.3,
      "step": 50000
    },
    {
      "epoch": 3.8570228366302604,
      "grad_norm": 15.944286346435547,
      "learning_rate": 3.0715267700297876e-05,
      "loss": 0.2916,
      "step": 50500
    },
    {
      "epoch": 3.895211181547392,
      "grad_norm": 7.264745712280273,
      "learning_rate": 3.052432597571222e-05,
      "loss": 0.296,
      "step": 51000
    },
    {
      "epoch": 3.933399526464523,
      "grad_norm": 38.19340133666992,
      "learning_rate": 3.0333384251126556e-05,
      "loss": 0.2876,
      "step": 51500
    },
    {
      "epoch": 3.9715878713816544,
      "grad_norm": 8.109071731567383,
      "learning_rate": 3.0142442526540904e-05,
      "loss": 0.2784,
      "step": 52000
    },
    {
      "epoch": 4.009776216298786,
      "grad_norm": 7.5704474449157715,
      "learning_rate": 2.9951500801955246e-05,
      "loss": 0.2679,
      "step": 52500
    },
    {
      "epoch": 4.047964561215917,
      "grad_norm": 0.6916011571884155,
      "learning_rate": 2.9760559077369587e-05,
      "loss": 0.2673,
      "step": 53000
    },
    {
      "epoch": 4.086152906133048,
      "grad_norm": 9.47843074798584,
      "learning_rate": 2.956961735278393e-05,
      "loss": 0.2743,
      "step": 53500
    },
    {
      "epoch": 4.124341251050179,
      "grad_norm": 11.260448455810547,
      "learning_rate": 2.9378675628198277e-05,
      "loss": 0.2619,
      "step": 54000
    },
    {
      "epoch": 4.162529595967311,
      "grad_norm": 8.411959648132324,
      "learning_rate": 2.918773390361262e-05,
      "loss": 0.2856,
      "step": 54500
    },
    {
      "epoch": 4.200717940884442,
      "grad_norm": 0.6121649742126465,
      "learning_rate": 2.899679217902696e-05,
      "loss": 0.2616,
      "step": 55000
    },
    {
      "epoch": 4.238906285801574,
      "grad_norm": 1.1079773902893066,
      "learning_rate": 2.880585045444131e-05,
      "loss": 0.2793,
      "step": 55500
    },
    {
      "epoch": 4.277094630718705,
      "grad_norm": 27.089139938354492,
      "learning_rate": 2.861490872985565e-05,
      "loss": 0.2749,
      "step": 56000
    },
    {
      "epoch": 4.315282975635836,
      "grad_norm": 17.4556941986084,
      "learning_rate": 2.8423967005269992e-05,
      "loss": 0.2972,
      "step": 56500
    },
    {
      "epoch": 4.353471320552967,
      "grad_norm": 2.409071922302246,
      "learning_rate": 2.8233025280684337e-05,
      "loss": 0.2827,
      "step": 57000
    },
    {
      "epoch": 4.391659665470098,
      "grad_norm": 0.4873517155647278,
      "learning_rate": 2.804208355609868e-05,
      "loss": 0.2834,
      "step": 57500
    },
    {
      "epoch": 4.4298480103872295,
      "grad_norm": 12.188663482666016,
      "learning_rate": 2.7851141831513024e-05,
      "loss": 0.2618,
      "step": 58000
    },
    {
      "epoch": 4.468036355304361,
      "grad_norm": 7.655147075653076,
      "learning_rate": 2.766020010692737e-05,
      "loss": 0.2671,
      "step": 58500
    },
    {
      "epoch": 4.506224700221493,
      "grad_norm": 0.4121517539024353,
      "learning_rate": 2.746925838234171e-05,
      "loss": 0.268,
      "step": 59000
    },
    {
      "epoch": 4.544413045138624,
      "grad_norm": 17.718610763549805,
      "learning_rate": 2.7278316657756052e-05,
      "loss": 0.2767,
      "step": 59500
    },
    {
      "epoch": 4.582601390055755,
      "grad_norm": 0.7225168943405151,
      "learning_rate": 2.7087374933170394e-05,
      "loss": 0.2711,
      "step": 60000
    },
    {
      "epoch": 4.620789734972886,
      "grad_norm": 15.518265724182129,
      "learning_rate": 2.6896433208584742e-05,
      "loss": 0.2693,
      "step": 60500
    },
    {
      "epoch": 4.6589780798900176,
      "grad_norm": 28.41982078552246,
      "learning_rate": 2.6705491483999084e-05,
      "loss": 0.2829,
      "step": 61000
    },
    {
      "epoch": 4.697166424807149,
      "grad_norm": 25.81138038635254,
      "learning_rate": 2.6514549759413426e-05,
      "loss": 0.262,
      "step": 61500
    },
    {
      "epoch": 4.73535476972428,
      "grad_norm": 0.47597429156303406,
      "learning_rate": 2.6323608034827774e-05,
      "loss": 0.2671,
      "step": 62000
    },
    {
      "epoch": 4.773543114641411,
      "grad_norm": 0.3632349669933319,
      "learning_rate": 2.6132666310242116e-05,
      "loss": 0.2552,
      "step": 62500
    },
    {
      "epoch": 4.811731459558542,
      "grad_norm": 0.6447281241416931,
      "learning_rate": 2.5941724585656457e-05,
      "loss": 0.2585,
      "step": 63000
    },
    {
      "epoch": 4.849919804475674,
      "grad_norm": 0.5180572867393494,
      "learning_rate": 2.5750782861070806e-05,
      "loss": 0.2805,
      "step": 63500
    },
    {
      "epoch": 4.888108149392806,
      "grad_norm": 26.37303924560547,
      "learning_rate": 2.5559841136485147e-05,
      "loss": 0.2556,
      "step": 64000
    },
    {
      "epoch": 4.926296494309937,
      "grad_norm": 14.301092147827148,
      "learning_rate": 2.536889941189949e-05,
      "loss": 0.2662,
      "step": 64500
    },
    {
      "epoch": 4.964484839227068,
      "grad_norm": 3.4961001873016357,
      "learning_rate": 2.5177957687313837e-05,
      "loss": 0.2687,
      "step": 65000
    },
    {
      "epoch": 5.002673184144199,
      "grad_norm": 9.513411521911621,
      "learning_rate": 2.498701596272818e-05,
      "loss": 0.2618,
      "step": 65500
    },
    {
      "epoch": 5.04086152906133,
      "grad_norm": 0.4332487881183624,
      "learning_rate": 2.479607423814252e-05,
      "loss": 0.261,
      "step": 66000
    },
    {
      "epoch": 5.0790498739784615,
      "grad_norm": 1.2555571794509888,
      "learning_rate": 2.4605132513556866e-05,
      "loss": 0.2461,
      "step": 66500
    },
    {
      "epoch": 5.117238218895593,
      "grad_norm": 0.7096536755561829,
      "learning_rate": 2.4414190788971207e-05,
      "loss": 0.2784,
      "step": 67000
    },
    {
      "epoch": 5.155426563812725,
      "grad_norm": 15.398632049560547,
      "learning_rate": 2.422324906438555e-05,
      "loss": 0.2626,
      "step": 67500
    },
    {
      "epoch": 5.193614908729856,
      "grad_norm": 33.75374984741211,
      "learning_rate": 2.4032307339799894e-05,
      "loss": 0.2543,
      "step": 68000
    },
    {
      "epoch": 5.231803253646987,
      "grad_norm": 24.489904403686523,
      "learning_rate": 2.3841365615214235e-05,
      "loss": 0.2737,
      "step": 68500
    },
    {
      "epoch": 5.269991598564118,
      "grad_norm": 15.5056791305542,
      "learning_rate": 2.365042389062858e-05,
      "loss": 0.2744,
      "step": 69000
    },
    {
      "epoch": 5.3081799434812496,
      "grad_norm": 0.9721960425376892,
      "learning_rate": 2.3459482166042925e-05,
      "loss": 0.2599,
      "step": 69500
    },
    {
      "epoch": 5.346368288398381,
      "grad_norm": 40.99592208862305,
      "learning_rate": 2.3268540441457267e-05,
      "loss": 0.2588,
      "step": 70000
    },
    {
      "epoch": 5.384556633315512,
      "grad_norm": 0.20367437601089478,
      "learning_rate": 2.3077598716871612e-05,
      "loss": 0.2685,
      "step": 70500
    },
    {
      "epoch": 5.422744978232643,
      "grad_norm": 19.80362319946289,
      "learning_rate": 2.2886656992285954e-05,
      "loss": 0.269,
      "step": 71000
    },
    {
      "epoch": 5.460933323149774,
      "grad_norm": 1.1278958320617676,
      "learning_rate": 2.26957152677003e-05,
      "loss": 0.2502,
      "step": 71500
    },
    {
      "epoch": 5.499121668066906,
      "grad_norm": 12.88783073425293,
      "learning_rate": 2.2504773543114644e-05,
      "loss": 0.2446,
      "step": 72000
    },
    {
      "epoch": 5.537310012984038,
      "grad_norm": 10.993349075317383,
      "learning_rate": 2.2313831818528985e-05,
      "loss": 0.2579,
      "step": 72500
    },
    {
      "epoch": 5.575498357901169,
      "grad_norm": 19.62842559814453,
      "learning_rate": 2.212289009394333e-05,
      "loss": 0.2479,
      "step": 73000
    },
    {
      "epoch": 5.6136867028183,
      "grad_norm": 12.47747802734375,
      "learning_rate": 2.1931948369357675e-05,
      "loss": 0.2694,
      "step": 73500
    },
    {
      "epoch": 5.651875047735431,
      "grad_norm": 0.35605719685554504,
      "learning_rate": 2.1741006644772017e-05,
      "loss": 0.2651,
      "step": 74000
    },
    {
      "epoch": 5.690063392652562,
      "grad_norm": 0.8397257328033447,
      "learning_rate": 2.1550064920186362e-05,
      "loss": 0.2655,
      "step": 74500
    },
    {
      "epoch": 5.7282517375696935,
      "grad_norm": 42.065460205078125,
      "learning_rate": 2.1359123195600704e-05,
      "loss": 0.2389,
      "step": 75000
    },
    {
      "epoch": 5.766440082486825,
      "grad_norm": 21.64744758605957,
      "learning_rate": 2.116818147101505e-05,
      "loss": 0.267,
      "step": 75500
    },
    {
      "epoch": 5.804628427403957,
      "grad_norm": 0.7971689105033875,
      "learning_rate": 2.097723974642939e-05,
      "loss": 0.2807,
      "step": 76000
    },
    {
      "epoch": 5.842816772321088,
      "grad_norm": 13.163165092468262,
      "learning_rate": 2.0786298021843735e-05,
      "loss": 0.2629,
      "step": 76500
    },
    {
      "epoch": 5.881005117238219,
      "grad_norm": 8.748577117919922,
      "learning_rate": 2.0595356297258077e-05,
      "loss": 0.2529,
      "step": 77000
    },
    {
      "epoch": 5.91919346215535,
      "grad_norm": 19.036611557006836,
      "learning_rate": 2.040441457267242e-05,
      "loss": 0.2733,
      "step": 77500
    },
    {
      "epoch": 5.9573818070724815,
      "grad_norm": 11.571429252624512,
      "learning_rate": 2.0213472848086764e-05,
      "loss": 0.2691,
      "step": 78000
    },
    {
      "epoch": 5.995570151989613,
      "grad_norm": 7.542545318603516,
      "learning_rate": 2.002253112350111e-05,
      "loss": 0.2373,
      "step": 78500
    },
    {
      "epoch": 6.033758496906744,
      "grad_norm": 17.84356117248535,
      "learning_rate": 1.983158939891545e-05,
      "loss": 0.2587,
      "step": 79000
    },
    {
      "epoch": 6.071946841823875,
      "grad_norm": 0.10502627491950989,
      "learning_rate": 1.9640647674329795e-05,
      "loss": 0.2532,
      "step": 79500
    },
    {
      "epoch": 6.110135186741006,
      "grad_norm": 31.816417694091797,
      "learning_rate": 1.944970594974414e-05,
      "loss": 0.257,
      "step": 80000
    },
    {
      "epoch": 6.1483235316581375,
      "grad_norm": 0.6845220327377319,
      "learning_rate": 1.9258764225158482e-05,
      "loss": 0.2643,
      "step": 80500
    },
    {
      "epoch": 6.18651187657527,
      "grad_norm": 0.3453670144081116,
      "learning_rate": 1.9067822500572827e-05,
      "loss": 0.2592,
      "step": 81000
    },
    {
      "epoch": 6.224700221492401,
      "grad_norm": 1.2410495281219482,
      "learning_rate": 1.887688077598717e-05,
      "loss": 0.2331,
      "step": 81500
    },
    {
      "epoch": 6.262888566409532,
      "grad_norm": 0.30783626437187195,
      "learning_rate": 1.8685939051401514e-05,
      "loss": 0.246,
      "step": 82000
    },
    {
      "epoch": 6.301076911326663,
      "grad_norm": 14.965741157531738,
      "learning_rate": 1.849499732681586e-05,
      "loss": 0.2556,
      "step": 82500
    },
    {
      "epoch": 6.339265256243794,
      "grad_norm": 12.77785587310791,
      "learning_rate": 1.83040556022302e-05,
      "loss": 0.2738,
      "step": 83000
    },
    {
      "epoch": 6.3774536011609255,
      "grad_norm": 11.223615646362305,
      "learning_rate": 1.8113113877644545e-05,
      "loss": 0.2513,
      "step": 83500
    },
    {
      "epoch": 6.415641946078057,
      "grad_norm": 9.92819881439209,
      "learning_rate": 1.7922172153058887e-05,
      "loss": 0.2507,
      "step": 84000
    },
    {
      "epoch": 6.453830290995188,
      "grad_norm": 0.7610211968421936,
      "learning_rate": 1.7731230428473232e-05,
      "loss": 0.2518,
      "step": 84500
    },
    {
      "epoch": 6.49201863591232,
      "grad_norm": 0.5058364272117615,
      "learning_rate": 1.7540288703887577e-05,
      "loss": 0.2449,
      "step": 85000
    },
    {
      "epoch": 6.530206980829451,
      "grad_norm": 1.2486214637756348,
      "learning_rate": 1.734934697930192e-05,
      "loss": 0.25,
      "step": 85500
    },
    {
      "epoch": 6.568395325746582,
      "grad_norm": 0.8370065689086914,
      "learning_rate": 1.715840525471626e-05,
      "loss": 0.2633,
      "step": 86000
    },
    {
      "epoch": 6.6065836706637135,
      "grad_norm": 39.566036224365234,
      "learning_rate": 1.6967463530130605e-05,
      "loss": 0.2475,
      "step": 86500
    },
    {
      "epoch": 6.644772015580845,
      "grad_norm": 0.3556690216064453,
      "learning_rate": 1.6776521805544947e-05,
      "loss": 0.2573,
      "step": 87000
    },
    {
      "epoch": 6.682960360497976,
      "grad_norm": 21.98523712158203,
      "learning_rate": 1.6585580080959292e-05,
      "loss": 0.2563,
      "step": 87500
    },
    {
      "epoch": 6.721148705415107,
      "grad_norm": 23.627582550048828,
      "learning_rate": 1.6394638356373633e-05,
      "loss": 0.2556,
      "step": 88000
    },
    {
      "epoch": 6.759337050332238,
      "grad_norm": 29.03441619873047,
      "learning_rate": 1.620369663178798e-05,
      "loss": 0.2693,
      "step": 88500
    },
    {
      "epoch": 6.7975253952493695,
      "grad_norm": 8.784932136535645,
      "learning_rate": 1.6012754907202323e-05,
      "loss": 0.2399,
      "step": 89000
    },
    {
      "epoch": 6.835713740166501,
      "grad_norm": 0.20375974476337433,
      "learning_rate": 1.5821813182616665e-05,
      "loss": 0.2628,
      "step": 89500
    },
    {
      "epoch": 6.873902085083633,
      "grad_norm": 1.9944132566452026,
      "learning_rate": 1.563087145803101e-05,
      "loss": 0.2777,
      "step": 90000
    },
    {
      "epoch": 6.912090430000764,
      "grad_norm": 11.64150619506836,
      "learning_rate": 1.543992973344535e-05,
      "loss": 0.2506,
      "step": 90500
    },
    {
      "epoch": 6.950278774917895,
      "grad_norm": 15.388120651245117,
      "learning_rate": 1.5248988008859697e-05,
      "loss": 0.2599,
      "step": 91000
    },
    {
      "epoch": 6.988467119835026,
      "grad_norm": 19.221769332885742,
      "learning_rate": 1.5058046284274042e-05,
      "loss": 0.245,
      "step": 91500
    },
    {
      "epoch": 7.0266554647521575,
      "grad_norm": 0.2202683389186859,
      "learning_rate": 1.4867104559688383e-05,
      "loss": 0.2566,
      "step": 92000
    },
    {
      "epoch": 7.064843809669289,
      "grad_norm": 0.35131531953811646,
      "learning_rate": 1.4676162835102728e-05,
      "loss": 0.2462,
      "step": 92500
    },
    {
      "epoch": 7.10303215458642,
      "grad_norm": 0.3524852991104126,
      "learning_rate": 1.4485221110517072e-05,
      "loss": 0.2432,
      "step": 93000
    },
    {
      "epoch": 7.141220499503551,
      "grad_norm": 4.706295967102051,
      "learning_rate": 1.4294279385931413e-05,
      "loss": 0.2358,
      "step": 93500
    },
    {
      "epoch": 7.179408844420683,
      "grad_norm": 40.42244338989258,
      "learning_rate": 1.4103337661345758e-05,
      "loss": 0.2304,
      "step": 94000
    },
    {
      "epoch": 7.217597189337814,
      "grad_norm": 0.3660713732242584,
      "learning_rate": 1.39123959367601e-05,
      "loss": 0.259,
      "step": 94500
    },
    {
      "epoch": 7.2557855342549455,
      "grad_norm": 23.617265701293945,
      "learning_rate": 1.3721454212174445e-05,
      "loss": 0.2456,
      "step": 95000
    },
    {
      "epoch": 7.293973879172077,
      "grad_norm": 22.1768741607666,
      "learning_rate": 1.353051248758879e-05,
      "loss": 0.2612,
      "step": 95500
    },
    {
      "epoch": 7.332162224089208,
      "grad_norm": 0.09289941936731339,
      "learning_rate": 1.3339570763003132e-05,
      "loss": 0.2587,
      "step": 96000
    },
    {
      "epoch": 7.370350569006339,
      "grad_norm": 7.024740695953369,
      "learning_rate": 1.3148629038417477e-05,
      "loss": 0.2324,
      "step": 96500
    },
    {
      "epoch": 7.40853891392347,
      "grad_norm": 13.771337509155273,
      "learning_rate": 1.2957687313831818e-05,
      "loss": 0.2454,
      "step": 97000
    },
    {
      "epoch": 7.4467272588406015,
      "grad_norm": 0.15899640321731567,
      "learning_rate": 1.2766745589246163e-05,
      "loss": 0.2507,
      "step": 97500
    },
    {
      "epoch": 7.484915603757733,
      "grad_norm": 3.0604777336120605,
      "learning_rate": 1.2575803864660507e-05,
      "loss": 0.227,
      "step": 98000
    },
    {
      "epoch": 7.523103948674865,
      "grad_norm": 2.056243658065796,
      "learning_rate": 1.238486214007485e-05,
      "loss": 0.2451,
      "step": 98500
    },
    {
      "epoch": 7.561292293591996,
      "grad_norm": 0.22731666266918182,
      "learning_rate": 1.2193920415489193e-05,
      "loss": 0.2581,
      "step": 99000
    },
    {
      "epoch": 7.599480638509127,
      "grad_norm": 0.868510365486145,
      "learning_rate": 1.2002978690903536e-05,
      "loss": 0.2361,
      "step": 99500
    },
    {
      "epoch": 7.637668983426258,
      "grad_norm": 0.5302019715309143,
      "learning_rate": 1.181203696631788e-05,
      "loss": 0.2391,
      "step": 100000
    },
    {
      "epoch": 7.6758573283433895,
      "grad_norm": 7.812943935394287,
      "learning_rate": 1.1621095241732225e-05,
      "loss": 0.2644,
      "step": 100500
    },
    {
      "epoch": 7.714045673260521,
      "grad_norm": 0.20553578436374664,
      "learning_rate": 1.1430153517146568e-05,
      "loss": 0.2335,
      "step": 101000
    },
    {
      "epoch": 7.752234018177652,
      "grad_norm": 1.2908098697662354,
      "learning_rate": 1.1239211792560911e-05,
      "loss": 0.2592,
      "step": 101500
    },
    {
      "epoch": 7.790422363094783,
      "grad_norm": 1.6607424020767212,
      "learning_rate": 1.1048270067975255e-05,
      "loss": 0.2588,
      "step": 102000
    },
    {
      "epoch": 7.828610708011915,
      "grad_norm": 0.11955161392688751,
      "learning_rate": 1.0857328343389598e-05,
      "loss": 0.2634,
      "step": 102500
    },
    {
      "epoch": 7.866799052929046,
      "grad_norm": 9.294825553894043,
      "learning_rate": 1.0666386618803941e-05,
      "loss": 0.2349,
      "step": 103000
    },
    {
      "epoch": 7.9049873978461775,
      "grad_norm": 36.57953643798828,
      "learning_rate": 1.0475444894218285e-05,
      "loss": 0.2372,
      "step": 103500
    },
    {
      "epoch": 7.943175742763309,
      "grad_norm": 13.029006004333496,
      "learning_rate": 1.0284503169632628e-05,
      "loss": 0.2468,
      "step": 104000
    },
    {
      "epoch": 7.98136408768044,
      "grad_norm": 0.68025803565979,
      "learning_rate": 1.0093561445046971e-05,
      "loss": 0.2388,
      "step": 104500
    },
    {
      "epoch": 8.019552432597571,
      "grad_norm": 8.688076972961426,
      "learning_rate": 9.902619720461316e-06,
      "loss": 0.2618,
      "step": 105000
    },
    {
      "epoch": 8.057740777514702,
      "grad_norm": 19.132665634155273,
      "learning_rate": 9.71167799587566e-06,
      "loss": 0.2259,
      "step": 105500
    },
    {
      "epoch": 8.095929122431833,
      "grad_norm": 6.991322994232178,
      "learning_rate": 9.520736271290003e-06,
      "loss": 0.233,
      "step": 106000
    },
    {
      "epoch": 8.134117467348965,
      "grad_norm": 25.402162551879883,
      "learning_rate": 9.329794546704346e-06,
      "loss": 0.2368,
      "step": 106500
    },
    {
      "epoch": 8.172305812266096,
      "grad_norm": 48.061485290527344,
      "learning_rate": 9.13885282211869e-06,
      "loss": 0.2403,
      "step": 107000
    },
    {
      "epoch": 8.210494157183227,
      "grad_norm": 20.120817184448242,
      "learning_rate": 8.947911097533033e-06,
      "loss": 0.2306,
      "step": 107500
    },
    {
      "epoch": 8.248682502100358,
      "grad_norm": 0.19249387085437775,
      "learning_rate": 8.756969372947376e-06,
      "loss": 0.2382,
      "step": 108000
    },
    {
      "epoch": 8.28687084701749,
      "grad_norm": 0.7380242347717285,
      "learning_rate": 8.56602764836172e-06,
      "loss": 0.2384,
      "step": 108500
    },
    {
      "epoch": 8.325059191934622,
      "grad_norm": 0.4861539602279663,
      "learning_rate": 8.375085923776065e-06,
      "loss": 0.2316,
      "step": 109000
    },
    {
      "epoch": 8.363247536851754,
      "grad_norm": 0.6669623255729675,
      "learning_rate": 8.184144199190408e-06,
      "loss": 0.2526,
      "step": 109500
    },
    {
      "epoch": 8.401435881768885,
      "grad_norm": 20.2558650970459,
      "learning_rate": 7.993202474604751e-06,
      "loss": 0.2315,
      "step": 110000
    },
    {
      "epoch": 8.439624226686016,
      "grad_norm": 8.348401069641113,
      "learning_rate": 7.802260750019095e-06,
      "loss": 0.2302,
      "step": 110500
    },
    {
      "epoch": 8.477812571603147,
      "grad_norm": 0.538433849811554,
      "learning_rate": 7.611319025433437e-06,
      "loss": 0.2337,
      "step": 111000
    },
    {
      "epoch": 8.516000916520278,
      "grad_norm": 0.47801923751831055,
      "learning_rate": 7.420377300847782e-06,
      "loss": 0.2456,
      "step": 111500
    },
    {
      "epoch": 8.55418926143741,
      "grad_norm": 46.01856231689453,
      "learning_rate": 7.2294355762621254e-06,
      "loss": 0.2581,
      "step": 112000
    },
    {
      "epoch": 8.59237760635454,
      "grad_norm": 0.32954898476600647,
      "learning_rate": 7.038493851676469e-06,
      "loss": 0.2664,
      "step": 112500
    },
    {
      "epoch": 8.630565951271672,
      "grad_norm": 0.14442265033721924,
      "learning_rate": 6.847552127090811e-06,
      "loss": 0.2278,
      "step": 113000
    },
    {
      "epoch": 8.668754296188803,
      "grad_norm": 54.63246154785156,
      "learning_rate": 6.656610402505156e-06,
      "loss": 0.2467,
      "step": 113500
    },
    {
      "epoch": 8.706942641105934,
      "grad_norm": 15.926807403564453,
      "learning_rate": 6.4656686779194996e-06,
      "loss": 0.2405,
      "step": 114000
    },
    {
      "epoch": 8.745130986023065,
      "grad_norm": 8.922300338745117,
      "learning_rate": 6.274726953333843e-06,
      "loss": 0.2652,
      "step": 114500
    },
    {
      "epoch": 8.783319330940197,
      "grad_norm": 23.752300262451172,
      "learning_rate": 6.083785228748186e-06,
      "loss": 0.2438,
      "step": 115000
    },
    {
      "epoch": 8.821507675857328,
      "grad_norm": 1.3295265436172485,
      "learning_rate": 5.8928435041625295e-06,
      "loss": 0.228,
      "step": 115500
    },
    {
      "epoch": 8.859696020774459,
      "grad_norm": 60.28753662109375,
      "learning_rate": 5.701901779576874e-06,
      "loss": 0.2003,
      "step": 116000
    },
    {
      "epoch": 8.89788436569159,
      "grad_norm": 4.515321731567383,
      "learning_rate": 5.510960054991217e-06,
      "loss": 0.2555,
      "step": 116500
    },
    {
      "epoch": 8.936072710608721,
      "grad_norm": 18.682785034179688,
      "learning_rate": 5.320018330405561e-06,
      "loss": 0.2332,
      "step": 117000
    },
    {
      "epoch": 8.974261055525854,
      "grad_norm": 0.2555617094039917,
      "learning_rate": 5.129076605819904e-06,
      "loss": 0.2485,
      "step": 117500
    },
    {
      "epoch": 9.012449400442986,
      "grad_norm": 30.2911434173584,
      "learning_rate": 4.938134881234248e-06,
      "loss": 0.235,
      "step": 118000
    },
    {
      "epoch": 9.050637745360117,
      "grad_norm": 0.23315677046775818,
      "learning_rate": 4.747193156648591e-06,
      "loss": 0.2136,
      "step": 118500
    },
    {
      "epoch": 9.088826090277248,
      "grad_norm": 15.95649528503418,
      "learning_rate": 4.5562514320629344e-06,
      "loss": 0.2454,
      "step": 119000
    },
    {
      "epoch": 9.12701443519438,
      "grad_norm": 29.824710845947266,
      "learning_rate": 4.365309707477279e-06,
      "loss": 0.2323,
      "step": 119500
    },
    {
      "epoch": 9.16520278011151,
      "grad_norm": 0.43394342064857483,
      "learning_rate": 4.174367982891621e-06,
      "loss": 0.2356,
      "step": 120000
    },
    {
      "epoch": 9.203391125028642,
      "grad_norm": 35.335636138916016,
      "learning_rate": 3.983426258305965e-06,
      "loss": 0.2145,
      "step": 120500
    },
    {
      "epoch": 9.241579469945773,
      "grad_norm": 30.485492706298828,
      "learning_rate": 3.7924845337203086e-06,
      "loss": 0.2393,
      "step": 121000
    },
    {
      "epoch": 9.279767814862904,
      "grad_norm": 0.1884792298078537,
      "learning_rate": 3.6015428091346523e-06,
      "loss": 0.2437,
      "step": 121500
    },
    {
      "epoch": 9.317956159780035,
      "grad_norm": 0.21030384302139282,
      "learning_rate": 3.4106010845489956e-06,
      "loss": 0.2355,
      "step": 122000
    },
    {
      "epoch": 9.356144504697166,
      "grad_norm": 0.17684732377529144,
      "learning_rate": 3.2196593599633394e-06,
      "loss": 0.2494,
      "step": 122500
    },
    {
      "epoch": 9.394332849614297,
      "grad_norm": 10.550362586975098,
      "learning_rate": 3.0287176353776827e-06,
      "loss": 0.2446,
      "step": 123000
    },
    {
      "epoch": 9.432521194531429,
      "grad_norm": 4.964612007141113,
      "learning_rate": 2.8377759107920264e-06,
      "loss": 0.2635,
      "step": 123500
    },
    {
      "epoch": 9.47070953944856,
      "grad_norm": 13.844147682189941,
      "learning_rate": 2.6468341862063698e-06,
      "loss": 0.2366,
      "step": 124000
    },
    {
      "epoch": 9.508897884365691,
      "grad_norm": 0.8257298469543457,
      "learning_rate": 2.4558924616207135e-06,
      "loss": 0.2396,
      "step": 124500
    },
    {
      "epoch": 9.547086229282822,
      "grad_norm": 0.21532142162322998,
      "learning_rate": 2.264950737035057e-06,
      "loss": 0.2421,
      "step": 125000
    },
    {
      "epoch": 9.585274574199953,
      "grad_norm": 0.40758001804351807,
      "learning_rate": 2.0740090124494006e-06,
      "loss": 0.2364,
      "step": 125500
    },
    {
      "epoch": 9.623462919117085,
      "grad_norm": 0.6087943315505981,
      "learning_rate": 1.883067287863744e-06,
      "loss": 0.2362,
      "step": 126000
    },
    {
      "epoch": 9.661651264034216,
      "grad_norm": 0.35555383563041687,
      "learning_rate": 1.6921255632780876e-06,
      "loss": 0.24,
      "step": 126500
    },
    {
      "epoch": 9.699839608951349,
      "grad_norm": 37.61552047729492,
      "learning_rate": 1.501183838692431e-06,
      "loss": 0.2225,
      "step": 127000
    },
    {
      "epoch": 9.73802795386848,
      "grad_norm": 8.031669616699219,
      "learning_rate": 1.3102421141067747e-06,
      "loss": 0.2294,
      "step": 127500
    },
    {
      "epoch": 9.776216298785611,
      "grad_norm": 7.946259021759033,
      "learning_rate": 1.1193003895211182e-06,
      "loss": 0.2231,
      "step": 128000
    },
    {
      "epoch": 9.814404643702742,
      "grad_norm": 13.569364547729492,
      "learning_rate": 9.283586649354618e-07,
      "loss": 0.221,
      "step": 128500
    },
    {
      "epoch": 9.852592988619874,
      "grad_norm": 0.3404081165790558,
      "learning_rate": 7.374169403498053e-07,
      "loss": 0.2834,
      "step": 129000
    },
    {
      "epoch": 9.890781333537005,
      "grad_norm": 12.903321266174316,
      "learning_rate": 5.464752157641488e-07,
      "loss": 0.2287,
      "step": 129500
    },
    {
      "epoch": 9.928969678454136,
      "grad_norm": 16.07479476928711,
      "learning_rate": 3.5553349117849236e-07,
      "loss": 0.2465,
      "step": 130000
    },
    {
      "epoch": 9.967158023371267,
      "grad_norm": 8.59829330444336,
      "learning_rate": 1.6459176659283587e-07,
      "loss": 0.2206,
      "step": 130500
    },
    {
      "epoch": 10.0,
      "step": 130930,
      "total_flos": 2.4450933594903552e+17,
      "train_loss": 0.27781669834049266,
      "train_runtime": 19937.1482,
      "train_samples_per_second": 52.537,
      "train_steps_per_second": 6.567
    }
  ],
  "logging_steps": 500,
  "max_steps": 130930,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.4450933594903552e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
