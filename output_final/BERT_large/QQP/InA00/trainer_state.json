{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 454810,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01099360172379675,
      "grad_norm": 12.660313606262207,
      "learning_rate": 4.994514192739826e-05,
      "loss": 0.5785,
      "step": 500
    },
    {
      "epoch": 0.0219872034475935,
      "grad_norm": 5.074657917022705,
      "learning_rate": 4.989017391877927e-05,
      "loss": 0.4874,
      "step": 1000
    },
    {
      "epoch": 0.03298080517139025,
      "grad_norm": 5.384535789489746,
      "learning_rate": 4.983520591016029e-05,
      "loss": 0.4669,
      "step": 1500
    },
    {
      "epoch": 0.043974406895187,
      "grad_norm": 4.975497245788574,
      "learning_rate": 4.97802379015413e-05,
      "loss": 0.4448,
      "step": 2000
    },
    {
      "epoch": 0.05496800861898375,
      "grad_norm": 3.30680513381958,
      "learning_rate": 4.9725269892922324e-05,
      "loss": 0.4398,
      "step": 2500
    },
    {
      "epoch": 0.0659616103427805,
      "grad_norm": 6.759955406188965,
      "learning_rate": 4.967030188430334e-05,
      "loss": 0.4334,
      "step": 3000
    },
    {
      "epoch": 0.07695521206657725,
      "grad_norm": 6.073638439178467,
      "learning_rate": 4.9615333875684354e-05,
      "loss": 0.4327,
      "step": 3500
    },
    {
      "epoch": 0.087948813790374,
      "grad_norm": 12.468738555908203,
      "learning_rate": 4.956036586706537e-05,
      "loss": 0.3927,
      "step": 4000
    },
    {
      "epoch": 0.09894241551417075,
      "grad_norm": 4.8412065505981445,
      "learning_rate": 4.9505397858446385e-05,
      "loss": 0.406,
      "step": 4500
    },
    {
      "epoch": 0.1099360172379675,
      "grad_norm": 8.260411262512207,
      "learning_rate": 4.94504298498274e-05,
      "loss": 0.413,
      "step": 5000
    },
    {
      "epoch": 0.12092961896176425,
      "grad_norm": 9.500689506530762,
      "learning_rate": 4.9395461841208415e-05,
      "loss": 0.384,
      "step": 5500
    },
    {
      "epoch": 0.131923220685561,
      "grad_norm": 11.622321128845215,
      "learning_rate": 4.934049383258944e-05,
      "loss": 0.3921,
      "step": 6000
    },
    {
      "epoch": 0.14291682240935774,
      "grad_norm": 26.99121856689453,
      "learning_rate": 4.928552582397045e-05,
      "loss": 0.403,
      "step": 6500
    },
    {
      "epoch": 0.1539104241331545,
      "grad_norm": 8.335683822631836,
      "learning_rate": 4.923055781535147e-05,
      "loss": 0.3909,
      "step": 7000
    },
    {
      "epoch": 0.16490402585695124,
      "grad_norm": 5.5474629402160645,
      "learning_rate": 4.917558980673249e-05,
      "loss": 0.3762,
      "step": 7500
    },
    {
      "epoch": 0.175897627580748,
      "grad_norm": 4.500017166137695,
      "learning_rate": 4.9120621798113504e-05,
      "loss": 0.3846,
      "step": 8000
    },
    {
      "epoch": 0.18689122930454474,
      "grad_norm": 3.8221800327301025,
      "learning_rate": 4.906565378949451e-05,
      "loss": 0.383,
      "step": 8500
    },
    {
      "epoch": 0.1978848310283415,
      "grad_norm": 1.268046498298645,
      "learning_rate": 4.9010685780875534e-05,
      "loss": 0.3878,
      "step": 9000
    },
    {
      "epoch": 0.20887843275213824,
      "grad_norm": 6.653203964233398,
      "learning_rate": 4.895571777225655e-05,
      "loss": 0.3775,
      "step": 9500
    },
    {
      "epoch": 0.219872034475935,
      "grad_norm": 4.123987197875977,
      "learning_rate": 4.8900749763637565e-05,
      "loss": 0.3855,
      "step": 10000
    },
    {
      "epoch": 0.23086563619973174,
      "grad_norm": 2.909237861633301,
      "learning_rate": 4.884578175501858e-05,
      "loss": 0.3727,
      "step": 10500
    },
    {
      "epoch": 0.2418592379235285,
      "grad_norm": 6.25817346572876,
      "learning_rate": 4.87908137463996e-05,
      "loss": 0.3849,
      "step": 11000
    },
    {
      "epoch": 0.25285283964732524,
      "grad_norm": 13.480652809143066,
      "learning_rate": 4.873584573778062e-05,
      "loss": 0.3718,
      "step": 11500
    },
    {
      "epoch": 0.263846441371122,
      "grad_norm": 4.780261993408203,
      "learning_rate": 4.8680877729161625e-05,
      "loss": 0.3633,
      "step": 12000
    },
    {
      "epoch": 0.2748400430949188,
      "grad_norm": 12.493049621582031,
      "learning_rate": 4.862590972054265e-05,
      "loss": 0.3675,
      "step": 12500
    },
    {
      "epoch": 0.2858336448187155,
      "grad_norm": 12.899275779724121,
      "learning_rate": 4.857094171192366e-05,
      "loss": 0.3675,
      "step": 13000
    },
    {
      "epoch": 0.29682724654251225,
      "grad_norm": 6.799164295196533,
      "learning_rate": 4.851597370330468e-05,
      "loss": 0.3802,
      "step": 13500
    },
    {
      "epoch": 0.307820848266309,
      "grad_norm": 7.8045806884765625,
      "learning_rate": 4.84610056946857e-05,
      "loss": 0.3636,
      "step": 14000
    },
    {
      "epoch": 0.3188144499901058,
      "grad_norm": 4.4612860679626465,
      "learning_rate": 4.8406037686066714e-05,
      "loss": 0.3741,
      "step": 14500
    },
    {
      "epoch": 0.3298080517139025,
      "grad_norm": 3.1498043537139893,
      "learning_rate": 4.835106967744773e-05,
      "loss": 0.3901,
      "step": 15000
    },
    {
      "epoch": 0.34080165343769925,
      "grad_norm": 4.975426197052002,
      "learning_rate": 4.8296101668828744e-05,
      "loss": 0.3583,
      "step": 15500
    },
    {
      "epoch": 0.351795255161496,
      "grad_norm": 8.131381034851074,
      "learning_rate": 4.824113366020976e-05,
      "loss": 0.3617,
      "step": 16000
    },
    {
      "epoch": 0.3627888568852928,
      "grad_norm": 6.214816570281982,
      "learning_rate": 4.8186165651590775e-05,
      "loss": 0.3717,
      "step": 16500
    },
    {
      "epoch": 0.3737824586090895,
      "grad_norm": 4.601458549499512,
      "learning_rate": 4.813119764297179e-05,
      "loss": 0.3666,
      "step": 17000
    },
    {
      "epoch": 0.38477606033288625,
      "grad_norm": 7.476944446563721,
      "learning_rate": 4.807622963435281e-05,
      "loss": 0.3523,
      "step": 17500
    },
    {
      "epoch": 0.395769662056683,
      "grad_norm": 4.409537315368652,
      "learning_rate": 4.802126162573383e-05,
      "loss": 0.3567,
      "step": 18000
    },
    {
      "epoch": 0.4067632637804798,
      "grad_norm": 2.3782947063446045,
      "learning_rate": 4.796629361711484e-05,
      "loss": 0.362,
      "step": 18500
    },
    {
      "epoch": 0.4177568655042765,
      "grad_norm": 14.420434951782227,
      "learning_rate": 4.791132560849586e-05,
      "loss": 0.3652,
      "step": 19000
    },
    {
      "epoch": 0.42875046722807325,
      "grad_norm": 15.60116195678711,
      "learning_rate": 4.785635759987687e-05,
      "loss": 0.3548,
      "step": 19500
    },
    {
      "epoch": 0.43974406895187,
      "grad_norm": 3.4788053035736084,
      "learning_rate": 4.780138959125789e-05,
      "loss": 0.3449,
      "step": 20000
    },
    {
      "epoch": 0.4507376706756668,
      "grad_norm": 4.821088790893555,
      "learning_rate": 4.774642158263891e-05,
      "loss": 0.3648,
      "step": 20500
    },
    {
      "epoch": 0.4617312723994635,
      "grad_norm": 9.566584587097168,
      "learning_rate": 4.7691453574019924e-05,
      "loss": 0.3596,
      "step": 21000
    },
    {
      "epoch": 0.47272487412326025,
      "grad_norm": 4.819116115570068,
      "learning_rate": 4.763648556540094e-05,
      "loss": 0.3491,
      "step": 21500
    },
    {
      "epoch": 0.483718475847057,
      "grad_norm": 6.5861687660217285,
      "learning_rate": 4.7581517556781954e-05,
      "loss": 0.3642,
      "step": 22000
    },
    {
      "epoch": 0.4947120775708538,
      "grad_norm": 14.750572204589844,
      "learning_rate": 4.7526549548162976e-05,
      "loss": 0.3449,
      "step": 22500
    },
    {
      "epoch": 0.5057056792946505,
      "grad_norm": 6.100365161895752,
      "learning_rate": 4.7471581539543985e-05,
      "loss": 0.3641,
      "step": 23000
    },
    {
      "epoch": 0.5166992810184473,
      "grad_norm": 16.624481201171875,
      "learning_rate": 4.7416613530925e-05,
      "loss": 0.3678,
      "step": 23500
    },
    {
      "epoch": 0.527692882742244,
      "grad_norm": 9.18799114227295,
      "learning_rate": 4.736164552230602e-05,
      "loss": 0.3445,
      "step": 24000
    },
    {
      "epoch": 0.5386864844660407,
      "grad_norm": 7.6170477867126465,
      "learning_rate": 4.730667751368704e-05,
      "loss": 0.3441,
      "step": 24500
    },
    {
      "epoch": 0.5496800861898375,
      "grad_norm": 7.2432122230529785,
      "learning_rate": 4.725170950506805e-05,
      "loss": 0.3533,
      "step": 25000
    },
    {
      "epoch": 0.5606736879136343,
      "grad_norm": 14.112428665161133,
      "learning_rate": 4.7196741496449074e-05,
      "loss": 0.3496,
      "step": 25500
    },
    {
      "epoch": 0.571667289637431,
      "grad_norm": 9.928651809692383,
      "learning_rate": 4.714177348783009e-05,
      "loss": 0.3672,
      "step": 26000
    },
    {
      "epoch": 0.5826608913612278,
      "grad_norm": 1.3449090719223022,
      "learning_rate": 4.70868054792111e-05,
      "loss": 0.3644,
      "step": 26500
    },
    {
      "epoch": 0.5936544930850245,
      "grad_norm": 4.952566146850586,
      "learning_rate": 4.703183747059212e-05,
      "loss": 0.3492,
      "step": 27000
    },
    {
      "epoch": 0.6046480948088213,
      "grad_norm": 7.056711673736572,
      "learning_rate": 4.6976869461973134e-05,
      "loss": 0.338,
      "step": 27500
    },
    {
      "epoch": 0.615641696532618,
      "grad_norm": 3.1022658348083496,
      "learning_rate": 4.692190145335415e-05,
      "loss": 0.3554,
      "step": 28000
    },
    {
      "epoch": 0.6266352982564147,
      "grad_norm": 0.8288804888725281,
      "learning_rate": 4.6866933444735164e-05,
      "loss": 0.3503,
      "step": 28500
    },
    {
      "epoch": 0.6376288999802115,
      "grad_norm": 1.7743088006973267,
      "learning_rate": 4.6811965436116186e-05,
      "loss": 0.3391,
      "step": 29000
    },
    {
      "epoch": 0.6486225017040083,
      "grad_norm": 9.428120613098145,
      "learning_rate": 4.67569974274972e-05,
      "loss": 0.3482,
      "step": 29500
    },
    {
      "epoch": 0.659616103427805,
      "grad_norm": 8.651213645935059,
      "learning_rate": 4.670202941887821e-05,
      "loss": 0.3465,
      "step": 30000
    },
    {
      "epoch": 0.6706097051516018,
      "grad_norm": 4.1347432136535645,
      "learning_rate": 4.664706141025923e-05,
      "loss": 0.3306,
      "step": 30500
    },
    {
      "epoch": 0.6816033068753985,
      "grad_norm": 6.125527381896973,
      "learning_rate": 4.659209340164025e-05,
      "loss": 0.3507,
      "step": 31000
    },
    {
      "epoch": 0.6925969085991953,
      "grad_norm": 4.868486404418945,
      "learning_rate": 4.653712539302126e-05,
      "loss": 0.3482,
      "step": 31500
    },
    {
      "epoch": 0.703590510322992,
      "grad_norm": 8.24593448638916,
      "learning_rate": 4.6482157384402284e-05,
      "loss": 0.3392,
      "step": 32000
    },
    {
      "epoch": 0.7145841120467887,
      "grad_norm": 2.8034331798553467,
      "learning_rate": 4.64271893757833e-05,
      "loss": 0.3356,
      "step": 32500
    },
    {
      "epoch": 0.7255777137705856,
      "grad_norm": 5.757754325866699,
      "learning_rate": 4.6372221367164314e-05,
      "loss": 0.3303,
      "step": 33000
    },
    {
      "epoch": 0.7365713154943823,
      "grad_norm": 8.36219310760498,
      "learning_rate": 4.631725335854533e-05,
      "loss": 0.3382,
      "step": 33500
    },
    {
      "epoch": 0.747564917218179,
      "grad_norm": 5.216170787811279,
      "learning_rate": 4.6262285349926344e-05,
      "loss": 0.3536,
      "step": 34000
    },
    {
      "epoch": 0.7585585189419758,
      "grad_norm": 4.911902904510498,
      "learning_rate": 4.620731734130736e-05,
      "loss": 0.3454,
      "step": 34500
    },
    {
      "epoch": 0.7695521206657725,
      "grad_norm": 7.381763935089111,
      "learning_rate": 4.6152349332688374e-05,
      "loss": 0.3345,
      "step": 35000
    },
    {
      "epoch": 0.7805457223895693,
      "grad_norm": 5.534759998321533,
      "learning_rate": 4.6097381324069396e-05,
      "loss": 0.3376,
      "step": 35500
    },
    {
      "epoch": 0.791539324113366,
      "grad_norm": 10.861287117004395,
      "learning_rate": 4.604241331545041e-05,
      "loss": 0.3468,
      "step": 36000
    },
    {
      "epoch": 0.8025329258371627,
      "grad_norm": 8.038270950317383,
      "learning_rate": 4.5987445306831427e-05,
      "loss": 0.3201,
      "step": 36500
    },
    {
      "epoch": 0.8135265275609596,
      "grad_norm": 5.9085612297058105,
      "learning_rate": 4.593247729821245e-05,
      "loss": 0.3395,
      "step": 37000
    },
    {
      "epoch": 0.8245201292847563,
      "grad_norm": 3.917942762374878,
      "learning_rate": 4.587750928959346e-05,
      "loss": 0.3405,
      "step": 37500
    },
    {
      "epoch": 0.835513731008553,
      "grad_norm": 11.806559562683105,
      "learning_rate": 4.582254128097447e-05,
      "loss": 0.3436,
      "step": 38000
    },
    {
      "epoch": 0.8465073327323498,
      "grad_norm": 5.344921588897705,
      "learning_rate": 4.5767573272355494e-05,
      "loss": 0.3456,
      "step": 38500
    },
    {
      "epoch": 0.8575009344561465,
      "grad_norm": 2.7331180572509766,
      "learning_rate": 4.571260526373651e-05,
      "loss": 0.3318,
      "step": 39000
    },
    {
      "epoch": 0.8684945361799433,
      "grad_norm": 5.556344985961914,
      "learning_rate": 4.5657637255117524e-05,
      "loss": 0.3381,
      "step": 39500
    },
    {
      "epoch": 0.87948813790374,
      "grad_norm": 3.0379350185394287,
      "learning_rate": 4.560266924649854e-05,
      "loss": 0.3578,
      "step": 40000
    },
    {
      "epoch": 0.8904817396275367,
      "grad_norm": 10.484158515930176,
      "learning_rate": 4.554770123787956e-05,
      "loss": 0.3309,
      "step": 40500
    },
    {
      "epoch": 0.9014753413513336,
      "grad_norm": 4.727932453155518,
      "learning_rate": 4.549273322926057e-05,
      "loss": 0.3391,
      "step": 41000
    },
    {
      "epoch": 0.9124689430751303,
      "grad_norm": 1.4484784603118896,
      "learning_rate": 4.5437765220641584e-05,
      "loss": 0.3312,
      "step": 41500
    },
    {
      "epoch": 0.923462544798927,
      "grad_norm": 3.9162023067474365,
      "learning_rate": 4.5382797212022606e-05,
      "loss": 0.3428,
      "step": 42000
    },
    {
      "epoch": 0.9344561465227238,
      "grad_norm": 13.400925636291504,
      "learning_rate": 4.532782920340362e-05,
      "loss": 0.3469,
      "step": 42500
    },
    {
      "epoch": 0.9454497482465205,
      "grad_norm": 7.244716644287109,
      "learning_rate": 4.5272861194784637e-05,
      "loss": 0.3358,
      "step": 43000
    },
    {
      "epoch": 0.9564433499703173,
      "grad_norm": 50.04055404663086,
      "learning_rate": 4.521789318616566e-05,
      "loss": 0.3324,
      "step": 43500
    },
    {
      "epoch": 0.967436951694114,
      "grad_norm": 1.5928322076797485,
      "learning_rate": 4.5162925177546674e-05,
      "loss": 0.3342,
      "step": 44000
    },
    {
      "epoch": 0.9784305534179107,
      "grad_norm": 2.406778335571289,
      "learning_rate": 4.510795716892768e-05,
      "loss": 0.3222,
      "step": 44500
    },
    {
      "epoch": 0.9894241551417076,
      "grad_norm": 7.802639961242676,
      "learning_rate": 4.5052989160308704e-05,
      "loss": 0.3415,
      "step": 45000
    },
    {
      "epoch": 1.0004177568655044,
      "grad_norm": 6.250036716461182,
      "learning_rate": 4.499802115168972e-05,
      "loss": 0.3323,
      "step": 45500
    },
    {
      "epoch": 1.011411358589301,
      "grad_norm": 5.126342296600342,
      "learning_rate": 4.4943053143070734e-05,
      "loss": 0.3142,
      "step": 46000
    },
    {
      "epoch": 1.0224049603130978,
      "grad_norm": 5.8610920906066895,
      "learning_rate": 4.488808513445175e-05,
      "loss": 0.3245,
      "step": 46500
    },
    {
      "epoch": 1.0333985620368946,
      "grad_norm": 11.111745834350586,
      "learning_rate": 4.483311712583277e-05,
      "loss": 0.3399,
      "step": 47000
    },
    {
      "epoch": 1.0443921637606912,
      "grad_norm": 2.3276267051696777,
      "learning_rate": 4.4778149117213786e-05,
      "loss": 0.3094,
      "step": 47500
    },
    {
      "epoch": 1.055385765484488,
      "grad_norm": 10.724480628967285,
      "learning_rate": 4.47231811085948e-05,
      "loss": 0.3553,
      "step": 48000
    },
    {
      "epoch": 1.0663793672082849,
      "grad_norm": 6.816959381103516,
      "learning_rate": 4.4668213099975816e-05,
      "loss": 0.3229,
      "step": 48500
    },
    {
      "epoch": 1.0773729689320815,
      "grad_norm": 9.322823524475098,
      "learning_rate": 4.461324509135683e-05,
      "loss": 0.3281,
      "step": 49000
    },
    {
      "epoch": 1.0883665706558783,
      "grad_norm": 4.714510917663574,
      "learning_rate": 4.4558277082737847e-05,
      "loss": 0.3194,
      "step": 49500
    },
    {
      "epoch": 1.099360172379675,
      "grad_norm": 7.4790873527526855,
      "learning_rate": 4.450330907411887e-05,
      "loss": 0.327,
      "step": 50000
    },
    {
      "epoch": 1.1103537741034717,
      "grad_norm": 13.549468040466309,
      "learning_rate": 4.4448341065499884e-05,
      "loss": 0.3338,
      "step": 50500
    },
    {
      "epoch": 1.1213473758272685,
      "grad_norm": 11.970882415771484,
      "learning_rate": 4.43933730568809e-05,
      "loss": 0.315,
      "step": 51000
    },
    {
      "epoch": 1.1323409775510653,
      "grad_norm": 10.85252571105957,
      "learning_rate": 4.4338405048261914e-05,
      "loss": 0.3299,
      "step": 51500
    },
    {
      "epoch": 1.1433345792748622,
      "grad_norm": 10.784136772155762,
      "learning_rate": 4.428343703964293e-05,
      "loss": 0.3254,
      "step": 52000
    },
    {
      "epoch": 1.1543281809986587,
      "grad_norm": 1.622444748878479,
      "learning_rate": 4.4228469031023944e-05,
      "loss": 0.339,
      "step": 52500
    },
    {
      "epoch": 1.1653217827224556,
      "grad_norm": 43.40085983276367,
      "learning_rate": 4.417350102240496e-05,
      "loss": 0.3359,
      "step": 53000
    },
    {
      "epoch": 1.1763153844462524,
      "grad_norm": 2.5507776737213135,
      "learning_rate": 4.411853301378598e-05,
      "loss": 0.3243,
      "step": 53500
    },
    {
      "epoch": 1.187308986170049,
      "grad_norm": 7.1784796714782715,
      "learning_rate": 4.4063565005166996e-05,
      "loss": 0.3116,
      "step": 54000
    },
    {
      "epoch": 1.1983025878938458,
      "grad_norm": 6.748432636260986,
      "learning_rate": 4.400859699654801e-05,
      "loss": 0.3304,
      "step": 54500
    },
    {
      "epoch": 1.2092961896176426,
      "grad_norm": 6.946142673492432,
      "learning_rate": 4.3953628987929026e-05,
      "loss": 0.3276,
      "step": 55000
    },
    {
      "epoch": 1.2202897913414392,
      "grad_norm": 1.9610453844070435,
      "learning_rate": 4.389866097931004e-05,
      "loss": 0.3086,
      "step": 55500
    },
    {
      "epoch": 1.231283393065236,
      "grad_norm": 14.188645362854004,
      "learning_rate": 4.384369297069106e-05,
      "loss": 0.3252,
      "step": 56000
    },
    {
      "epoch": 1.2422769947890329,
      "grad_norm": 5.430747032165527,
      "learning_rate": 4.378872496207208e-05,
      "loss": 0.3249,
      "step": 56500
    },
    {
      "epoch": 1.2532705965128295,
      "grad_norm": 9.545607566833496,
      "learning_rate": 4.3733756953453094e-05,
      "loss": 0.3212,
      "step": 57000
    },
    {
      "epoch": 1.2642641982366263,
      "grad_norm": 9.018916130065918,
      "learning_rate": 4.367878894483411e-05,
      "loss": 0.3168,
      "step": 57500
    },
    {
      "epoch": 1.275257799960423,
      "grad_norm": 1.2648240327835083,
      "learning_rate": 4.3623820936215124e-05,
      "loss": 0.3099,
      "step": 58000
    },
    {
      "epoch": 1.2862514016842197,
      "grad_norm": 6.8696513175964355,
      "learning_rate": 4.3568852927596146e-05,
      "loss": 0.3137,
      "step": 58500
    },
    {
      "epoch": 1.2972450034080165,
      "grad_norm": 6.86074686050415,
      "learning_rate": 4.3513884918977154e-05,
      "loss": 0.311,
      "step": 59000
    },
    {
      "epoch": 1.3082386051318133,
      "grad_norm": 6.923558235168457,
      "learning_rate": 4.345891691035817e-05,
      "loss": 0.3279,
      "step": 59500
    },
    {
      "epoch": 1.31923220685561,
      "grad_norm": 6.823428630828857,
      "learning_rate": 4.340394890173919e-05,
      "loss": 0.3164,
      "step": 60000
    },
    {
      "epoch": 1.3302258085794068,
      "grad_norm": 15.35879898071289,
      "learning_rate": 4.3348980893120206e-05,
      "loss": 0.317,
      "step": 60500
    },
    {
      "epoch": 1.3412194103032036,
      "grad_norm": 10.110989570617676,
      "learning_rate": 4.329401288450122e-05,
      "loss": 0.3064,
      "step": 61000
    },
    {
      "epoch": 1.3522130120270002,
      "grad_norm": 11.222654342651367,
      "learning_rate": 4.3239044875882236e-05,
      "loss": 0.3284,
      "step": 61500
    },
    {
      "epoch": 1.363206613750797,
      "grad_norm": 6.541056156158447,
      "learning_rate": 4.318407686726326e-05,
      "loss": 0.3181,
      "step": 62000
    },
    {
      "epoch": 1.3742002154745938,
      "grad_norm": 10.668859481811523,
      "learning_rate": 4.3129108858644273e-05,
      "loss": 0.3353,
      "step": 62500
    },
    {
      "epoch": 1.3851938171983904,
      "grad_norm": 4.842268943786621,
      "learning_rate": 4.307414085002529e-05,
      "loss": 0.3207,
      "step": 63000
    },
    {
      "epoch": 1.3961874189221872,
      "grad_norm": 11.421856880187988,
      "learning_rate": 4.3019172841406304e-05,
      "loss": 0.3222,
      "step": 63500
    },
    {
      "epoch": 1.407181020645984,
      "grad_norm": 8.708451271057129,
      "learning_rate": 4.296420483278732e-05,
      "loss": 0.317,
      "step": 64000
    },
    {
      "epoch": 1.4181746223697809,
      "grad_norm": 11.524147033691406,
      "learning_rate": 4.2909236824168334e-05,
      "loss": 0.311,
      "step": 64500
    },
    {
      "epoch": 1.4291682240935775,
      "grad_norm": 8.493680000305176,
      "learning_rate": 4.2854268815549356e-05,
      "loss": 0.314,
      "step": 65000
    },
    {
      "epoch": 1.4401618258173743,
      "grad_norm": 8.822970390319824,
      "learning_rate": 4.279930080693037e-05,
      "loss": 0.3156,
      "step": 65500
    },
    {
      "epoch": 1.451155427541171,
      "grad_norm": 4.579957485198975,
      "learning_rate": 4.2744332798311386e-05,
      "loss": 0.3142,
      "step": 66000
    },
    {
      "epoch": 1.462149029264968,
      "grad_norm": 8.91949462890625,
      "learning_rate": 4.26893647896924e-05,
      "loss": 0.3189,
      "step": 66500
    },
    {
      "epoch": 1.4731426309887645,
      "grad_norm": 11.067248344421387,
      "learning_rate": 4.2634396781073416e-05,
      "loss": 0.3186,
      "step": 67000
    },
    {
      "epoch": 1.4841362327125613,
      "grad_norm": 1.258942723274231,
      "learning_rate": 4.257942877245443e-05,
      "loss": 0.3252,
      "step": 67500
    },
    {
      "epoch": 1.4951298344363582,
      "grad_norm": 9.3698091506958,
      "learning_rate": 4.2524460763835446e-05,
      "loss": 0.3249,
      "step": 68000
    },
    {
      "epoch": 1.5061234361601548,
      "grad_norm": 9.80678939819336,
      "learning_rate": 4.246949275521647e-05,
      "loss": 0.3012,
      "step": 68500
    },
    {
      "epoch": 1.5171170378839516,
      "grad_norm": 12.210348129272461,
      "learning_rate": 4.2414524746597483e-05,
      "loss": 0.336,
      "step": 69000
    },
    {
      "epoch": 1.5281106396077484,
      "grad_norm": 10.103690147399902,
      "learning_rate": 4.23595567379785e-05,
      "loss": 0.3104,
      "step": 69500
    },
    {
      "epoch": 1.539104241331545,
      "grad_norm": 12.642767906188965,
      "learning_rate": 4.2304588729359514e-05,
      "loss": 0.3141,
      "step": 70000
    },
    {
      "epoch": 1.5500978430553418,
      "grad_norm": 12.4256591796875,
      "learning_rate": 4.224962072074053e-05,
      "loss": 0.3199,
      "step": 70500
    },
    {
      "epoch": 1.5610914447791386,
      "grad_norm": 2.297490358352661,
      "learning_rate": 4.2194652712121544e-05,
      "loss": 0.3162,
      "step": 71000
    },
    {
      "epoch": 1.5720850465029352,
      "grad_norm": 7.098315715789795,
      "learning_rate": 4.2139684703502566e-05,
      "loss": 0.3368,
      "step": 71500
    },
    {
      "epoch": 1.583078648226732,
      "grad_norm": 9.446782112121582,
      "learning_rate": 4.208471669488358e-05,
      "loss": 0.3222,
      "step": 72000
    },
    {
      "epoch": 1.5940722499505289,
      "grad_norm": 2.159083366394043,
      "learning_rate": 4.2029748686264596e-05,
      "loss": 0.3324,
      "step": 72500
    },
    {
      "epoch": 1.6050658516743255,
      "grad_norm": 8.672493934631348,
      "learning_rate": 4.197478067764561e-05,
      "loss": 0.3275,
      "step": 73000
    },
    {
      "epoch": 1.6160594533981223,
      "grad_norm": 59.90230941772461,
      "learning_rate": 4.1919812669026626e-05,
      "loss": 0.3022,
      "step": 73500
    },
    {
      "epoch": 1.6270530551219191,
      "grad_norm": 3.7299256324768066,
      "learning_rate": 4.186484466040764e-05,
      "loss": 0.302,
      "step": 74000
    },
    {
      "epoch": 1.6380466568457157,
      "grad_norm": 1.9799489974975586,
      "learning_rate": 4.1809876651788657e-05,
      "loss": 0.3326,
      "step": 74500
    },
    {
      "epoch": 1.6490402585695125,
      "grad_norm": 3.5191292762756348,
      "learning_rate": 4.175490864316968e-05,
      "loss": 0.3341,
      "step": 75000
    },
    {
      "epoch": 1.6600338602933093,
      "grad_norm": 8.155940055847168,
      "learning_rate": 4.1699940634550694e-05,
      "loss": 0.3223,
      "step": 75500
    },
    {
      "epoch": 1.671027462017106,
      "grad_norm": 11.969199180603027,
      "learning_rate": 4.164497262593171e-05,
      "loss": 0.3161,
      "step": 76000
    },
    {
      "epoch": 1.6820210637409028,
      "grad_norm": 6.272096157073975,
      "learning_rate": 4.159000461731273e-05,
      "loss": 0.3111,
      "step": 76500
    },
    {
      "epoch": 1.6930146654646996,
      "grad_norm": 8.18444538116455,
      "learning_rate": 4.1535036608693746e-05,
      "loss": 0.2978,
      "step": 77000
    },
    {
      "epoch": 1.7040082671884962,
      "grad_norm": 3.5265398025512695,
      "learning_rate": 4.1480068600074754e-05,
      "loss": 0.3115,
      "step": 77500
    },
    {
      "epoch": 1.715001868912293,
      "grad_norm": 11.159759521484375,
      "learning_rate": 4.1425100591455776e-05,
      "loss": 0.3332,
      "step": 78000
    },
    {
      "epoch": 1.7259954706360898,
      "grad_norm": 3.9695756435394287,
      "learning_rate": 4.137013258283679e-05,
      "loss": 0.3041,
      "step": 78500
    },
    {
      "epoch": 1.7369890723598864,
      "grad_norm": 9.84706974029541,
      "learning_rate": 4.1315164574217806e-05,
      "loss": 0.3217,
      "step": 79000
    },
    {
      "epoch": 1.7479826740836832,
      "grad_norm": 1.5455602407455444,
      "learning_rate": 4.126019656559882e-05,
      "loss": 0.3215,
      "step": 79500
    },
    {
      "epoch": 1.75897627580748,
      "grad_norm": 9.262640953063965,
      "learning_rate": 4.120522855697984e-05,
      "loss": 0.3153,
      "step": 80000
    },
    {
      "epoch": 1.7699698775312767,
      "grad_norm": 14.162032127380371,
      "learning_rate": 4.115026054836086e-05,
      "loss": 0.3038,
      "step": 80500
    },
    {
      "epoch": 1.7809634792550737,
      "grad_norm": 13.123693466186523,
      "learning_rate": 4.1095292539741867e-05,
      "loss": 0.3048,
      "step": 81000
    },
    {
      "epoch": 1.7919570809788703,
      "grad_norm": 1.0989173650741577,
      "learning_rate": 4.104032453112289e-05,
      "loss": 0.3161,
      "step": 81500
    },
    {
      "epoch": 1.802950682702667,
      "grad_norm": 5.385015487670898,
      "learning_rate": 4.0985356522503904e-05,
      "loss": 0.3166,
      "step": 82000
    },
    {
      "epoch": 1.813944284426464,
      "grad_norm": 1.5322954654693604,
      "learning_rate": 4.093038851388492e-05,
      "loss": 0.3074,
      "step": 82500
    },
    {
      "epoch": 1.8249378861502605,
      "grad_norm": 11.671422004699707,
      "learning_rate": 4.087542050526594e-05,
      "loss": 0.3194,
      "step": 83000
    },
    {
      "epoch": 1.8359314878740571,
      "grad_norm": 14.900120735168457,
      "learning_rate": 4.0820452496646956e-05,
      "loss": 0.3223,
      "step": 83500
    },
    {
      "epoch": 1.8469250895978542,
      "grad_norm": 7.150140285491943,
      "learning_rate": 4.076548448802797e-05,
      "loss": 0.3229,
      "step": 84000
    },
    {
      "epoch": 1.8579186913216508,
      "grad_norm": 10.951445579528809,
      "learning_rate": 4.0710516479408986e-05,
      "loss": 0.3125,
      "step": 84500
    },
    {
      "epoch": 1.8689122930454476,
      "grad_norm": 5.4015092849731445,
      "learning_rate": 4.065554847079e-05,
      "loss": 0.3361,
      "step": 85000
    },
    {
      "epoch": 1.8799058947692444,
      "grad_norm": 6.9761128425598145,
      "learning_rate": 4.0600580462171016e-05,
      "loss": 0.3112,
      "step": 85500
    },
    {
      "epoch": 1.890899496493041,
      "grad_norm": 1.6438030004501343,
      "learning_rate": 4.054561245355203e-05,
      "loss": 0.3294,
      "step": 86000
    },
    {
      "epoch": 1.9018930982168378,
      "grad_norm": 12.635666847229004,
      "learning_rate": 4.049064444493305e-05,
      "loss": 0.3033,
      "step": 86500
    },
    {
      "epoch": 1.9128866999406346,
      "grad_norm": 1.8293393850326538,
      "learning_rate": 4.043567643631407e-05,
      "loss": 0.3088,
      "step": 87000
    },
    {
      "epoch": 1.9238803016644312,
      "grad_norm": 9.099433898925781,
      "learning_rate": 4.038070842769508e-05,
      "loss": 0.3126,
      "step": 87500
    },
    {
      "epoch": 1.934873903388228,
      "grad_norm": 7.671930313110352,
      "learning_rate": 4.0325740419076105e-05,
      "loss": 0.329,
      "step": 88000
    },
    {
      "epoch": 1.9458675051120249,
      "grad_norm": 1.3941655158996582,
      "learning_rate": 4.0270772410457114e-05,
      "loss": 0.3199,
      "step": 88500
    },
    {
      "epoch": 1.9568611068358215,
      "grad_norm": 9.637309074401855,
      "learning_rate": 4.021580440183813e-05,
      "loss": 0.3108,
      "step": 89000
    },
    {
      "epoch": 1.9678547085596183,
      "grad_norm": 0.9559910297393799,
      "learning_rate": 4.016083639321915e-05,
      "loss": 0.3042,
      "step": 89500
    },
    {
      "epoch": 1.9788483102834151,
      "grad_norm": 1.0116803646087646,
      "learning_rate": 4.0105868384600166e-05,
      "loss": 0.3387,
      "step": 90000
    },
    {
      "epoch": 1.9898419120072117,
      "grad_norm": 0.8181836009025574,
      "learning_rate": 4.005090037598118e-05,
      "loss": 0.311,
      "step": 90500
    },
    {
      "epoch": 2.0008355137310088,
      "grad_norm": 9.865342140197754,
      "learning_rate": 3.9995932367362196e-05,
      "loss": 0.315,
      "step": 91000
    },
    {
      "epoch": 2.0118291154548054,
      "grad_norm": 10.645466804504395,
      "learning_rate": 3.994096435874322e-05,
      "loss": 0.3039,
      "step": 91500
    },
    {
      "epoch": 2.022822717178602,
      "grad_norm": 2.6725611686706543,
      "learning_rate": 3.9885996350124226e-05,
      "loss": 0.3006,
      "step": 92000
    },
    {
      "epoch": 2.033816318902399,
      "grad_norm": 8.495718955993652,
      "learning_rate": 3.983102834150524e-05,
      "loss": 0.3114,
      "step": 92500
    },
    {
      "epoch": 2.0448099206261956,
      "grad_norm": 4.050534725189209,
      "learning_rate": 3.977606033288626e-05,
      "loss": 0.3101,
      "step": 93000
    },
    {
      "epoch": 2.055803522349992,
      "grad_norm": 9.678757667541504,
      "learning_rate": 3.972109232426728e-05,
      "loss": 0.3034,
      "step": 93500
    },
    {
      "epoch": 2.0667971240737892,
      "grad_norm": 7.1358842849731445,
      "learning_rate": 3.9666124315648293e-05,
      "loss": 0.3031,
      "step": 94000
    },
    {
      "epoch": 2.077790725797586,
      "grad_norm": 1.185701608657837,
      "learning_rate": 3.9611156307029315e-05,
      "loss": 0.3061,
      "step": 94500
    },
    {
      "epoch": 2.0887843275213824,
      "grad_norm": 5.692849636077881,
      "learning_rate": 3.955618829841033e-05,
      "loss": 0.2968,
      "step": 95000
    },
    {
      "epoch": 2.0997779292451795,
      "grad_norm": 2.2216100692749023,
      "learning_rate": 3.950122028979134e-05,
      "loss": 0.3211,
      "step": 95500
    },
    {
      "epoch": 2.110771530968976,
      "grad_norm": 11.643511772155762,
      "learning_rate": 3.944625228117236e-05,
      "loss": 0.3008,
      "step": 96000
    },
    {
      "epoch": 2.1217651326927727,
      "grad_norm": 7.496740818023682,
      "learning_rate": 3.9391284272553376e-05,
      "loss": 0.3194,
      "step": 96500
    },
    {
      "epoch": 2.1327587344165697,
      "grad_norm": 3.3536930084228516,
      "learning_rate": 3.933631626393439e-05,
      "loss": 0.3113,
      "step": 97000
    },
    {
      "epoch": 2.1437523361403663,
      "grad_norm": 8.354691505432129,
      "learning_rate": 3.9281348255315406e-05,
      "loss": 0.3048,
      "step": 97500
    },
    {
      "epoch": 2.154745937864163,
      "grad_norm": 9.404593467712402,
      "learning_rate": 3.922638024669643e-05,
      "loss": 0.3175,
      "step": 98000
    },
    {
      "epoch": 2.16573953958796,
      "grad_norm": 15.68801212310791,
      "learning_rate": 3.917141223807744e-05,
      "loss": 0.3189,
      "step": 98500
    },
    {
      "epoch": 2.1767331413117565,
      "grad_norm": 0.9393897652626038,
      "learning_rate": 3.911644422945845e-05,
      "loss": 0.3011,
      "step": 99000
    },
    {
      "epoch": 2.187726743035553,
      "grad_norm": 6.319961071014404,
      "learning_rate": 3.906147622083947e-05,
      "loss": 0.3161,
      "step": 99500
    },
    {
      "epoch": 2.19872034475935,
      "grad_norm": 0.25479209423065186,
      "learning_rate": 3.900650821222049e-05,
      "loss": 0.3026,
      "step": 100000
    },
    {
      "epoch": 2.209713946483147,
      "grad_norm": 1.4826370477676392,
      "learning_rate": 3.8951540203601503e-05,
      "loss": 0.301,
      "step": 100500
    },
    {
      "epoch": 2.2207075482069434,
      "grad_norm": 12.064138412475586,
      "learning_rate": 3.8896572194982525e-05,
      "loss": 0.3018,
      "step": 101000
    },
    {
      "epoch": 2.2317011499307404,
      "grad_norm": 2.757827043533325,
      "learning_rate": 3.884160418636354e-05,
      "loss": 0.3121,
      "step": 101500
    },
    {
      "epoch": 2.242694751654537,
      "grad_norm": 9.334854125976562,
      "learning_rate": 3.8786636177744556e-05,
      "loss": 0.296,
      "step": 102000
    },
    {
      "epoch": 2.2536883533783336,
      "grad_norm": 11.124839782714844,
      "learning_rate": 3.873166816912557e-05,
      "loss": 0.2808,
      "step": 102500
    },
    {
      "epoch": 2.2646819551021307,
      "grad_norm": 10.933819770812988,
      "learning_rate": 3.8676700160506586e-05,
      "loss": 0.3158,
      "step": 103000
    },
    {
      "epoch": 2.2756755568259273,
      "grad_norm": 10.800768852233887,
      "learning_rate": 3.86217321518876e-05,
      "loss": 0.3178,
      "step": 103500
    },
    {
      "epoch": 2.2866691585497243,
      "grad_norm": 8.5425443649292,
      "learning_rate": 3.8566764143268616e-05,
      "loss": 0.302,
      "step": 104000
    },
    {
      "epoch": 2.297662760273521,
      "grad_norm": 6.126797676086426,
      "learning_rate": 3.851179613464964e-05,
      "loss": 0.3117,
      "step": 104500
    },
    {
      "epoch": 2.3086563619973175,
      "grad_norm": 9.902619361877441,
      "learning_rate": 3.845682812603065e-05,
      "loss": 0.3047,
      "step": 105000
    },
    {
      "epoch": 2.3196499637211145,
      "grad_norm": 6.866889476776123,
      "learning_rate": 3.840186011741167e-05,
      "loss": 0.2941,
      "step": 105500
    },
    {
      "epoch": 2.330643565444911,
      "grad_norm": 12.1033296585083,
      "learning_rate": 3.834689210879269e-05,
      "loss": 0.315,
      "step": 106000
    },
    {
      "epoch": 2.3416371671687077,
      "grad_norm": 9.344437599182129,
      "learning_rate": 3.82919241001737e-05,
      "loss": 0.3058,
      "step": 106500
    },
    {
      "epoch": 2.3526307688925048,
      "grad_norm": 1.799941897392273,
      "learning_rate": 3.8236956091554713e-05,
      "loss": 0.3053,
      "step": 107000
    },
    {
      "epoch": 2.3636243706163014,
      "grad_norm": 2.097012758255005,
      "learning_rate": 3.8181988082935735e-05,
      "loss": 0.2994,
      "step": 107500
    },
    {
      "epoch": 2.374617972340098,
      "grad_norm": 3.5548250675201416,
      "learning_rate": 3.812702007431675e-05,
      "loss": 0.3233,
      "step": 108000
    },
    {
      "epoch": 2.385611574063895,
      "grad_norm": 0.5422198176383972,
      "learning_rate": 3.8072052065697766e-05,
      "loss": 0.2944,
      "step": 108500
    },
    {
      "epoch": 2.3966051757876916,
      "grad_norm": 0.31997764110565186,
      "learning_rate": 3.801708405707878e-05,
      "loss": 0.2807,
      "step": 109000
    },
    {
      "epoch": 2.407598777511488,
      "grad_norm": 5.933233261108398,
      "learning_rate": 3.79621160484598e-05,
      "loss": 0.3087,
      "step": 109500
    },
    {
      "epoch": 2.4185923792352853,
      "grad_norm": 6.728976249694824,
      "learning_rate": 3.790714803984081e-05,
      "loss": 0.2966,
      "step": 110000
    },
    {
      "epoch": 2.429585980959082,
      "grad_norm": 4.2829718589782715,
      "learning_rate": 3.7852180031221826e-05,
      "loss": 0.2822,
      "step": 110500
    },
    {
      "epoch": 2.4405795826828784,
      "grad_norm": 0.5274419784545898,
      "learning_rate": 3.779721202260285e-05,
      "loss": 0.3023,
      "step": 111000
    },
    {
      "epoch": 2.4515731844066755,
      "grad_norm": 9.847600936889648,
      "learning_rate": 3.774224401398386e-05,
      "loss": 0.3292,
      "step": 111500
    },
    {
      "epoch": 2.462566786130472,
      "grad_norm": 7.965893268585205,
      "learning_rate": 3.768727600536488e-05,
      "loss": 0.3052,
      "step": 112000
    },
    {
      "epoch": 2.4735603878542687,
      "grad_norm": 20.956584930419922,
      "learning_rate": 3.76323079967459e-05,
      "loss": 0.2935,
      "step": 112500
    },
    {
      "epoch": 2.4845539895780657,
      "grad_norm": 1.0456161499023438,
      "learning_rate": 3.7577339988126915e-05,
      "loss": 0.2835,
      "step": 113000
    },
    {
      "epoch": 2.4955475913018623,
      "grad_norm": 1.101731300354004,
      "learning_rate": 3.752237197950793e-05,
      "loss": 0.3199,
      "step": 113500
    },
    {
      "epoch": 2.506541193025659,
      "grad_norm": 4.554466247558594,
      "learning_rate": 3.7467403970888945e-05,
      "loss": 0.3039,
      "step": 114000
    },
    {
      "epoch": 2.517534794749456,
      "grad_norm": 1.063336730003357,
      "learning_rate": 3.741243596226996e-05,
      "loss": 0.3099,
      "step": 114500
    },
    {
      "epoch": 2.5285283964732526,
      "grad_norm": 3.494676113128662,
      "learning_rate": 3.7357467953650976e-05,
      "loss": 0.2975,
      "step": 115000
    },
    {
      "epoch": 2.539521998197049,
      "grad_norm": 7.187907695770264,
      "learning_rate": 3.730249994503199e-05,
      "loss": 0.2876,
      "step": 115500
    },
    {
      "epoch": 2.550515599920846,
      "grad_norm": 0.7576963305473328,
      "learning_rate": 3.724753193641301e-05,
      "loss": 0.3021,
      "step": 116000
    },
    {
      "epoch": 2.561509201644643,
      "grad_norm": 1.4913263320922852,
      "learning_rate": 3.719256392779403e-05,
      "loss": 0.305,
      "step": 116500
    },
    {
      "epoch": 2.5725028033684394,
      "grad_norm": 10.317115783691406,
      "learning_rate": 3.713759591917504e-05,
      "loss": 0.3026,
      "step": 117000
    },
    {
      "epoch": 2.5834964050922364,
      "grad_norm": 13.697500228881836,
      "learning_rate": 3.708262791055606e-05,
      "loss": 0.2917,
      "step": 117500
    },
    {
      "epoch": 2.594490006816033,
      "grad_norm": 6.44077730178833,
      "learning_rate": 3.702765990193707e-05,
      "loss": 0.2973,
      "step": 118000
    },
    {
      "epoch": 2.6054836085398296,
      "grad_norm": 7.023762226104736,
      "learning_rate": 3.697269189331809e-05,
      "loss": 0.2943,
      "step": 118500
    },
    {
      "epoch": 2.6164772102636267,
      "grad_norm": 19.223806381225586,
      "learning_rate": 3.691772388469911e-05,
      "loss": 0.3194,
      "step": 119000
    },
    {
      "epoch": 2.6274708119874233,
      "grad_norm": 8.380880355834961,
      "learning_rate": 3.6862755876080125e-05,
      "loss": 0.3023,
      "step": 119500
    },
    {
      "epoch": 2.63846441371122,
      "grad_norm": 11.10332202911377,
      "learning_rate": 3.680778786746114e-05,
      "loss": 0.2949,
      "step": 120000
    },
    {
      "epoch": 2.649458015435017,
      "grad_norm": 2.5547420978546143,
      "learning_rate": 3.6752819858842155e-05,
      "loss": 0.3036,
      "step": 120500
    },
    {
      "epoch": 2.6604516171588135,
      "grad_norm": 10.149148941040039,
      "learning_rate": 3.669785185022317e-05,
      "loss": 0.3035,
      "step": 121000
    },
    {
      "epoch": 2.67144521888261,
      "grad_norm": 1.7927303314208984,
      "learning_rate": 3.6642883841604186e-05,
      "loss": 0.3112,
      "step": 121500
    },
    {
      "epoch": 2.682438820606407,
      "grad_norm": 6.049162864685059,
      "learning_rate": 3.65879158329852e-05,
      "loss": 0.3221,
      "step": 122000
    },
    {
      "epoch": 2.6934324223302037,
      "grad_norm": 4.089443206787109,
      "learning_rate": 3.653294782436622e-05,
      "loss": 0.3009,
      "step": 122500
    },
    {
      "epoch": 2.7044260240540003,
      "grad_norm": 18.39468765258789,
      "learning_rate": 3.647797981574724e-05,
      "loss": 0.2916,
      "step": 123000
    },
    {
      "epoch": 2.7154196257777974,
      "grad_norm": 0.5081496834754944,
      "learning_rate": 3.642301180712825e-05,
      "loss": 0.2789,
      "step": 123500
    },
    {
      "epoch": 2.726413227501594,
      "grad_norm": 13.047924041748047,
      "learning_rate": 3.6368043798509275e-05,
      "loss": 0.3083,
      "step": 124000
    },
    {
      "epoch": 2.7374068292253906,
      "grad_norm": 13.293658256530762,
      "learning_rate": 3.631307578989028e-05,
      "loss": 0.3002,
      "step": 124500
    },
    {
      "epoch": 2.7484004309491876,
      "grad_norm": 13.249524116516113,
      "learning_rate": 3.62581077812713e-05,
      "loss": 0.3092,
      "step": 125000
    },
    {
      "epoch": 2.759394032672984,
      "grad_norm": 19.654735565185547,
      "learning_rate": 3.620313977265232e-05,
      "loss": 0.3058,
      "step": 125500
    },
    {
      "epoch": 2.770387634396781,
      "grad_norm": 14.121988296508789,
      "learning_rate": 3.6148171764033335e-05,
      "loss": 0.3015,
      "step": 126000
    },
    {
      "epoch": 2.781381236120578,
      "grad_norm": 12.813506126403809,
      "learning_rate": 3.609320375541435e-05,
      "loss": 0.3018,
      "step": 126500
    },
    {
      "epoch": 2.7923748378443745,
      "grad_norm": 11.71113395690918,
      "learning_rate": 3.6038235746795365e-05,
      "loss": 0.3043,
      "step": 127000
    },
    {
      "epoch": 2.803368439568171,
      "grad_norm": 12.854676246643066,
      "learning_rate": 3.598326773817639e-05,
      "loss": 0.3059,
      "step": 127500
    },
    {
      "epoch": 2.814362041291968,
      "grad_norm": 0.5513259172439575,
      "learning_rate": 3.59282997295574e-05,
      "loss": 0.2926,
      "step": 128000
    },
    {
      "epoch": 2.8253556430157647,
      "grad_norm": 13.774333953857422,
      "learning_rate": 3.587333172093841e-05,
      "loss": 0.2905,
      "step": 128500
    },
    {
      "epoch": 2.8363492447395617,
      "grad_norm": 8.444316864013672,
      "learning_rate": 3.581836371231943e-05,
      "loss": 0.2969,
      "step": 129000
    },
    {
      "epoch": 2.8473428464633583,
      "grad_norm": 5.209111213684082,
      "learning_rate": 3.576339570370045e-05,
      "loss": 0.3031,
      "step": 129500
    },
    {
      "epoch": 2.858336448187155,
      "grad_norm": 4.918737888336182,
      "learning_rate": 3.570842769508146e-05,
      "loss": 0.3031,
      "step": 130000
    },
    {
      "epoch": 2.869330049910952,
      "grad_norm": 16.64807891845703,
      "learning_rate": 3.5653459686462485e-05,
      "loss": 0.2973,
      "step": 130500
    },
    {
      "epoch": 2.8803236516347486,
      "grad_norm": 12.479476928710938,
      "learning_rate": 3.55984916778435e-05,
      "loss": 0.294,
      "step": 131000
    },
    {
      "epoch": 2.891317253358545,
      "grad_norm": 1.2764140367507935,
      "learning_rate": 3.5543523669224515e-05,
      "loss": 0.2996,
      "step": 131500
    },
    {
      "epoch": 2.902310855082342,
      "grad_norm": 6.306889057159424,
      "learning_rate": 3.548855566060553e-05,
      "loss": 0.2882,
      "step": 132000
    },
    {
      "epoch": 2.913304456806139,
      "grad_norm": 13.864242553710938,
      "learning_rate": 3.5433587651986545e-05,
      "loss": 0.3029,
      "step": 132500
    },
    {
      "epoch": 2.924298058529936,
      "grad_norm": 4.046827793121338,
      "learning_rate": 3.537861964336756e-05,
      "loss": 0.285,
      "step": 133000
    },
    {
      "epoch": 2.9352916602537324,
      "grad_norm": 1.2483056783676147,
      "learning_rate": 3.5323651634748575e-05,
      "loss": 0.2983,
      "step": 133500
    },
    {
      "epoch": 2.946285261977529,
      "grad_norm": 10.385467529296875,
      "learning_rate": 3.52686836261296e-05,
      "loss": 0.2985,
      "step": 134000
    },
    {
      "epoch": 2.957278863701326,
      "grad_norm": 9.135749816894531,
      "learning_rate": 3.521371561751061e-05,
      "loss": 0.2998,
      "step": 134500
    },
    {
      "epoch": 2.9682724654251227,
      "grad_norm": 5.7887187004089355,
      "learning_rate": 3.515874760889163e-05,
      "loss": 0.3006,
      "step": 135000
    },
    {
      "epoch": 2.9792660671489193,
      "grad_norm": 11.468867301940918,
      "learning_rate": 3.510377960027264e-05,
      "loss": 0.2929,
      "step": 135500
    },
    {
      "epoch": 2.9902596688727163,
      "grad_norm": 7.03387451171875,
      "learning_rate": 3.504881159165366e-05,
      "loss": 0.2994,
      "step": 136000
    },
    {
      "epoch": 3.001253270596513,
      "grad_norm": 4.151477336883545,
      "learning_rate": 3.499384358303467e-05,
      "loss": 0.2847,
      "step": 136500
    },
    {
      "epoch": 3.0122468723203095,
      "grad_norm": 0.63908451795578,
      "learning_rate": 3.4938875574415695e-05,
      "loss": 0.296,
      "step": 137000
    },
    {
      "epoch": 3.023240474044106,
      "grad_norm": 6.63067102432251,
      "learning_rate": 3.488390756579671e-05,
      "loss": 0.3051,
      "step": 137500
    },
    {
      "epoch": 3.034234075767903,
      "grad_norm": 9.608457565307617,
      "learning_rate": 3.4828939557177725e-05,
      "loss": 0.2909,
      "step": 138000
    },
    {
      "epoch": 3.0452276774916998,
      "grad_norm": 38.31596374511719,
      "learning_rate": 3.477397154855874e-05,
      "loss": 0.2943,
      "step": 138500
    },
    {
      "epoch": 3.056221279215497,
      "grad_norm": 0.4342547357082367,
      "learning_rate": 3.4719003539939755e-05,
      "loss": 0.3051,
      "step": 139000
    },
    {
      "epoch": 3.0672148809392934,
      "grad_norm": 7.807497978210449,
      "learning_rate": 3.466403553132077e-05,
      "loss": 0.2884,
      "step": 139500
    },
    {
      "epoch": 3.07820848266309,
      "grad_norm": 5.541719913482666,
      "learning_rate": 3.4609067522701786e-05,
      "loss": 0.2843,
      "step": 140000
    },
    {
      "epoch": 3.089202084386887,
      "grad_norm": 0.37382790446281433,
      "learning_rate": 3.455409951408281e-05,
      "loss": 0.2957,
      "step": 140500
    },
    {
      "epoch": 3.1001956861106836,
      "grad_norm": 16.81133270263672,
      "learning_rate": 3.449913150546382e-05,
      "loss": 0.2954,
      "step": 141000
    },
    {
      "epoch": 3.1111892878344802,
      "grad_norm": 14.021695137023926,
      "learning_rate": 3.444416349684484e-05,
      "loss": 0.2818,
      "step": 141500
    },
    {
      "epoch": 3.1221828895582773,
      "grad_norm": 7.998987197875977,
      "learning_rate": 3.438919548822585e-05,
      "loss": 0.2863,
      "step": 142000
    },
    {
      "epoch": 3.133176491282074,
      "grad_norm": 7.910961627960205,
      "learning_rate": 3.4334227479606875e-05,
      "loss": 0.2884,
      "step": 142500
    },
    {
      "epoch": 3.1441700930058705,
      "grad_norm": 9.19328498840332,
      "learning_rate": 3.427925947098788e-05,
      "loss": 0.2968,
      "step": 143000
    },
    {
      "epoch": 3.1551636947296675,
      "grad_norm": 1.1894488334655762,
      "learning_rate": 3.42242914623689e-05,
      "loss": 0.2842,
      "step": 143500
    },
    {
      "epoch": 3.166157296453464,
      "grad_norm": 4.250494003295898,
      "learning_rate": 3.416932345374992e-05,
      "loss": 0.2919,
      "step": 144000
    },
    {
      "epoch": 3.1771508981772607,
      "grad_norm": 1.1123703718185425,
      "learning_rate": 3.4114355445130935e-05,
      "loss": 0.2854,
      "step": 144500
    },
    {
      "epoch": 3.1881444999010577,
      "grad_norm": 1.50236976146698,
      "learning_rate": 3.405938743651195e-05,
      "loss": 0.288,
      "step": 145000
    },
    {
      "epoch": 3.1991381016248543,
      "grad_norm": 0.5388885736465454,
      "learning_rate": 3.400441942789297e-05,
      "loss": 0.2871,
      "step": 145500
    },
    {
      "epoch": 3.210131703348651,
      "grad_norm": 7.373965740203857,
      "learning_rate": 3.394945141927399e-05,
      "loss": 0.2849,
      "step": 146000
    },
    {
      "epoch": 3.221125305072448,
      "grad_norm": 14.169344902038574,
      "learning_rate": 3.3894483410654996e-05,
      "loss": 0.3122,
      "step": 146500
    },
    {
      "epoch": 3.2321189067962446,
      "grad_norm": 7.653840065002441,
      "learning_rate": 3.383951540203602e-05,
      "loss": 0.2933,
      "step": 147000
    },
    {
      "epoch": 3.243112508520041,
      "grad_norm": 19.90491485595703,
      "learning_rate": 3.378454739341703e-05,
      "loss": 0.2696,
      "step": 147500
    },
    {
      "epoch": 3.2541061102438382,
      "grad_norm": 16.71002769470215,
      "learning_rate": 3.372957938479805e-05,
      "loss": 0.2954,
      "step": 148000
    },
    {
      "epoch": 3.265099711967635,
      "grad_norm": 8.204931259155273,
      "learning_rate": 3.367461137617906e-05,
      "loss": 0.2992,
      "step": 148500
    },
    {
      "epoch": 3.2760933136914314,
      "grad_norm": 15.129700660705566,
      "learning_rate": 3.3619643367560085e-05,
      "loss": 0.3091,
      "step": 149000
    },
    {
      "epoch": 3.2870869154152285,
      "grad_norm": 7.756717681884766,
      "learning_rate": 3.35646753589411e-05,
      "loss": 0.2916,
      "step": 149500
    },
    {
      "epoch": 3.298080517139025,
      "grad_norm": 12.551115036010742,
      "learning_rate": 3.350970735032211e-05,
      "loss": 0.2804,
      "step": 150000
    },
    {
      "epoch": 3.3090741188628217,
      "grad_norm": 6.938497543334961,
      "learning_rate": 3.345473934170313e-05,
      "loss": 0.3079,
      "step": 150500
    },
    {
      "epoch": 3.3200677205866187,
      "grad_norm": 0.8755176067352295,
      "learning_rate": 3.3399771333084145e-05,
      "loss": 0.3087,
      "step": 151000
    },
    {
      "epoch": 3.3310613223104153,
      "grad_norm": 13.346745491027832,
      "learning_rate": 3.334480332446516e-05,
      "loss": 0.2968,
      "step": 151500
    },
    {
      "epoch": 3.342054924034212,
      "grad_norm": 15.3798246383667,
      "learning_rate": 3.328983531584618e-05,
      "loss": 0.2889,
      "step": 152000
    },
    {
      "epoch": 3.353048525758009,
      "grad_norm": 11.675865173339844,
      "learning_rate": 3.32348673072272e-05,
      "loss": 0.2931,
      "step": 152500
    },
    {
      "epoch": 3.3640421274818055,
      "grad_norm": 3.3035695552825928,
      "learning_rate": 3.317989929860821e-05,
      "loss": 0.2891,
      "step": 153000
    },
    {
      "epoch": 3.375035729205602,
      "grad_norm": 5.506224632263184,
      "learning_rate": 3.312493128998923e-05,
      "loss": 0.2838,
      "step": 153500
    },
    {
      "epoch": 3.386029330929399,
      "grad_norm": 0.5243566632270813,
      "learning_rate": 3.306996328137024e-05,
      "loss": 0.2924,
      "step": 154000
    },
    {
      "epoch": 3.3970229326531958,
      "grad_norm": 9.33509635925293,
      "learning_rate": 3.301499527275126e-05,
      "loss": 0.2944,
      "step": 154500
    },
    {
      "epoch": 3.4080165343769924,
      "grad_norm": 0.5014046430587769,
      "learning_rate": 3.296002726413227e-05,
      "loss": 0.3037,
      "step": 155000
    },
    {
      "epoch": 3.4190101361007894,
      "grad_norm": 5.47611665725708,
      "learning_rate": 3.2905059255513295e-05,
      "loss": 0.3044,
      "step": 155500
    },
    {
      "epoch": 3.430003737824586,
      "grad_norm": 91.87459564208984,
      "learning_rate": 3.285009124689431e-05,
      "loss": 0.3027,
      "step": 156000
    },
    {
      "epoch": 3.4409973395483826,
      "grad_norm": 16.05875587463379,
      "learning_rate": 3.2795123238275325e-05,
      "loss": 0.3041,
      "step": 156500
    },
    {
      "epoch": 3.4519909412721796,
      "grad_norm": 18.708093643188477,
      "learning_rate": 3.274015522965635e-05,
      "loss": 0.2757,
      "step": 157000
    },
    {
      "epoch": 3.4629845429959762,
      "grad_norm": 7.951850414276123,
      "learning_rate": 3.2685187221037355e-05,
      "loss": 0.3023,
      "step": 157500
    },
    {
      "epoch": 3.473978144719773,
      "grad_norm": 2.5825109481811523,
      "learning_rate": 3.263021921241837e-05,
      "loss": 0.2808,
      "step": 158000
    },
    {
      "epoch": 3.48497174644357,
      "grad_norm": 17.3467960357666,
      "learning_rate": 3.257525120379939e-05,
      "loss": 0.2869,
      "step": 158500
    },
    {
      "epoch": 3.4959653481673665,
      "grad_norm": 17.970930099487305,
      "learning_rate": 3.252028319518041e-05,
      "loss": 0.2849,
      "step": 159000
    },
    {
      "epoch": 3.506958949891163,
      "grad_norm": 16.126754760742188,
      "learning_rate": 3.246531518656142e-05,
      "loss": 0.2792,
      "step": 159500
    },
    {
      "epoch": 3.51795255161496,
      "grad_norm": 1.4622963666915894,
      "learning_rate": 3.241034717794244e-05,
      "loss": 0.276,
      "step": 160000
    },
    {
      "epoch": 3.5289461533387567,
      "grad_norm": 6.16251802444458,
      "learning_rate": 3.235537916932346e-05,
      "loss": 0.2942,
      "step": 160500
    },
    {
      "epoch": 3.5399397550625533,
      "grad_norm": 10.926855087280273,
      "learning_rate": 3.230041116070447e-05,
      "loss": 0.2907,
      "step": 161000
    },
    {
      "epoch": 3.5509333567863504,
      "grad_norm": 10.533696174621582,
      "learning_rate": 3.224544315208548e-05,
      "loss": 0.2799,
      "step": 161500
    },
    {
      "epoch": 3.561926958510147,
      "grad_norm": 14.110986709594727,
      "learning_rate": 3.2190475143466505e-05,
      "loss": 0.2923,
      "step": 162000
    },
    {
      "epoch": 3.572920560233944,
      "grad_norm": 4.057143211364746,
      "learning_rate": 3.213550713484752e-05,
      "loss": 0.2876,
      "step": 162500
    },
    {
      "epoch": 3.5839141619577406,
      "grad_norm": 11.691658973693848,
      "learning_rate": 3.2080539126228535e-05,
      "loss": 0.2848,
      "step": 163000
    },
    {
      "epoch": 3.594907763681537,
      "grad_norm": 2.7102091312408447,
      "learning_rate": 3.202557111760956e-05,
      "loss": 0.2902,
      "step": 163500
    },
    {
      "epoch": 3.6059013654053342,
      "grad_norm": 6.770330429077148,
      "learning_rate": 3.197060310899057e-05,
      "loss": 0.2916,
      "step": 164000
    },
    {
      "epoch": 3.616894967129131,
      "grad_norm": 1.212012529373169,
      "learning_rate": 3.191563510037158e-05,
      "loss": 0.3035,
      "step": 164500
    },
    {
      "epoch": 3.6278885688529274,
      "grad_norm": 12.022546768188477,
      "learning_rate": 3.18606670917526e-05,
      "loss": 0.2961,
      "step": 165000
    },
    {
      "epoch": 3.6388821705767245,
      "grad_norm": 11.471626281738281,
      "learning_rate": 3.180569908313362e-05,
      "loss": 0.2737,
      "step": 165500
    },
    {
      "epoch": 3.649875772300521,
      "grad_norm": 1.8199399709701538,
      "learning_rate": 3.175073107451463e-05,
      "loss": 0.2803,
      "step": 166000
    },
    {
      "epoch": 3.660869374024318,
      "grad_norm": 0.49397334456443787,
      "learning_rate": 3.169576306589565e-05,
      "loss": 0.2841,
      "step": 166500
    },
    {
      "epoch": 3.6718629757481147,
      "grad_norm": 6.156454563140869,
      "learning_rate": 3.164079505727667e-05,
      "loss": 0.2995,
      "step": 167000
    },
    {
      "epoch": 3.6828565774719113,
      "grad_norm": 6.8086957931518555,
      "learning_rate": 3.1585827048657685e-05,
      "loss": 0.2767,
      "step": 167500
    },
    {
      "epoch": 3.6938501791957083,
      "grad_norm": 0.3053564727306366,
      "learning_rate": 3.15308590400387e-05,
      "loss": 0.2823,
      "step": 168000
    },
    {
      "epoch": 3.704843780919505,
      "grad_norm": 12.513322830200195,
      "learning_rate": 3.1475891031419715e-05,
      "loss": 0.3101,
      "step": 168500
    },
    {
      "epoch": 3.7158373826433015,
      "grad_norm": 13.03089427947998,
      "learning_rate": 3.142092302280073e-05,
      "loss": 0.2913,
      "step": 169000
    },
    {
      "epoch": 3.7268309843670986,
      "grad_norm": 14.645920753479004,
      "learning_rate": 3.1365955014181745e-05,
      "loss": 0.2982,
      "step": 169500
    },
    {
      "epoch": 3.737824586090895,
      "grad_norm": 0.6779377460479736,
      "learning_rate": 3.131098700556277e-05,
      "loss": 0.3021,
      "step": 170000
    },
    {
      "epoch": 3.748818187814692,
      "grad_norm": 0.32249969244003296,
      "learning_rate": 3.125601899694378e-05,
      "loss": 0.2771,
      "step": 170500
    },
    {
      "epoch": 3.759811789538489,
      "grad_norm": 6.418672561645508,
      "learning_rate": 3.12010509883248e-05,
      "loss": 0.2968,
      "step": 171000
    },
    {
      "epoch": 3.7708053912622854,
      "grad_norm": 7.455796241760254,
      "learning_rate": 3.114608297970581e-05,
      "loss": 0.2986,
      "step": 171500
    },
    {
      "epoch": 3.781798992986082,
      "grad_norm": 6.8785552978515625,
      "learning_rate": 3.109111497108683e-05,
      "loss": 0.3015,
      "step": 172000
    },
    {
      "epoch": 3.792792594709879,
      "grad_norm": 2.9195897579193115,
      "learning_rate": 3.103614696246784e-05,
      "loss": 0.3059,
      "step": 172500
    },
    {
      "epoch": 3.8037861964336757,
      "grad_norm": 0.8255017399787903,
      "learning_rate": 3.098117895384886e-05,
      "loss": 0.2812,
      "step": 173000
    },
    {
      "epoch": 3.8147797981574723,
      "grad_norm": 9.398905754089355,
      "learning_rate": 3.092621094522988e-05,
      "loss": 0.2902,
      "step": 173500
    },
    {
      "epoch": 3.8257733998812693,
      "grad_norm": 5.028994083404541,
      "learning_rate": 3.0871242936610895e-05,
      "loss": 0.2939,
      "step": 174000
    },
    {
      "epoch": 3.836767001605066,
      "grad_norm": 6.00448751449585,
      "learning_rate": 3.081627492799191e-05,
      "loss": 0.2953,
      "step": 174500
    },
    {
      "epoch": 3.8477606033288625,
      "grad_norm": 11.38614559173584,
      "learning_rate": 3.076130691937293e-05,
      "loss": 0.2892,
      "step": 175000
    },
    {
      "epoch": 3.8587542050526595,
      "grad_norm": 8.052933692932129,
      "learning_rate": 3.070633891075394e-05,
      "loss": 0.2685,
      "step": 175500
    },
    {
      "epoch": 3.869747806776456,
      "grad_norm": 19.311359405517578,
      "learning_rate": 3.0651370902134955e-05,
      "loss": 0.2877,
      "step": 176000
    },
    {
      "epoch": 3.8807414085002527,
      "grad_norm": 13.189095497131348,
      "learning_rate": 3.059640289351598e-05,
      "loss": 0.2867,
      "step": 176500
    },
    {
      "epoch": 3.8917350102240498,
      "grad_norm": 5.865490436553955,
      "learning_rate": 3.054143488489699e-05,
      "loss": 0.2923,
      "step": 177000
    },
    {
      "epoch": 3.9027286119478464,
      "grad_norm": 2.7876639366149902,
      "learning_rate": 3.0486466876278007e-05,
      "loss": 0.2859,
      "step": 177500
    },
    {
      "epoch": 3.913722213671643,
      "grad_norm": 2.5812528133392334,
      "learning_rate": 3.0431498867659026e-05,
      "loss": 0.2996,
      "step": 178000
    },
    {
      "epoch": 3.92471581539544,
      "grad_norm": 22.17088508605957,
      "learning_rate": 3.037653085904004e-05,
      "loss": 0.2993,
      "step": 178500
    },
    {
      "epoch": 3.9357094171192366,
      "grad_norm": 11.265103340148926,
      "learning_rate": 3.032156285042106e-05,
      "loss": 0.2882,
      "step": 179000
    },
    {
      "epoch": 3.946703018843033,
      "grad_norm": 1.9530328512191772,
      "learning_rate": 3.026659484180207e-05,
      "loss": 0.2925,
      "step": 179500
    },
    {
      "epoch": 3.9576966205668302,
      "grad_norm": 14.542168617248535,
      "learning_rate": 3.0211626833183086e-05,
      "loss": 0.2845,
      "step": 180000
    },
    {
      "epoch": 3.968690222290627,
      "grad_norm": 10.891446113586426,
      "learning_rate": 3.0156658824564105e-05,
      "loss": 0.2884,
      "step": 180500
    },
    {
      "epoch": 3.9796838240144234,
      "grad_norm": 9.870865821838379,
      "learning_rate": 3.0101690815945123e-05,
      "loss": 0.2798,
      "step": 181000
    },
    {
      "epoch": 3.9906774257382205,
      "grad_norm": 15.107434272766113,
      "learning_rate": 3.0046722807326138e-05,
      "loss": 0.2934,
      "step": 181500
    },
    {
      "epoch": 4.0016710274620175,
      "grad_norm": 2.980036973953247,
      "learning_rate": 2.9991754798707157e-05,
      "loss": 0.2799,
      "step": 182000
    },
    {
      "epoch": 4.012664629185814,
      "grad_norm": 9.302570343017578,
      "learning_rate": 2.9936786790088172e-05,
      "loss": 0.2768,
      "step": 182500
    },
    {
      "epoch": 4.023658230909611,
      "grad_norm": 1.0592466592788696,
      "learning_rate": 2.9881818781469184e-05,
      "loss": 0.3002,
      "step": 183000
    },
    {
      "epoch": 4.034651832633408,
      "grad_norm": 2.493684768676758,
      "learning_rate": 2.9826850772850202e-05,
      "loss": 0.2848,
      "step": 183500
    },
    {
      "epoch": 4.045645434357204,
      "grad_norm": 3.0801236629486084,
      "learning_rate": 2.9771882764231217e-05,
      "loss": 0.2748,
      "step": 184000
    },
    {
      "epoch": 4.056639036081001,
      "grad_norm": 6.035112380981445,
      "learning_rate": 2.9716914755612236e-05,
      "loss": 0.2707,
      "step": 184500
    },
    {
      "epoch": 4.067632637804798,
      "grad_norm": 9.477520942687988,
      "learning_rate": 2.966194674699325e-05,
      "loss": 0.2849,
      "step": 185000
    },
    {
      "epoch": 4.078626239528594,
      "grad_norm": 0.4312649369239807,
      "learning_rate": 2.960697873837427e-05,
      "loss": 0.2715,
      "step": 185500
    },
    {
      "epoch": 4.089619841252391,
      "grad_norm": 0.9321169257164001,
      "learning_rate": 2.9552010729755288e-05,
      "loss": 0.2862,
      "step": 186000
    },
    {
      "epoch": 4.100613442976188,
      "grad_norm": 0.2938496768474579,
      "learning_rate": 2.9497042721136296e-05,
      "loss": 0.2849,
      "step": 186500
    },
    {
      "epoch": 4.111607044699984,
      "grad_norm": 10.14545726776123,
      "learning_rate": 2.9442074712517315e-05,
      "loss": 0.2884,
      "step": 187000
    },
    {
      "epoch": 4.122600646423781,
      "grad_norm": 10.802544593811035,
      "learning_rate": 2.9387106703898333e-05,
      "loss": 0.2767,
      "step": 187500
    },
    {
      "epoch": 4.1335942481475785,
      "grad_norm": 9.25711441040039,
      "learning_rate": 2.9332138695279348e-05,
      "loss": 0.2643,
      "step": 188000
    },
    {
      "epoch": 4.144587849871375,
      "grad_norm": 0.5568578243255615,
      "learning_rate": 2.9277170686660367e-05,
      "loss": 0.2799,
      "step": 188500
    },
    {
      "epoch": 4.155581451595172,
      "grad_norm": 14.10058879852295,
      "learning_rate": 2.9222202678041382e-05,
      "loss": 0.2776,
      "step": 189000
    },
    {
      "epoch": 4.166575053318969,
      "grad_norm": 0.27841830253601074,
      "learning_rate": 2.91672346694224e-05,
      "loss": 0.2802,
      "step": 189500
    },
    {
      "epoch": 4.177568655042765,
      "grad_norm": 9.741232872009277,
      "learning_rate": 2.9112266660803412e-05,
      "loss": 0.2679,
      "step": 190000
    },
    {
      "epoch": 4.188562256766562,
      "grad_norm": 0.33416494727134705,
      "learning_rate": 2.9057298652184427e-05,
      "loss": 0.2782,
      "step": 190500
    },
    {
      "epoch": 4.199555858490359,
      "grad_norm": 20.807531356811523,
      "learning_rate": 2.9002330643565446e-05,
      "loss": 0.2702,
      "step": 191000
    },
    {
      "epoch": 4.210549460214155,
      "grad_norm": 4.990043640136719,
      "learning_rate": 2.894736263494646e-05,
      "loss": 0.2985,
      "step": 191500
    },
    {
      "epoch": 4.221543061937952,
      "grad_norm": 12.40206527709961,
      "learning_rate": 2.889239462632748e-05,
      "loss": 0.2875,
      "step": 192000
    },
    {
      "epoch": 4.232536663661749,
      "grad_norm": 7.094698429107666,
      "learning_rate": 2.8837426617708498e-05,
      "loss": 0.2646,
      "step": 192500
    },
    {
      "epoch": 4.243530265385545,
      "grad_norm": 12.614456176757812,
      "learning_rate": 2.8782458609089513e-05,
      "loss": 0.2946,
      "step": 193000
    },
    {
      "epoch": 4.254523867109342,
      "grad_norm": 12.409491539001465,
      "learning_rate": 2.872749060047053e-05,
      "loss": 0.2803,
      "step": 193500
    },
    {
      "epoch": 4.265517468833139,
      "grad_norm": 21.68822479248047,
      "learning_rate": 2.8672522591851543e-05,
      "loss": 0.2891,
      "step": 194000
    },
    {
      "epoch": 4.276511070556936,
      "grad_norm": 11.497876167297363,
      "learning_rate": 2.861755458323256e-05,
      "loss": 0.2924,
      "step": 194500
    },
    {
      "epoch": 4.287504672280733,
      "grad_norm": 8.792983055114746,
      "learning_rate": 2.8562586574613577e-05,
      "loss": 0.2715,
      "step": 195000
    },
    {
      "epoch": 4.29849827400453,
      "grad_norm": 9.291259765625,
      "learning_rate": 2.8507618565994592e-05,
      "loss": 0.2802,
      "step": 195500
    },
    {
      "epoch": 4.309491875728326,
      "grad_norm": 14.404077529907227,
      "learning_rate": 2.845265055737561e-05,
      "loss": 0.2952,
      "step": 196000
    },
    {
      "epoch": 4.320485477452123,
      "grad_norm": 14.871371269226074,
      "learning_rate": 2.8397682548756626e-05,
      "loss": 0.2672,
      "step": 196500
    },
    {
      "epoch": 4.33147907917592,
      "grad_norm": 7.801172256469727,
      "learning_rate": 2.8342714540137644e-05,
      "loss": 0.2729,
      "step": 197000
    },
    {
      "epoch": 4.342472680899716,
      "grad_norm": 6.270750522613525,
      "learning_rate": 2.8287746531518656e-05,
      "loss": 0.2918,
      "step": 197500
    },
    {
      "epoch": 4.353466282623513,
      "grad_norm": 0.4025278389453888,
      "learning_rate": 2.823277852289967e-05,
      "loss": 0.292,
      "step": 198000
    },
    {
      "epoch": 4.36445988434731,
      "grad_norm": 0.3161731958389282,
      "learning_rate": 2.817781051428069e-05,
      "loss": 0.2671,
      "step": 198500
    },
    {
      "epoch": 4.375453486071106,
      "grad_norm": 6.233709335327148,
      "learning_rate": 2.8122842505661705e-05,
      "loss": 0.2951,
      "step": 199000
    },
    {
      "epoch": 4.386447087794903,
      "grad_norm": 0.673202633857727,
      "learning_rate": 2.8067874497042723e-05,
      "loss": 0.2738,
      "step": 199500
    },
    {
      "epoch": 4.3974406895187,
      "grad_norm": 13.60539722442627,
      "learning_rate": 2.801290648842374e-05,
      "loss": 0.2662,
      "step": 200000
    },
    {
      "epoch": 4.4084342912424965,
      "grad_norm": 1.9216920137405396,
      "learning_rate": 2.7957938479804757e-05,
      "loss": 0.2787,
      "step": 200500
    },
    {
      "epoch": 4.419427892966294,
      "grad_norm": 15.355144500732422,
      "learning_rate": 2.790297047118577e-05,
      "loss": 0.2795,
      "step": 201000
    },
    {
      "epoch": 4.430421494690091,
      "grad_norm": 7.8315606117248535,
      "learning_rate": 2.7848002462566787e-05,
      "loss": 0.2784,
      "step": 201500
    },
    {
      "epoch": 4.441415096413887,
      "grad_norm": 7.8024725914001465,
      "learning_rate": 2.7793034453947802e-05,
      "loss": 0.2632,
      "step": 202000
    },
    {
      "epoch": 4.452408698137684,
      "grad_norm": 19.893861770629883,
      "learning_rate": 2.773806644532882e-05,
      "loss": 0.2781,
      "step": 202500
    },
    {
      "epoch": 4.463402299861481,
      "grad_norm": 7.059620380401611,
      "learning_rate": 2.7683098436709836e-05,
      "loss": 0.2849,
      "step": 203000
    },
    {
      "epoch": 4.474395901585277,
      "grad_norm": 1.010055422782898,
      "learning_rate": 2.7628130428090854e-05,
      "loss": 0.2765,
      "step": 203500
    },
    {
      "epoch": 4.485389503309074,
      "grad_norm": 18.69727325439453,
      "learning_rate": 2.757316241947187e-05,
      "loss": 0.2842,
      "step": 204000
    },
    {
      "epoch": 4.496383105032871,
      "grad_norm": 6.222092628479004,
      "learning_rate": 2.751819441085288e-05,
      "loss": 0.274,
      "step": 204500
    },
    {
      "epoch": 4.507376706756667,
      "grad_norm": 8.013936996459961,
      "learning_rate": 2.74632264022339e-05,
      "loss": 0.275,
      "step": 205000
    },
    {
      "epoch": 4.518370308480464,
      "grad_norm": 10.256958961486816,
      "learning_rate": 2.7408258393614915e-05,
      "loss": 0.2851,
      "step": 205500
    },
    {
      "epoch": 4.529363910204261,
      "grad_norm": 8.52966594696045,
      "learning_rate": 2.7353290384995933e-05,
      "loss": 0.2705,
      "step": 206000
    },
    {
      "epoch": 4.5403575119280575,
      "grad_norm": 1.0093050003051758,
      "learning_rate": 2.729832237637695e-05,
      "loss": 0.2835,
      "step": 206500
    },
    {
      "epoch": 4.5513511136518545,
      "grad_norm": 12.679258346557617,
      "learning_rate": 2.7243354367757967e-05,
      "loss": 0.2957,
      "step": 207000
    },
    {
      "epoch": 4.562344715375652,
      "grad_norm": 21.51849365234375,
      "learning_rate": 2.7188386359138985e-05,
      "loss": 0.2755,
      "step": 207500
    },
    {
      "epoch": 4.573338317099449,
      "grad_norm": 4.683279037475586,
      "learning_rate": 2.713341835052e-05,
      "loss": 0.2871,
      "step": 208000
    },
    {
      "epoch": 4.584331918823245,
      "grad_norm": 17.53233528137207,
      "learning_rate": 2.7078450341901012e-05,
      "loss": 0.2964,
      "step": 208500
    },
    {
      "epoch": 4.595325520547042,
      "grad_norm": 1.7800623178482056,
      "learning_rate": 2.702348233328203e-05,
      "loss": 0.2718,
      "step": 209000
    },
    {
      "epoch": 4.606319122270838,
      "grad_norm": 17.32254981994629,
      "learning_rate": 2.6968514324663046e-05,
      "loss": 0.2919,
      "step": 209500
    },
    {
      "epoch": 4.617312723994635,
      "grad_norm": 0.6261805295944214,
      "learning_rate": 2.6913546316044064e-05,
      "loss": 0.2917,
      "step": 210000
    },
    {
      "epoch": 4.628306325718432,
      "grad_norm": 9.729216575622559,
      "learning_rate": 2.685857830742508e-05,
      "loss": 0.2802,
      "step": 210500
    },
    {
      "epoch": 4.639299927442229,
      "grad_norm": 0.7912858724594116,
      "learning_rate": 2.6803610298806098e-05,
      "loss": 0.269,
      "step": 211000
    },
    {
      "epoch": 4.650293529166025,
      "grad_norm": 10.349013328552246,
      "learning_rate": 2.6748642290187116e-05,
      "loss": 0.2985,
      "step": 211500
    },
    {
      "epoch": 4.661287130889822,
      "grad_norm": 1.4406890869140625,
      "learning_rate": 2.6693674281568125e-05,
      "loss": 0.2824,
      "step": 212000
    },
    {
      "epoch": 4.672280732613618,
      "grad_norm": 0.1872672736644745,
      "learning_rate": 2.6638706272949143e-05,
      "loss": 0.2859,
      "step": 212500
    },
    {
      "epoch": 4.6832743343374155,
      "grad_norm": 11.05215835571289,
      "learning_rate": 2.658373826433016e-05,
      "loss": 0.2713,
      "step": 213000
    },
    {
      "epoch": 4.6942679360612125,
      "grad_norm": 25.16408920288086,
      "learning_rate": 2.6528770255711177e-05,
      "loss": 0.2759,
      "step": 213500
    },
    {
      "epoch": 4.7052615377850096,
      "grad_norm": 13.880818367004395,
      "learning_rate": 2.6473802247092195e-05,
      "loss": 0.2865,
      "step": 214000
    },
    {
      "epoch": 4.716255139508806,
      "grad_norm": 0.1247299388051033,
      "learning_rate": 2.641883423847321e-05,
      "loss": 0.2912,
      "step": 214500
    },
    {
      "epoch": 4.727248741232603,
      "grad_norm": 13.998242378234863,
      "learning_rate": 2.636386622985423e-05,
      "loss": 0.2756,
      "step": 215000
    },
    {
      "epoch": 4.738242342956399,
      "grad_norm": 11.941643714904785,
      "learning_rate": 2.630889822123524e-05,
      "loss": 0.2874,
      "step": 215500
    },
    {
      "epoch": 4.749235944680196,
      "grad_norm": 1.483677625656128,
      "learning_rate": 2.6253930212616256e-05,
      "loss": 0.2899,
      "step": 216000
    },
    {
      "epoch": 4.760229546403993,
      "grad_norm": 1.431413173675537,
      "learning_rate": 2.6198962203997274e-05,
      "loss": 0.2882,
      "step": 216500
    },
    {
      "epoch": 4.77122314812779,
      "grad_norm": 17.06234359741211,
      "learning_rate": 2.614399419537829e-05,
      "loss": 0.2742,
      "step": 217000
    },
    {
      "epoch": 4.782216749851586,
      "grad_norm": 0.4278561472892761,
      "learning_rate": 2.6089026186759308e-05,
      "loss": 0.2957,
      "step": 217500
    },
    {
      "epoch": 4.793210351575383,
      "grad_norm": 11.261299133300781,
      "learning_rate": 2.6034058178140326e-05,
      "loss": 0.2801,
      "step": 218000
    },
    {
      "epoch": 4.80420395329918,
      "grad_norm": 5.906264305114746,
      "learning_rate": 2.597909016952134e-05,
      "loss": 0.295,
      "step": 218500
    },
    {
      "epoch": 4.815197555022976,
      "grad_norm": 26.210229873657227,
      "learning_rate": 2.592412216090236e-05,
      "loss": 0.2897,
      "step": 219000
    },
    {
      "epoch": 4.8261911567467735,
      "grad_norm": 6.645903587341309,
      "learning_rate": 2.586915415228337e-05,
      "loss": 0.2971,
      "step": 219500
    },
    {
      "epoch": 4.8371847584705705,
      "grad_norm": 9.25091552734375,
      "learning_rate": 2.5814186143664387e-05,
      "loss": 0.2874,
      "step": 220000
    },
    {
      "epoch": 4.848178360194367,
      "grad_norm": 2.984302520751953,
      "learning_rate": 2.5759218135045405e-05,
      "loss": 0.2698,
      "step": 220500
    },
    {
      "epoch": 4.859171961918164,
      "grad_norm": 9.211226463317871,
      "learning_rate": 2.570425012642642e-05,
      "loss": 0.2824,
      "step": 221000
    },
    {
      "epoch": 4.870165563641961,
      "grad_norm": 16.32169532775879,
      "learning_rate": 2.564928211780744e-05,
      "loss": 0.285,
      "step": 221500
    },
    {
      "epoch": 4.881159165365757,
      "grad_norm": 10.376031875610352,
      "learning_rate": 2.5594314109188454e-05,
      "loss": 0.2814,
      "step": 222000
    },
    {
      "epoch": 4.892152767089554,
      "grad_norm": 19.765552520751953,
      "learning_rate": 2.5539346100569472e-05,
      "loss": 0.3005,
      "step": 222500
    },
    {
      "epoch": 4.903146368813351,
      "grad_norm": 0.45602408051490784,
      "learning_rate": 2.5484378091950484e-05,
      "loss": 0.2821,
      "step": 223000
    },
    {
      "epoch": 4.914139970537147,
      "grad_norm": 1.346720576286316,
      "learning_rate": 2.54294100833315e-05,
      "loss": 0.2765,
      "step": 223500
    },
    {
      "epoch": 4.925133572260944,
      "grad_norm": 0.5892652869224548,
      "learning_rate": 2.5374442074712518e-05,
      "loss": 0.2759,
      "step": 224000
    },
    {
      "epoch": 4.936127173984741,
      "grad_norm": 16.852113723754883,
      "learning_rate": 2.5319474066093536e-05,
      "loss": 0.2752,
      "step": 224500
    },
    {
      "epoch": 4.947120775708537,
      "grad_norm": 0.3536875247955322,
      "learning_rate": 2.526450605747455e-05,
      "loss": 0.2682,
      "step": 225000
    },
    {
      "epoch": 4.958114377432334,
      "grad_norm": 12.363527297973633,
      "learning_rate": 2.520953804885557e-05,
      "loss": 0.3007,
      "step": 225500
    },
    {
      "epoch": 4.9691079791561314,
      "grad_norm": 1.1149711608886719,
      "learning_rate": 2.5154570040236585e-05,
      "loss": 0.2818,
      "step": 226000
    },
    {
      "epoch": 4.980101580879928,
      "grad_norm": 4.127918720245361,
      "learning_rate": 2.5099602031617597e-05,
      "loss": 0.2851,
      "step": 226500
    },
    {
      "epoch": 4.991095182603725,
      "grad_norm": 7.331664562225342,
      "learning_rate": 2.5044634022998615e-05,
      "loss": 0.2788,
      "step": 227000
    },
    {
      "epoch": 5.002088784327522,
      "grad_norm": 11.66906452178955,
      "learning_rate": 2.498966601437963e-05,
      "loss": 0.2696,
      "step": 227500
    },
    {
      "epoch": 5.013082386051318,
      "grad_norm": 6.035951137542725,
      "learning_rate": 2.493469800576065e-05,
      "loss": 0.2838,
      "step": 228000
    },
    {
      "epoch": 5.024075987775115,
      "grad_norm": 11.381061553955078,
      "learning_rate": 2.4879729997141664e-05,
      "loss": 0.2757,
      "step": 228500
    },
    {
      "epoch": 5.035069589498912,
      "grad_norm": 1.3383537530899048,
      "learning_rate": 2.4824761988522683e-05,
      "loss": 0.2655,
      "step": 229000
    },
    {
      "epoch": 5.046063191222708,
      "grad_norm": 8.908550262451172,
      "learning_rate": 2.4769793979903698e-05,
      "loss": 0.273,
      "step": 229500
    },
    {
      "epoch": 5.057056792946505,
      "grad_norm": 0.34972140192985535,
      "learning_rate": 2.4714825971284713e-05,
      "loss": 0.2891,
      "step": 230000
    },
    {
      "epoch": 5.068050394670302,
      "grad_norm": 5.585142135620117,
      "learning_rate": 2.465985796266573e-05,
      "loss": 0.301,
      "step": 230500
    },
    {
      "epoch": 5.079043996394098,
      "grad_norm": 3.1081197261810303,
      "learning_rate": 2.4604889954046746e-05,
      "loss": 0.2752,
      "step": 231000
    },
    {
      "epoch": 5.090037598117895,
      "grad_norm": 16.281269073486328,
      "learning_rate": 2.454992194542776e-05,
      "loss": 0.2778,
      "step": 231500
    },
    {
      "epoch": 5.101031199841692,
      "grad_norm": 9.074775695800781,
      "learning_rate": 2.449495393680878e-05,
      "loss": 0.263,
      "step": 232000
    },
    {
      "epoch": 5.1120248015654886,
      "grad_norm": 12.635969161987305,
      "learning_rate": 2.4439985928189795e-05,
      "loss": 0.2844,
      "step": 232500
    },
    {
      "epoch": 5.123018403289286,
      "grad_norm": 11.491802215576172,
      "learning_rate": 2.438501791957081e-05,
      "loss": 0.2761,
      "step": 233000
    },
    {
      "epoch": 5.134012005013083,
      "grad_norm": 0.32805952429771423,
      "learning_rate": 2.433004991095183e-05,
      "loss": 0.2741,
      "step": 233500
    },
    {
      "epoch": 5.145005606736879,
      "grad_norm": 0.5452677011489868,
      "learning_rate": 2.4275081902332844e-05,
      "loss": 0.2753,
      "step": 234000
    },
    {
      "epoch": 5.155999208460676,
      "grad_norm": 18.636051177978516,
      "learning_rate": 2.4220113893713862e-05,
      "loss": 0.2722,
      "step": 234500
    },
    {
      "epoch": 5.166992810184473,
      "grad_norm": 0.3345981240272522,
      "learning_rate": 2.4165145885094874e-05,
      "loss": 0.2756,
      "step": 235000
    },
    {
      "epoch": 5.177986411908269,
      "grad_norm": 13.528042793273926,
      "learning_rate": 2.4110177876475893e-05,
      "loss": 0.2715,
      "step": 235500
    },
    {
      "epoch": 5.188980013632066,
      "grad_norm": 2.0086822509765625,
      "learning_rate": 2.4055209867856908e-05,
      "loss": 0.2951,
      "step": 236000
    },
    {
      "epoch": 5.199973615355863,
      "grad_norm": 14.498665809631348,
      "learning_rate": 2.4000241859237923e-05,
      "loss": 0.2911,
      "step": 236500
    },
    {
      "epoch": 5.210967217079659,
      "grad_norm": 7.604376316070557,
      "learning_rate": 2.394527385061894e-05,
      "loss": 0.2911,
      "step": 237000
    },
    {
      "epoch": 5.221960818803456,
      "grad_norm": 8.649808883666992,
      "learning_rate": 2.3890305841999956e-05,
      "loss": 0.2531,
      "step": 237500
    },
    {
      "epoch": 5.232954420527253,
      "grad_norm": 2.0484209060668945,
      "learning_rate": 2.3835337833380975e-05,
      "loss": 0.2838,
      "step": 238000
    },
    {
      "epoch": 5.2439480222510495,
      "grad_norm": 13.586066246032715,
      "learning_rate": 2.378036982476199e-05,
      "loss": 0.2657,
      "step": 238500
    },
    {
      "epoch": 5.2549416239748465,
      "grad_norm": 4.074124336242676,
      "learning_rate": 2.3725401816143005e-05,
      "loss": 0.269,
      "step": 239000
    },
    {
      "epoch": 5.265935225698644,
      "grad_norm": 46.605316162109375,
      "learning_rate": 2.3670433807524024e-05,
      "loss": 0.2749,
      "step": 239500
    },
    {
      "epoch": 5.27692882742244,
      "grad_norm": 0.8337525129318237,
      "learning_rate": 2.361546579890504e-05,
      "loss": 0.2502,
      "step": 240000
    },
    {
      "epoch": 5.287922429146237,
      "grad_norm": 3.1130142211914062,
      "learning_rate": 2.3560497790286054e-05,
      "loss": 0.2977,
      "step": 240500
    },
    {
      "epoch": 5.298916030870034,
      "grad_norm": 0.3753397464752197,
      "learning_rate": 2.3505529781667072e-05,
      "loss": 0.2754,
      "step": 241000
    },
    {
      "epoch": 5.30990963259383,
      "grad_norm": 9.35236930847168,
      "learning_rate": 2.3450561773048087e-05,
      "loss": 0.2813,
      "step": 241500
    },
    {
      "epoch": 5.320903234317627,
      "grad_norm": 12.886363983154297,
      "learning_rate": 2.3395593764429103e-05,
      "loss": 0.2841,
      "step": 242000
    },
    {
      "epoch": 5.331896836041424,
      "grad_norm": 0.5303388833999634,
      "learning_rate": 2.3340625755810118e-05,
      "loss": 0.2826,
      "step": 242500
    },
    {
      "epoch": 5.34289043776522,
      "grad_norm": 0.20306125283241272,
      "learning_rate": 2.3285657747191136e-05,
      "loss": 0.2685,
      "step": 243000
    },
    {
      "epoch": 5.353884039489017,
      "grad_norm": 20.73016357421875,
      "learning_rate": 2.3230689738572155e-05,
      "loss": 0.2841,
      "step": 243500
    },
    {
      "epoch": 5.364877641212814,
      "grad_norm": 0.4133450388908386,
      "learning_rate": 2.3175721729953166e-05,
      "loss": 0.2677,
      "step": 244000
    },
    {
      "epoch": 5.375871242936611,
      "grad_norm": 13.086383819580078,
      "learning_rate": 2.3120753721334185e-05,
      "loss": 0.2834,
      "step": 244500
    },
    {
      "epoch": 5.3868648446604075,
      "grad_norm": 12.282903671264648,
      "learning_rate": 2.30657857127152e-05,
      "loss": 0.2808,
      "step": 245000
    },
    {
      "epoch": 5.3978584463842045,
      "grad_norm": 0.5758650302886963,
      "learning_rate": 2.3010817704096215e-05,
      "loss": 0.2685,
      "step": 245500
    },
    {
      "epoch": 5.408852048108001,
      "grad_norm": 0.913515031337738,
      "learning_rate": 2.2955849695477234e-05,
      "loss": 0.2733,
      "step": 246000
    },
    {
      "epoch": 5.419845649831798,
      "grad_norm": 5.602423191070557,
      "learning_rate": 2.290088168685825e-05,
      "loss": 0.2606,
      "step": 246500
    },
    {
      "epoch": 5.430839251555595,
      "grad_norm": 12.185246467590332,
      "learning_rate": 2.2845913678239267e-05,
      "loss": 0.293,
      "step": 247000
    },
    {
      "epoch": 5.441832853279392,
      "grad_norm": 9.332497596740723,
      "learning_rate": 2.2790945669620282e-05,
      "loss": 0.2606,
      "step": 247500
    },
    {
      "epoch": 5.452826455003188,
      "grad_norm": 15.638381004333496,
      "learning_rate": 2.2735977661001297e-05,
      "loss": 0.3024,
      "step": 248000
    },
    {
      "epoch": 5.463820056726985,
      "grad_norm": 7.892204284667969,
      "learning_rate": 2.2681009652382316e-05,
      "loss": 0.2793,
      "step": 248500
    },
    {
      "epoch": 5.474813658450782,
      "grad_norm": 0.5271374583244324,
      "learning_rate": 2.262604164376333e-05,
      "loss": 0.2595,
      "step": 249000
    },
    {
      "epoch": 5.485807260174578,
      "grad_norm": 6.844726085662842,
      "learning_rate": 2.2571073635144346e-05,
      "loss": 0.2724,
      "step": 249500
    },
    {
      "epoch": 5.496800861898375,
      "grad_norm": 2.488741159439087,
      "learning_rate": 2.2516105626525365e-05,
      "loss": 0.2876,
      "step": 250000
    },
    {
      "epoch": 5.507794463622172,
      "grad_norm": 4.955134868621826,
      "learning_rate": 2.246113761790638e-05,
      "loss": 0.2701,
      "step": 250500
    },
    {
      "epoch": 5.518788065345968,
      "grad_norm": 19.265901565551758,
      "learning_rate": 2.2406169609287395e-05,
      "loss": 0.2651,
      "step": 251000
    },
    {
      "epoch": 5.5297816670697655,
      "grad_norm": 6.597935676574707,
      "learning_rate": 2.235120160066841e-05,
      "loss": 0.2849,
      "step": 251500
    },
    {
      "epoch": 5.5407752687935625,
      "grad_norm": 12.157054901123047,
      "learning_rate": 2.229623359204943e-05,
      "loss": 0.2838,
      "step": 252000
    },
    {
      "epoch": 5.551768870517359,
      "grad_norm": 14.59920597076416,
      "learning_rate": 2.2241265583430447e-05,
      "loss": 0.2671,
      "step": 252500
    },
    {
      "epoch": 5.562762472241156,
      "grad_norm": 0.7690017223358154,
      "learning_rate": 2.218629757481146e-05,
      "loss": 0.2595,
      "step": 253000
    },
    {
      "epoch": 5.573756073964953,
      "grad_norm": 28.348628997802734,
      "learning_rate": 2.2131329566192477e-05,
      "loss": 0.2675,
      "step": 253500
    },
    {
      "epoch": 5.584749675688749,
      "grad_norm": 0.8166775703430176,
      "learning_rate": 2.2076361557573492e-05,
      "loss": 0.2928,
      "step": 254000
    },
    {
      "epoch": 5.595743277412546,
      "grad_norm": 27.495237350463867,
      "learning_rate": 2.202139354895451e-05,
      "loss": 0.2771,
      "step": 254500
    },
    {
      "epoch": 5.606736879136343,
      "grad_norm": 5.010690689086914,
      "learning_rate": 2.1966425540335526e-05,
      "loss": 0.2796,
      "step": 255000
    },
    {
      "epoch": 5.617730480860139,
      "grad_norm": 8.571292877197266,
      "learning_rate": 2.191145753171654e-05,
      "loss": 0.2857,
      "step": 255500
    },
    {
      "epoch": 5.628724082583936,
      "grad_norm": 19.500751495361328,
      "learning_rate": 2.185648952309756e-05,
      "loss": 0.2698,
      "step": 256000
    },
    {
      "epoch": 5.639717684307733,
      "grad_norm": 9.32770824432373,
      "learning_rate": 2.1801521514478575e-05,
      "loss": 0.2857,
      "step": 256500
    },
    {
      "epoch": 5.650711286031529,
      "grad_norm": 0.4631577432155609,
      "learning_rate": 2.174655350585959e-05,
      "loss": 0.2863,
      "step": 257000
    },
    {
      "epoch": 5.661704887755326,
      "grad_norm": 12.864089965820312,
      "learning_rate": 2.169158549724061e-05,
      "loss": 0.2749,
      "step": 257500
    },
    {
      "epoch": 5.6726984894791235,
      "grad_norm": 0.25792744755744934,
      "learning_rate": 2.1636617488621623e-05,
      "loss": 0.2967,
      "step": 258000
    },
    {
      "epoch": 5.68369209120292,
      "grad_norm": 16.9388484954834,
      "learning_rate": 2.158164948000264e-05,
      "loss": 0.2827,
      "step": 258500
    },
    {
      "epoch": 5.694685692926717,
      "grad_norm": 7.523621082305908,
      "learning_rate": 2.1526681471383657e-05,
      "loss": 0.2794,
      "step": 259000
    },
    {
      "epoch": 5.705679294650514,
      "grad_norm": 16.111387252807617,
      "learning_rate": 2.1471713462764672e-05,
      "loss": 0.265,
      "step": 259500
    },
    {
      "epoch": 5.71667289637431,
      "grad_norm": 14.801931381225586,
      "learning_rate": 2.1416745454145687e-05,
      "loss": 0.2614,
      "step": 260000
    },
    {
      "epoch": 5.727666498098107,
      "grad_norm": 7.158009052276611,
      "learning_rate": 2.1361777445526702e-05,
      "loss": 0.2592,
      "step": 260500
    },
    {
      "epoch": 5.738660099821904,
      "grad_norm": 7.957882404327393,
      "learning_rate": 2.130680943690772e-05,
      "loss": 0.273,
      "step": 261000
    },
    {
      "epoch": 5.7496537015457,
      "grad_norm": 8.201266288757324,
      "learning_rate": 2.125184142828874e-05,
      "loss": 0.2906,
      "step": 261500
    },
    {
      "epoch": 5.760647303269497,
      "grad_norm": 28.154571533203125,
      "learning_rate": 2.119687341966975e-05,
      "loss": 0.2576,
      "step": 262000
    },
    {
      "epoch": 5.771640904993294,
      "grad_norm": 1.7552863359451294,
      "learning_rate": 2.114190541105077e-05,
      "loss": 0.287,
      "step": 262500
    },
    {
      "epoch": 5.78263450671709,
      "grad_norm": 0.49746382236480713,
      "learning_rate": 2.1086937402431785e-05,
      "loss": 0.2833,
      "step": 263000
    },
    {
      "epoch": 5.793628108440887,
      "grad_norm": 0.6685019135475159,
      "learning_rate": 2.1031969393812803e-05,
      "loss": 0.2759,
      "step": 263500
    },
    {
      "epoch": 5.804621710164684,
      "grad_norm": 10.25273609161377,
      "learning_rate": 2.097700138519382e-05,
      "loss": 0.2765,
      "step": 264000
    },
    {
      "epoch": 5.815615311888481,
      "grad_norm": 14.122756004333496,
      "learning_rate": 2.0922033376574834e-05,
      "loss": 0.2957,
      "step": 264500
    },
    {
      "epoch": 5.826608913612278,
      "grad_norm": 19.042037963867188,
      "learning_rate": 2.0867065367955852e-05,
      "loss": 0.2647,
      "step": 265000
    },
    {
      "epoch": 5.837602515336075,
      "grad_norm": 0.19651558995246887,
      "learning_rate": 2.0812097359336867e-05,
      "loss": 0.278,
      "step": 265500
    },
    {
      "epoch": 5.848596117059871,
      "grad_norm": 7.7542901039123535,
      "learning_rate": 2.0757129350717882e-05,
      "loss": 0.2699,
      "step": 266000
    },
    {
      "epoch": 5.859589718783668,
      "grad_norm": 14.042734146118164,
      "learning_rate": 2.07021613420989e-05,
      "loss": 0.2902,
      "step": 266500
    },
    {
      "epoch": 5.870583320507465,
      "grad_norm": 0.35630714893341064,
      "learning_rate": 2.0647193333479916e-05,
      "loss": 0.2757,
      "step": 267000
    },
    {
      "epoch": 5.881576922231261,
      "grad_norm": 19.886438369750977,
      "learning_rate": 2.059222532486093e-05,
      "loss": 0.2637,
      "step": 267500
    },
    {
      "epoch": 5.892570523955058,
      "grad_norm": 0.49246740341186523,
      "learning_rate": 2.053725731624195e-05,
      "loss": 0.291,
      "step": 268000
    },
    {
      "epoch": 5.903564125678855,
      "grad_norm": 0.7969474196434021,
      "learning_rate": 2.0482289307622965e-05,
      "loss": 0.2803,
      "step": 268500
    },
    {
      "epoch": 5.914557727402651,
      "grad_norm": 9.402531623840332,
      "learning_rate": 2.0427321299003983e-05,
      "loss": 0.2668,
      "step": 269000
    },
    {
      "epoch": 5.925551329126448,
      "grad_norm": 0.5412302017211914,
      "learning_rate": 2.0372353290384995e-05,
      "loss": 0.2924,
      "step": 269500
    },
    {
      "epoch": 5.936544930850245,
      "grad_norm": 22.80775260925293,
      "learning_rate": 2.0317385281766013e-05,
      "loss": 0.2689,
      "step": 270000
    },
    {
      "epoch": 5.9475385325740415,
      "grad_norm": 13.241375923156738,
      "learning_rate": 2.0262417273147032e-05,
      "loss": 0.2782,
      "step": 270500
    },
    {
      "epoch": 5.958532134297839,
      "grad_norm": 17.45516014099121,
      "learning_rate": 2.0207449264528044e-05,
      "loss": 0.2831,
      "step": 271000
    },
    {
      "epoch": 5.969525736021636,
      "grad_norm": 12.434664726257324,
      "learning_rate": 2.0152481255909062e-05,
      "loss": 0.2795,
      "step": 271500
    },
    {
      "epoch": 5.980519337745433,
      "grad_norm": 3.3043744564056396,
      "learning_rate": 2.0097513247290077e-05,
      "loss": 0.271,
      "step": 272000
    },
    {
      "epoch": 5.991512939469229,
      "grad_norm": 16.330381393432617,
      "learning_rate": 2.0042545238671096e-05,
      "loss": 0.2905,
      "step": 272500
    },
    {
      "epoch": 6.002506541193026,
      "grad_norm": 7.6294426918029785,
      "learning_rate": 1.998757723005211e-05,
      "loss": 0.2625,
      "step": 273000
    },
    {
      "epoch": 6.013500142916822,
      "grad_norm": 0.896478533744812,
      "learning_rate": 1.9932609221433126e-05,
      "loss": 0.2819,
      "step": 273500
    },
    {
      "epoch": 6.024493744640619,
      "grad_norm": 17.469039916992188,
      "learning_rate": 1.9877641212814144e-05,
      "loss": 0.2648,
      "step": 274000
    },
    {
      "epoch": 6.035487346364416,
      "grad_norm": 3.831825017929077,
      "learning_rate": 1.982267320419516e-05,
      "loss": 0.2589,
      "step": 274500
    },
    {
      "epoch": 6.046480948088212,
      "grad_norm": 23.13195037841797,
      "learning_rate": 1.9767705195576175e-05,
      "loss": 0.2672,
      "step": 275000
    },
    {
      "epoch": 6.057474549812009,
      "grad_norm": 7.1002912521362305,
      "learning_rate": 1.9712737186957193e-05,
      "loss": 0.2547,
      "step": 275500
    },
    {
      "epoch": 6.068468151535806,
      "grad_norm": 2.0739102363586426,
      "learning_rate": 1.9657769178338208e-05,
      "loss": 0.2671,
      "step": 276000
    },
    {
      "epoch": 6.0794617532596025,
      "grad_norm": 820.1348266601562,
      "learning_rate": 1.9602801169719223e-05,
      "loss": 0.2525,
      "step": 276500
    },
    {
      "epoch": 6.0904553549833995,
      "grad_norm": 0.13700884580612183,
      "learning_rate": 1.9547833161100242e-05,
      "loss": 0.2842,
      "step": 277000
    },
    {
      "epoch": 6.101448956707197,
      "grad_norm": 10.450754165649414,
      "learning_rate": 1.9492865152481257e-05,
      "loss": 0.2871,
      "step": 277500
    },
    {
      "epoch": 6.112442558430994,
      "grad_norm": 1.4027057886123657,
      "learning_rate": 1.9437897143862275e-05,
      "loss": 0.2667,
      "step": 278000
    },
    {
      "epoch": 6.12343616015479,
      "grad_norm": 17.552833557128906,
      "learning_rate": 1.9382929135243287e-05,
      "loss": 0.2498,
      "step": 278500
    },
    {
      "epoch": 6.134429761878587,
      "grad_norm": 3.6855967044830322,
      "learning_rate": 1.9327961126624306e-05,
      "loss": 0.2877,
      "step": 279000
    },
    {
      "epoch": 6.145423363602384,
      "grad_norm": 1.7342815399169922,
      "learning_rate": 1.927299311800532e-05,
      "loss": 0.2687,
      "step": 279500
    },
    {
      "epoch": 6.15641696532618,
      "grad_norm": 12.510220527648926,
      "learning_rate": 1.9218025109386336e-05,
      "loss": 0.2594,
      "step": 280000
    },
    {
      "epoch": 6.167410567049977,
      "grad_norm": 32.71046829223633,
      "learning_rate": 1.9163057100767354e-05,
      "loss": 0.2573,
      "step": 280500
    },
    {
      "epoch": 6.178404168773774,
      "grad_norm": 13.37217903137207,
      "learning_rate": 1.910808909214837e-05,
      "loss": 0.2494,
      "step": 281000
    },
    {
      "epoch": 6.18939777049757,
      "grad_norm": 0.21316416561603546,
      "learning_rate": 1.9053121083529388e-05,
      "loss": 0.267,
      "step": 281500
    },
    {
      "epoch": 6.200391372221367,
      "grad_norm": 1.720728874206543,
      "learning_rate": 1.8998153074910403e-05,
      "loss": 0.2783,
      "step": 282000
    },
    {
      "epoch": 6.211384973945164,
      "grad_norm": 10.208795547485352,
      "learning_rate": 1.8943185066291418e-05,
      "loss": 0.2892,
      "step": 282500
    },
    {
      "epoch": 6.2223785756689605,
      "grad_norm": 7.02426815032959,
      "learning_rate": 1.8888217057672437e-05,
      "loss": 0.2564,
      "step": 283000
    },
    {
      "epoch": 6.2333721773927575,
      "grad_norm": 13.644546508789062,
      "learning_rate": 1.8833249049053452e-05,
      "loss": 0.2736,
      "step": 283500
    },
    {
      "epoch": 6.2443657791165545,
      "grad_norm": 7.088092803955078,
      "learning_rate": 1.8778281040434467e-05,
      "loss": 0.2404,
      "step": 284000
    },
    {
      "epoch": 6.255359380840351,
      "grad_norm": 0.44788858294487,
      "learning_rate": 1.8723313031815486e-05,
      "loss": 0.2774,
      "step": 284500
    },
    {
      "epoch": 6.266352982564148,
      "grad_norm": 0.12719903886318207,
      "learning_rate": 1.86683450231965e-05,
      "loss": 0.2516,
      "step": 285000
    },
    {
      "epoch": 6.277346584287945,
      "grad_norm": 11.777623176574707,
      "learning_rate": 1.8613377014577516e-05,
      "loss": 0.2524,
      "step": 285500
    },
    {
      "epoch": 6.288340186011741,
      "grad_norm": 10.677752494812012,
      "learning_rate": 1.855840900595853e-05,
      "loss": 0.2695,
      "step": 286000
    },
    {
      "epoch": 6.299333787735538,
      "grad_norm": 12.083273887634277,
      "learning_rate": 1.850344099733955e-05,
      "loss": 0.2863,
      "step": 286500
    },
    {
      "epoch": 6.310327389459335,
      "grad_norm": 13.183741569519043,
      "learning_rate": 1.8448472988720568e-05,
      "loss": 0.2567,
      "step": 287000
    },
    {
      "epoch": 6.321320991183131,
      "grad_norm": 25.343124389648438,
      "learning_rate": 1.839350498010158e-05,
      "loss": 0.2629,
      "step": 287500
    },
    {
      "epoch": 6.332314592906928,
      "grad_norm": 4.699884414672852,
      "learning_rate": 1.8338536971482598e-05,
      "loss": 0.2641,
      "step": 288000
    },
    {
      "epoch": 6.343308194630725,
      "grad_norm": 14.194457054138184,
      "learning_rate": 1.8283568962863613e-05,
      "loss": 0.2776,
      "step": 288500
    },
    {
      "epoch": 6.354301796354521,
      "grad_norm": 1.4368342161178589,
      "learning_rate": 1.8228600954244632e-05,
      "loss": 0.2634,
      "step": 289000
    },
    {
      "epoch": 6.3652953980783185,
      "grad_norm": 11.70663833618164,
      "learning_rate": 1.8173632945625647e-05,
      "loss": 0.2689,
      "step": 289500
    },
    {
      "epoch": 6.3762889998021155,
      "grad_norm": 11.453720092773438,
      "learning_rate": 1.8118664937006662e-05,
      "loss": 0.2716,
      "step": 290000
    },
    {
      "epoch": 6.387282601525912,
      "grad_norm": 0.25347796082496643,
      "learning_rate": 1.806369692838768e-05,
      "loss": 0.2677,
      "step": 290500
    },
    {
      "epoch": 6.398276203249709,
      "grad_norm": 1.2699402570724487,
      "learning_rate": 1.8008728919768696e-05,
      "loss": 0.279,
      "step": 291000
    },
    {
      "epoch": 6.409269804973506,
      "grad_norm": 13.40225887298584,
      "learning_rate": 1.795376091114971e-05,
      "loss": 0.2788,
      "step": 291500
    },
    {
      "epoch": 6.420263406697302,
      "grad_norm": 7.943157196044922,
      "learning_rate": 1.789879290253073e-05,
      "loss": 0.2703,
      "step": 292000
    },
    {
      "epoch": 6.431257008421099,
      "grad_norm": 1.4397212266921997,
      "learning_rate": 1.7843824893911744e-05,
      "loss": 0.2605,
      "step": 292500
    },
    {
      "epoch": 6.442250610144896,
      "grad_norm": 23.88913345336914,
      "learning_rate": 1.778885688529276e-05,
      "loss": 0.2888,
      "step": 293000
    },
    {
      "epoch": 6.453244211868692,
      "grad_norm": 14.224853515625,
      "learning_rate": 1.7733888876673778e-05,
      "loss": 0.2685,
      "step": 293500
    },
    {
      "epoch": 6.464237813592489,
      "grad_norm": 2.399991512298584,
      "learning_rate": 1.7678920868054793e-05,
      "loss": 0.2589,
      "step": 294000
    },
    {
      "epoch": 6.475231415316286,
      "grad_norm": 1.3966000080108643,
      "learning_rate": 1.762395285943581e-05,
      "loss": 0.2807,
      "step": 294500
    },
    {
      "epoch": 6.486225017040082,
      "grad_norm": 1.0332376956939697,
      "learning_rate": 1.7568984850816823e-05,
      "loss": 0.2675,
      "step": 295000
    },
    {
      "epoch": 6.497218618763879,
      "grad_norm": 11.71692180633545,
      "learning_rate": 1.7514016842197842e-05,
      "loss": 0.2508,
      "step": 295500
    },
    {
      "epoch": 6.5082122204876764,
      "grad_norm": 0.29394227266311646,
      "learning_rate": 1.745904883357886e-05,
      "loss": 0.2726,
      "step": 296000
    },
    {
      "epoch": 6.519205822211473,
      "grad_norm": 12.305815696716309,
      "learning_rate": 1.7404080824959872e-05,
      "loss": 0.2725,
      "step": 296500
    },
    {
      "epoch": 6.53019942393527,
      "grad_norm": 1.9142589569091797,
      "learning_rate": 1.734911281634089e-05,
      "loss": 0.2631,
      "step": 297000
    },
    {
      "epoch": 6.541193025659067,
      "grad_norm": 9.228385925292969,
      "learning_rate": 1.7294144807721906e-05,
      "loss": 0.2725,
      "step": 297500
    },
    {
      "epoch": 6.552186627382863,
      "grad_norm": 10.469792366027832,
      "learning_rate": 1.7239176799102924e-05,
      "loss": 0.2743,
      "step": 298000
    },
    {
      "epoch": 6.56318022910666,
      "grad_norm": 8.856887817382812,
      "learning_rate": 1.718420879048394e-05,
      "loss": 0.2968,
      "step": 298500
    },
    {
      "epoch": 6.574173830830457,
      "grad_norm": 0.42467254400253296,
      "learning_rate": 1.7129240781864954e-05,
      "loss": 0.2657,
      "step": 299000
    },
    {
      "epoch": 6.585167432554253,
      "grad_norm": 0.040815047919750214,
      "learning_rate": 1.7074272773245973e-05,
      "loss": 0.2643,
      "step": 299500
    },
    {
      "epoch": 6.59616103427805,
      "grad_norm": 0.7177983522415161,
      "learning_rate": 1.7019304764626988e-05,
      "loss": 0.2682,
      "step": 300000
    },
    {
      "epoch": 6.607154636001847,
      "grad_norm": 2.8395352363586426,
      "learning_rate": 1.6964336756008003e-05,
      "loss": 0.2855,
      "step": 300500
    },
    {
      "epoch": 6.618148237725643,
      "grad_norm": 7.50841760635376,
      "learning_rate": 1.690936874738902e-05,
      "loss": 0.2538,
      "step": 301000
    },
    {
      "epoch": 6.62914183944944,
      "grad_norm": 34.11650466918945,
      "learning_rate": 1.6854400738770037e-05,
      "loss": 0.2891,
      "step": 301500
    },
    {
      "epoch": 6.640135441173237,
      "grad_norm": 6.562036514282227,
      "learning_rate": 1.6799432730151052e-05,
      "loss": 0.2775,
      "step": 302000
    },
    {
      "epoch": 6.6511290428970335,
      "grad_norm": 21.881845474243164,
      "learning_rate": 1.674446472153207e-05,
      "loss": 0.2786,
      "step": 302500
    },
    {
      "epoch": 6.662122644620831,
      "grad_norm": 3.174856185913086,
      "learning_rate": 1.6689496712913085e-05,
      "loss": 0.2578,
      "step": 303000
    },
    {
      "epoch": 6.673116246344628,
      "grad_norm": 24.4919376373291,
      "learning_rate": 1.6634528704294104e-05,
      "loss": 0.2654,
      "step": 303500
    },
    {
      "epoch": 6.684109848068424,
      "grad_norm": 0.3773067891597748,
      "learning_rate": 1.6579560695675116e-05,
      "loss": 0.2817,
      "step": 304000
    },
    {
      "epoch": 6.695103449792221,
      "grad_norm": 0.7594506740570068,
      "learning_rate": 1.6524592687056134e-05,
      "loss": 0.267,
      "step": 304500
    },
    {
      "epoch": 6.706097051516018,
      "grad_norm": 13.220117568969727,
      "learning_rate": 1.6469624678437153e-05,
      "loss": 0.2947,
      "step": 305000
    },
    {
      "epoch": 6.717090653239815,
      "grad_norm": 0.8806545734405518,
      "learning_rate": 1.6414656669818164e-05,
      "loss": 0.2716,
      "step": 305500
    },
    {
      "epoch": 6.728084254963611,
      "grad_norm": 7.150729656219482,
      "learning_rate": 1.6359688661199183e-05,
      "loss": 0.2749,
      "step": 306000
    },
    {
      "epoch": 6.739077856687408,
      "grad_norm": 0.6124107837677002,
      "learning_rate": 1.6304720652580198e-05,
      "loss": 0.2641,
      "step": 306500
    },
    {
      "epoch": 6.750071458411204,
      "grad_norm": 0.3848426938056946,
      "learning_rate": 1.6249752643961216e-05,
      "loss": 0.2436,
      "step": 307000
    },
    {
      "epoch": 6.761065060135001,
      "grad_norm": 0.823664128780365,
      "learning_rate": 1.619478463534223e-05,
      "loss": 0.2792,
      "step": 307500
    },
    {
      "epoch": 6.772058661858798,
      "grad_norm": 6.355714797973633,
      "learning_rate": 1.6139816626723247e-05,
      "loss": 0.2684,
      "step": 308000
    },
    {
      "epoch": 6.783052263582595,
      "grad_norm": 6.7547993659973145,
      "learning_rate": 1.6084848618104265e-05,
      "loss": 0.2705,
      "step": 308500
    },
    {
      "epoch": 6.7940458653063915,
      "grad_norm": 0.5571535229682922,
      "learning_rate": 1.602988060948528e-05,
      "loss": 0.2711,
      "step": 309000
    },
    {
      "epoch": 6.805039467030189,
      "grad_norm": 0.37588396668434143,
      "learning_rate": 1.5974912600866295e-05,
      "loss": 0.2955,
      "step": 309500
    },
    {
      "epoch": 6.816033068753985,
      "grad_norm": 13.03355884552002,
      "learning_rate": 1.5919944592247314e-05,
      "loss": 0.2733,
      "step": 310000
    },
    {
      "epoch": 6.827026670477782,
      "grad_norm": 1.6009459495544434,
      "learning_rate": 1.586497658362833e-05,
      "loss": 0.2641,
      "step": 310500
    },
    {
      "epoch": 6.838020272201579,
      "grad_norm": 10.681873321533203,
      "learning_rate": 1.5810008575009344e-05,
      "loss": 0.2725,
      "step": 311000
    },
    {
      "epoch": 6.849013873925376,
      "grad_norm": 16.470001220703125,
      "learning_rate": 1.5755040566390363e-05,
      "loss": 0.2775,
      "step": 311500
    },
    {
      "epoch": 6.860007475649172,
      "grad_norm": 1.5064518451690674,
      "learning_rate": 1.5700072557771378e-05,
      "loss": 0.2669,
      "step": 312000
    },
    {
      "epoch": 6.871001077372969,
      "grad_norm": 17.275392532348633,
      "learning_rate": 1.5645104549152396e-05,
      "loss": 0.2653,
      "step": 312500
    },
    {
      "epoch": 6.881994679096765,
      "grad_norm": 0.47955748438835144,
      "learning_rate": 1.5590136540533408e-05,
      "loss": 0.2734,
      "step": 313000
    },
    {
      "epoch": 6.892988280820562,
      "grad_norm": 0.6859388947486877,
      "learning_rate": 1.5535168531914427e-05,
      "loss": 0.2736,
      "step": 313500
    },
    {
      "epoch": 6.903981882544359,
      "grad_norm": 0.5017831921577454,
      "learning_rate": 1.5480200523295445e-05,
      "loss": 0.2775,
      "step": 314000
    },
    {
      "epoch": 6.914975484268156,
      "grad_norm": 0.5704687833786011,
      "learning_rate": 1.542523251467646e-05,
      "loss": 0.2897,
      "step": 314500
    },
    {
      "epoch": 6.9259690859919525,
      "grad_norm": 1.3905911445617676,
      "learning_rate": 1.5370264506057475e-05,
      "loss": 0.2619,
      "step": 315000
    },
    {
      "epoch": 6.9369626877157495,
      "grad_norm": 1.8278248310089111,
      "learning_rate": 1.531529649743849e-05,
      "loss": 0.2765,
      "step": 315500
    },
    {
      "epoch": 6.947956289439546,
      "grad_norm": 8.460119247436523,
      "learning_rate": 1.526032848881951e-05,
      "loss": 0.2518,
      "step": 316000
    },
    {
      "epoch": 6.958949891163343,
      "grad_norm": 0.4912320673465729,
      "learning_rate": 1.5205360480200522e-05,
      "loss": 0.2829,
      "step": 316500
    },
    {
      "epoch": 6.96994349288714,
      "grad_norm": 6.315787315368652,
      "learning_rate": 1.5150392471581539e-05,
      "loss": 0.2674,
      "step": 317000
    },
    {
      "epoch": 6.980937094610937,
      "grad_norm": 13.065736770629883,
      "learning_rate": 1.5095424462962558e-05,
      "loss": 0.2647,
      "step": 317500
    },
    {
      "epoch": 6.991930696334733,
      "grad_norm": 0.720180332660675,
      "learning_rate": 1.5040456454343574e-05,
      "loss": 0.2707,
      "step": 318000
    },
    {
      "epoch": 7.00292429805853,
      "grad_norm": 6.9167799949646,
      "learning_rate": 1.4985488445724588e-05,
      "loss": 0.2771,
      "step": 318500
    },
    {
      "epoch": 7.013917899782327,
      "grad_norm": 19.348661422729492,
      "learning_rate": 1.4930520437105605e-05,
      "loss": 0.2596,
      "step": 319000
    },
    {
      "epoch": 7.024911501506123,
      "grad_norm": 6.928661346435547,
      "learning_rate": 1.4875552428486621e-05,
      "loss": 0.2475,
      "step": 319500
    },
    {
      "epoch": 7.03590510322992,
      "grad_norm": 0.3622629940509796,
      "learning_rate": 1.482058441986764e-05,
      "loss": 0.2486,
      "step": 320000
    },
    {
      "epoch": 7.046898704953717,
      "grad_norm": 3.805826187133789,
      "learning_rate": 1.4765616411248653e-05,
      "loss": 0.296,
      "step": 320500
    },
    {
      "epoch": 7.057892306677513,
      "grad_norm": 6.417112350463867,
      "learning_rate": 1.471064840262967e-05,
      "loss": 0.258,
      "step": 321000
    },
    {
      "epoch": 7.0688859084013105,
      "grad_norm": 0.8873195052146912,
      "learning_rate": 1.4655680394010687e-05,
      "loss": 0.2824,
      "step": 321500
    },
    {
      "epoch": 7.0798795101251075,
      "grad_norm": 0.37695103883743286,
      "learning_rate": 1.4600712385391702e-05,
      "loss": 0.2672,
      "step": 322000
    },
    {
      "epoch": 7.090873111848904,
      "grad_norm": 22.449119567871094,
      "learning_rate": 1.4545744376772719e-05,
      "loss": 0.2535,
      "step": 322500
    },
    {
      "epoch": 7.101866713572701,
      "grad_norm": 13.605504989624023,
      "learning_rate": 1.4490776368153736e-05,
      "loss": 0.2719,
      "step": 323000
    },
    {
      "epoch": 7.112860315296498,
      "grad_norm": 0.6208437085151672,
      "learning_rate": 1.4435808359534753e-05,
      "loss": 0.2607,
      "step": 323500
    },
    {
      "epoch": 7.123853917020294,
      "grad_norm": 0.6695338487625122,
      "learning_rate": 1.4380840350915768e-05,
      "loss": 0.2692,
      "step": 324000
    },
    {
      "epoch": 7.134847518744091,
      "grad_norm": 24.85114097595215,
      "learning_rate": 1.4325872342296784e-05,
      "loss": 0.2673,
      "step": 324500
    },
    {
      "epoch": 7.145841120467888,
      "grad_norm": 13.725542068481445,
      "learning_rate": 1.4270904333677801e-05,
      "loss": 0.2745,
      "step": 325000
    },
    {
      "epoch": 7.156834722191684,
      "grad_norm": 1.2299749851226807,
      "learning_rate": 1.4215936325058815e-05,
      "loss": 0.2446,
      "step": 325500
    },
    {
      "epoch": 7.167828323915481,
      "grad_norm": 43.543792724609375,
      "learning_rate": 1.4160968316439831e-05,
      "loss": 0.2507,
      "step": 326000
    },
    {
      "epoch": 7.178821925639278,
      "grad_norm": 2.7117557525634766,
      "learning_rate": 1.410600030782085e-05,
      "loss": 0.2675,
      "step": 326500
    },
    {
      "epoch": 7.189815527363074,
      "grad_norm": 14.741411209106445,
      "learning_rate": 1.4051032299201867e-05,
      "loss": 0.2848,
      "step": 327000
    },
    {
      "epoch": 7.200809129086871,
      "grad_norm": 7.433108806610107,
      "learning_rate": 1.399606429058288e-05,
      "loss": 0.2547,
      "step": 327500
    },
    {
      "epoch": 7.2118027308106685,
      "grad_norm": 30.521902084350586,
      "learning_rate": 1.3941096281963897e-05,
      "loss": 0.2638,
      "step": 328000
    },
    {
      "epoch": 7.222796332534465,
      "grad_norm": 7.367488384246826,
      "learning_rate": 1.3886128273344914e-05,
      "loss": 0.259,
      "step": 328500
    },
    {
      "epoch": 7.233789934258262,
      "grad_norm": 3.63354229927063,
      "learning_rate": 1.383116026472593e-05,
      "loss": 0.2814,
      "step": 329000
    },
    {
      "epoch": 7.244783535982059,
      "grad_norm": 0.2974664866924286,
      "learning_rate": 1.3776192256106946e-05,
      "loss": 0.268,
      "step": 329500
    },
    {
      "epoch": 7.255777137705855,
      "grad_norm": 8.44840145111084,
      "learning_rate": 1.3721224247487963e-05,
      "loss": 0.2508,
      "step": 330000
    },
    {
      "epoch": 7.266770739429652,
      "grad_norm": 7.657398223876953,
      "learning_rate": 1.366625623886898e-05,
      "loss": 0.254,
      "step": 330500
    },
    {
      "epoch": 7.277764341153449,
      "grad_norm": 12.107443809509277,
      "learning_rate": 1.3611288230249994e-05,
      "loss": 0.2502,
      "step": 331000
    },
    {
      "epoch": 7.288757942877245,
      "grad_norm": 3.6219642162323,
      "learning_rate": 1.3556320221631011e-05,
      "loss": 0.2543,
      "step": 331500
    },
    {
      "epoch": 7.299751544601042,
      "grad_norm": 14.24657154083252,
      "learning_rate": 1.3501352213012028e-05,
      "loss": 0.2715,
      "step": 332000
    },
    {
      "epoch": 7.310745146324839,
      "grad_norm": 5.7803544998168945,
      "learning_rate": 1.3446384204393045e-05,
      "loss": 0.2478,
      "step": 332500
    },
    {
      "epoch": 7.321738748048635,
      "grad_norm": 26.815235137939453,
      "learning_rate": 1.3391416195774058e-05,
      "loss": 0.2538,
      "step": 333000
    },
    {
      "epoch": 7.332732349772432,
      "grad_norm": 1.3733162879943848,
      "learning_rate": 1.3336448187155077e-05,
      "loss": 0.2699,
      "step": 333500
    },
    {
      "epoch": 7.343725951496229,
      "grad_norm": 0.8834508657455444,
      "learning_rate": 1.3281480178536094e-05,
      "loss": 0.2684,
      "step": 334000
    },
    {
      "epoch": 7.354719553220026,
      "grad_norm": 0.312654584646225,
      "learning_rate": 1.322651216991711e-05,
      "loss": 0.2842,
      "step": 334500
    },
    {
      "epoch": 7.365713154943823,
      "grad_norm": 1.465052843093872,
      "learning_rate": 1.3171544161298124e-05,
      "loss": 0.2531,
      "step": 335000
    },
    {
      "epoch": 7.37670675666762,
      "grad_norm": 20.769939422607422,
      "learning_rate": 1.311657615267914e-05,
      "loss": 0.2662,
      "step": 335500
    },
    {
      "epoch": 7.387700358391416,
      "grad_norm": 14.01921272277832,
      "learning_rate": 1.306160814406016e-05,
      "loss": 0.2528,
      "step": 336000
    },
    {
      "epoch": 7.398693960115213,
      "grad_norm": 5.324960708618164,
      "learning_rate": 1.3006640135441173e-05,
      "loss": 0.2615,
      "step": 336500
    },
    {
      "epoch": 7.40968756183901,
      "grad_norm": 16.46138572692871,
      "learning_rate": 1.295167212682219e-05,
      "loss": 0.2593,
      "step": 337000
    },
    {
      "epoch": 7.420681163562806,
      "grad_norm": 0.47619229555130005,
      "learning_rate": 1.2896704118203206e-05,
      "loss": 0.2767,
      "step": 337500
    },
    {
      "epoch": 7.431674765286603,
      "grad_norm": 0.23515154421329498,
      "learning_rate": 1.2841736109584223e-05,
      "loss": 0.2282,
      "step": 338000
    },
    {
      "epoch": 7.4426683670104,
      "grad_norm": 21.702959060668945,
      "learning_rate": 1.2786768100965238e-05,
      "loss": 0.265,
      "step": 338500
    },
    {
      "epoch": 7.453661968734196,
      "grad_norm": 5.511950969696045,
      "learning_rate": 1.2731800092346255e-05,
      "loss": 0.2912,
      "step": 339000
    },
    {
      "epoch": 7.464655570457993,
      "grad_norm": 7.291085720062256,
      "learning_rate": 1.2676832083727272e-05,
      "loss": 0.2688,
      "step": 339500
    },
    {
      "epoch": 7.47564917218179,
      "grad_norm": 0.3622892200946808,
      "learning_rate": 1.2621864075108289e-05,
      "loss": 0.2446,
      "step": 340000
    },
    {
      "epoch": 7.4866427739055865,
      "grad_norm": 24.785545349121094,
      "learning_rate": 1.2566896066489304e-05,
      "loss": 0.2583,
      "step": 340500
    },
    {
      "epoch": 7.497636375629384,
      "grad_norm": 18.24456787109375,
      "learning_rate": 1.251192805787032e-05,
      "loss": 0.2948,
      "step": 341000
    },
    {
      "epoch": 7.508629977353181,
      "grad_norm": 0.3474753499031067,
      "learning_rate": 1.2456960049251336e-05,
      "loss": 0.2646,
      "step": 341500
    },
    {
      "epoch": 7.519623579076978,
      "grad_norm": 0.9631456136703491,
      "learning_rate": 1.2401992040632352e-05,
      "loss": 0.273,
      "step": 342000
    },
    {
      "epoch": 7.530617180800774,
      "grad_norm": 0.8138507604598999,
      "learning_rate": 1.234702403201337e-05,
      "loss": 0.2828,
      "step": 342500
    },
    {
      "epoch": 7.541610782524571,
      "grad_norm": 0.2399323284626007,
      "learning_rate": 1.2292056023394386e-05,
      "loss": 0.2741,
      "step": 343000
    },
    {
      "epoch": 7.552604384248367,
      "grad_norm": 0.4775421619415283,
      "learning_rate": 1.2237088014775401e-05,
      "loss": 0.2504,
      "step": 343500
    },
    {
      "epoch": 7.563597985972164,
      "grad_norm": 32.190757751464844,
      "learning_rate": 1.2182120006156418e-05,
      "loss": 0.2629,
      "step": 344000
    },
    {
      "epoch": 7.574591587695961,
      "grad_norm": 1.213343858718872,
      "learning_rate": 1.2127151997537433e-05,
      "loss": 0.2895,
      "step": 344500
    },
    {
      "epoch": 7.585585189419758,
      "grad_norm": 8.925688743591309,
      "learning_rate": 1.2072183988918452e-05,
      "loss": 0.2758,
      "step": 345000
    },
    {
      "epoch": 7.596578791143554,
      "grad_norm": 19.883668899536133,
      "learning_rate": 1.2017215980299467e-05,
      "loss": 0.2673,
      "step": 345500
    },
    {
      "epoch": 7.607572392867351,
      "grad_norm": 6.212285041809082,
      "learning_rate": 1.1962247971680482e-05,
      "loss": 0.2745,
      "step": 346000
    },
    {
      "epoch": 7.6185659945911475,
      "grad_norm": 7.97428035736084,
      "learning_rate": 1.1907279963061499e-05,
      "loss": 0.2475,
      "step": 346500
    },
    {
      "epoch": 7.6295595963149445,
      "grad_norm": 0.835823118686676,
      "learning_rate": 1.1852311954442515e-05,
      "loss": 0.2803,
      "step": 347000
    },
    {
      "epoch": 7.6405531980387416,
      "grad_norm": 0.12898573279380798,
      "learning_rate": 1.1797343945823532e-05,
      "loss": 0.254,
      "step": 347500
    },
    {
      "epoch": 7.651546799762539,
      "grad_norm": 2.5905539989471436,
      "learning_rate": 1.1742375937204547e-05,
      "loss": 0.2578,
      "step": 348000
    },
    {
      "epoch": 7.662540401486335,
      "grad_norm": 9.952960014343262,
      "learning_rate": 1.1687407928585564e-05,
      "loss": 0.2769,
      "step": 348500
    },
    {
      "epoch": 7.673534003210132,
      "grad_norm": 14.621337890625,
      "learning_rate": 1.163243991996658e-05,
      "loss": 0.2542,
      "step": 349000
    },
    {
      "epoch": 7.684527604933929,
      "grad_norm": 16.269363403320312,
      "learning_rate": 1.1577471911347596e-05,
      "loss": 0.2913,
      "step": 349500
    },
    {
      "epoch": 7.695521206657725,
      "grad_norm": 0.4451834559440613,
      "learning_rate": 1.1522503902728613e-05,
      "loss": 0.2741,
      "step": 350000
    },
    {
      "epoch": 7.706514808381522,
      "grad_norm": 29.35198402404785,
      "learning_rate": 1.1467535894109628e-05,
      "loss": 0.2619,
      "step": 350500
    },
    {
      "epoch": 7.717508410105319,
      "grad_norm": 33.43767166137695,
      "learning_rate": 1.1412567885490645e-05,
      "loss": 0.2697,
      "step": 351000
    },
    {
      "epoch": 7.728502011829115,
      "grad_norm": 0.38866299390792847,
      "learning_rate": 1.135759987687166e-05,
      "loss": 0.2515,
      "step": 351500
    },
    {
      "epoch": 7.739495613552912,
      "grad_norm": 0.3432430624961853,
      "learning_rate": 1.1302631868252678e-05,
      "loss": 0.2688,
      "step": 352000
    },
    {
      "epoch": 7.750489215276709,
      "grad_norm": 9.588069915771484,
      "learning_rate": 1.1247663859633693e-05,
      "loss": 0.2539,
      "step": 352500
    },
    {
      "epoch": 7.7614828170005055,
      "grad_norm": 0.9656268954277039,
      "learning_rate": 1.119269585101471e-05,
      "loss": 0.2489,
      "step": 353000
    },
    {
      "epoch": 7.7724764187243025,
      "grad_norm": 25.718908309936523,
      "learning_rate": 1.1137727842395725e-05,
      "loss": 0.2796,
      "step": 353500
    },
    {
      "epoch": 7.7834700204480995,
      "grad_norm": 0.2751670777797699,
      "learning_rate": 1.1082759833776742e-05,
      "loss": 0.2636,
      "step": 354000
    },
    {
      "epoch": 7.794463622171896,
      "grad_norm": 5.706303596496582,
      "learning_rate": 1.1027791825157759e-05,
      "loss": 0.2702,
      "step": 354500
    },
    {
      "epoch": 7.805457223895693,
      "grad_norm": 13.396392822265625,
      "learning_rate": 1.0972823816538776e-05,
      "loss": 0.2681,
      "step": 355000
    },
    {
      "epoch": 7.81645082561949,
      "grad_norm": 32.353607177734375,
      "learning_rate": 1.0917855807919791e-05,
      "loss": 0.2382,
      "step": 355500
    },
    {
      "epoch": 7.827444427343286,
      "grad_norm": 0.2581191956996918,
      "learning_rate": 1.0862887799300806e-05,
      "loss": 0.2728,
      "step": 356000
    },
    {
      "epoch": 7.838438029067083,
      "grad_norm": 24.561582565307617,
      "learning_rate": 1.0807919790681825e-05,
      "loss": 0.2624,
      "step": 356500
    },
    {
      "epoch": 7.84943163079088,
      "grad_norm": 13.9099702835083,
      "learning_rate": 1.075295178206284e-05,
      "loss": 0.2636,
      "step": 357000
    },
    {
      "epoch": 7.860425232514676,
      "grad_norm": 9.4712553024292,
      "learning_rate": 1.0697983773443856e-05,
      "loss": 0.2711,
      "step": 357500
    },
    {
      "epoch": 7.871418834238473,
      "grad_norm": 1.6488934755325317,
      "learning_rate": 1.0643015764824872e-05,
      "loss": 0.2554,
      "step": 358000
    },
    {
      "epoch": 7.88241243596227,
      "grad_norm": 2.1579911708831787,
      "learning_rate": 1.0588047756205888e-05,
      "loss": 0.2977,
      "step": 358500
    },
    {
      "epoch": 7.893406037686066,
      "grad_norm": 35.49015426635742,
      "learning_rate": 1.0533079747586905e-05,
      "loss": 0.2791,
      "step": 359000
    },
    {
      "epoch": 7.9043996394098635,
      "grad_norm": 7.2873711585998535,
      "learning_rate": 1.0478111738967922e-05,
      "loss": 0.2786,
      "step": 359500
    },
    {
      "epoch": 7.9153932411336605,
      "grad_norm": 0.6143662929534912,
      "learning_rate": 1.0423143730348937e-05,
      "loss": 0.2599,
      "step": 360000
    },
    {
      "epoch": 7.926386842857457,
      "grad_norm": 0.6351032257080078,
      "learning_rate": 1.0368175721729952e-05,
      "loss": 0.2536,
      "step": 360500
    },
    {
      "epoch": 7.937380444581254,
      "grad_norm": 1.9005131721496582,
      "learning_rate": 1.031320771311097e-05,
      "loss": 0.2599,
      "step": 361000
    },
    {
      "epoch": 7.948374046305051,
      "grad_norm": 7.923094272613525,
      "learning_rate": 1.0258239704491986e-05,
      "loss": 0.2746,
      "step": 361500
    },
    {
      "epoch": 7.959367648028847,
      "grad_norm": 1.2769962549209595,
      "learning_rate": 1.0203271695873003e-05,
      "loss": 0.2726,
      "step": 362000
    },
    {
      "epoch": 7.970361249752644,
      "grad_norm": 0.8133516907691956,
      "learning_rate": 1.0148303687254018e-05,
      "loss": 0.2608,
      "step": 362500
    },
    {
      "epoch": 7.981354851476441,
      "grad_norm": 10.378576278686523,
      "learning_rate": 1.0093335678635035e-05,
      "loss": 0.2665,
      "step": 363000
    },
    {
      "epoch": 7.992348453200237,
      "grad_norm": 18.358715057373047,
      "learning_rate": 1.0038367670016051e-05,
      "loss": 0.2781,
      "step": 363500
    },
    {
      "epoch": 8.003342054924035,
      "grad_norm": 0.6664692759513855,
      "learning_rate": 9.983399661397068e-06,
      "loss": 0.261,
      "step": 364000
    },
    {
      "epoch": 8.01433565664783,
      "grad_norm": 0.5799651741981506,
      "learning_rate": 9.928431652778083e-06,
      "loss": 0.2559,
      "step": 364500
    },
    {
      "epoch": 8.025329258371627,
      "grad_norm": 11.784241676330566,
      "learning_rate": 9.8734636441591e-06,
      "loss": 0.2587,
      "step": 365000
    },
    {
      "epoch": 8.036322860095424,
      "grad_norm": 0.11405061930418015,
      "learning_rate": 9.818495635540117e-06,
      "loss": 0.2624,
      "step": 365500
    },
    {
      "epoch": 8.047316461819221,
      "grad_norm": 9.08200454711914,
      "learning_rate": 9.763527626921132e-06,
      "loss": 0.2649,
      "step": 366000
    },
    {
      "epoch": 8.058310063543018,
      "grad_norm": 8.54967212677002,
      "learning_rate": 9.708559618302149e-06,
      "loss": 0.2711,
      "step": 366500
    },
    {
      "epoch": 8.069303665266816,
      "grad_norm": 1.8095698356628418,
      "learning_rate": 9.653591609683164e-06,
      "loss": 0.2406,
      "step": 367000
    },
    {
      "epoch": 8.08029726699061,
      "grad_norm": 24.99897003173828,
      "learning_rate": 9.59862360106418e-06,
      "loss": 0.2431,
      "step": 367500
    },
    {
      "epoch": 8.091290868714408,
      "grad_norm": 51.13410186767578,
      "learning_rate": 9.543655592445198e-06,
      "loss": 0.2671,
      "step": 368000
    },
    {
      "epoch": 8.102284470438205,
      "grad_norm": 10.574066162109375,
      "learning_rate": 9.488687583826214e-06,
      "loss": 0.2577,
      "step": 368500
    },
    {
      "epoch": 8.113278072162002,
      "grad_norm": 0.45105910301208496,
      "learning_rate": 9.43371957520723e-06,
      "loss": 0.2362,
      "step": 369000
    },
    {
      "epoch": 8.124271673885799,
      "grad_norm": 12.547565460205078,
      "learning_rate": 9.378751566588246e-06,
      "loss": 0.2505,
      "step": 369500
    },
    {
      "epoch": 8.135265275609596,
      "grad_norm": 1.4920982122421265,
      "learning_rate": 9.323783557969261e-06,
      "loss": 0.2656,
      "step": 370000
    },
    {
      "epoch": 8.146258877333391,
      "grad_norm": 0.5405572056770325,
      "learning_rate": 9.268815549350278e-06,
      "loss": 0.2776,
      "step": 370500
    },
    {
      "epoch": 8.157252479057188,
      "grad_norm": 0.6006206274032593,
      "learning_rate": 9.213847540731295e-06,
      "loss": 0.263,
      "step": 371000
    },
    {
      "epoch": 8.168246080780985,
      "grad_norm": 15.397306442260742,
      "learning_rate": 9.15887953211231e-06,
      "loss": 0.282,
      "step": 371500
    },
    {
      "epoch": 8.179239682504782,
      "grad_norm": 3.9856138229370117,
      "learning_rate": 9.103911523493327e-06,
      "loss": 0.2558,
      "step": 372000
    },
    {
      "epoch": 8.19023328422858,
      "grad_norm": 16.448223114013672,
      "learning_rate": 9.048943514874344e-06,
      "loss": 0.2585,
      "step": 372500
    },
    {
      "epoch": 8.201226885952376,
      "grad_norm": 9.710247039794922,
      "learning_rate": 8.99397550625536e-06,
      "loss": 0.2568,
      "step": 373000
    },
    {
      "epoch": 8.212220487676172,
      "grad_norm": 21.155141830444336,
      "learning_rate": 8.939007497636376e-06,
      "loss": 0.2474,
      "step": 373500
    },
    {
      "epoch": 8.223214089399969,
      "grad_norm": 15.547738075256348,
      "learning_rate": 8.884039489017393e-06,
      "loss": 0.2593,
      "step": 374000
    },
    {
      "epoch": 8.234207691123766,
      "grad_norm": 18.229917526245117,
      "learning_rate": 8.829071480398408e-06,
      "loss": 0.2582,
      "step": 374500
    },
    {
      "epoch": 8.245201292847563,
      "grad_norm": 5.083061218261719,
      "learning_rate": 8.774103471779426e-06,
      "loss": 0.2855,
      "step": 375000
    },
    {
      "epoch": 8.25619489457136,
      "grad_norm": 0.3040788173675537,
      "learning_rate": 8.719135463160441e-06,
      "loss": 0.2506,
      "step": 375500
    },
    {
      "epoch": 8.267188496295157,
      "grad_norm": 17.079639434814453,
      "learning_rate": 8.664167454541456e-06,
      "loss": 0.2447,
      "step": 376000
    },
    {
      "epoch": 8.278182098018952,
      "grad_norm": 34.01074981689453,
      "learning_rate": 8.609199445922473e-06,
      "loss": 0.2702,
      "step": 376500
    },
    {
      "epoch": 8.28917569974275,
      "grad_norm": 0.8367383480072021,
      "learning_rate": 8.55423143730349e-06,
      "loss": 0.2634,
      "step": 377000
    },
    {
      "epoch": 8.300169301466546,
      "grad_norm": 8.205109596252441,
      "learning_rate": 8.499263428684507e-06,
      "loss": 0.2589,
      "step": 377500
    },
    {
      "epoch": 8.311162903190343,
      "grad_norm": 10.698573112487793,
      "learning_rate": 8.444295420065522e-06,
      "loss": 0.2521,
      "step": 378000
    },
    {
      "epoch": 8.32215650491414,
      "grad_norm": 7.001713275909424,
      "learning_rate": 8.389327411446539e-06,
      "loss": 0.2569,
      "step": 378500
    },
    {
      "epoch": 8.333150106637937,
      "grad_norm": 12.257295608520508,
      "learning_rate": 8.334359402827554e-06,
      "loss": 0.2593,
      "step": 379000
    },
    {
      "epoch": 8.344143708361733,
      "grad_norm": 5.782690048217773,
      "learning_rate": 8.279391394208572e-06,
      "loss": 0.2493,
      "step": 379500
    },
    {
      "epoch": 8.35513731008553,
      "grad_norm": 22.938623428344727,
      "learning_rate": 8.224423385589587e-06,
      "loss": 0.2862,
      "step": 380000
    },
    {
      "epoch": 8.366130911809327,
      "grad_norm": 22.593664169311523,
      "learning_rate": 8.169455376970603e-06,
      "loss": 0.2582,
      "step": 380500
    },
    {
      "epoch": 8.377124513533124,
      "grad_norm": 17.651029586791992,
      "learning_rate": 8.11448736835162e-06,
      "loss": 0.2619,
      "step": 381000
    },
    {
      "epoch": 8.38811811525692,
      "grad_norm": 0.4676258862018585,
      "learning_rate": 8.059519359732636e-06,
      "loss": 0.2743,
      "step": 381500
    },
    {
      "epoch": 8.399111716980718,
      "grad_norm": 1.1082463264465332,
      "learning_rate": 8.004551351113653e-06,
      "loss": 0.2509,
      "step": 382000
    },
    {
      "epoch": 8.410105318704513,
      "grad_norm": 15.62587833404541,
      "learning_rate": 7.949583342494668e-06,
      "loss": 0.2547,
      "step": 382500
    },
    {
      "epoch": 8.42109892042831,
      "grad_norm": 0.4446337819099426,
      "learning_rate": 7.894615333875685e-06,
      "loss": 0.2638,
      "step": 383000
    },
    {
      "epoch": 8.432092522152107,
      "grad_norm": 10.209952354431152,
      "learning_rate": 7.8396473252567e-06,
      "loss": 0.2577,
      "step": 383500
    },
    {
      "epoch": 8.443086123875904,
      "grad_norm": 0.292096346616745,
      "learning_rate": 7.784679316637719e-06,
      "loss": 0.2526,
      "step": 384000
    },
    {
      "epoch": 8.454079725599701,
      "grad_norm": 13.498376846313477,
      "learning_rate": 7.729711308018734e-06,
      "loss": 0.2812,
      "step": 384500
    },
    {
      "epoch": 8.465073327323498,
      "grad_norm": 22.636734008789062,
      "learning_rate": 7.67474329939975e-06,
      "loss": 0.2446,
      "step": 385000
    },
    {
      "epoch": 8.476066929047294,
      "grad_norm": 0.6988667845726013,
      "learning_rate": 7.6197752907807656e-06,
      "loss": 0.2638,
      "step": 385500
    },
    {
      "epoch": 8.48706053077109,
      "grad_norm": 1.8448106050491333,
      "learning_rate": 7.5648072821617815e-06,
      "loss": 0.2769,
      "step": 386000
    },
    {
      "epoch": 8.498054132494888,
      "grad_norm": 10.17341136932373,
      "learning_rate": 7.509839273542798e-06,
      "loss": 0.2611,
      "step": 386500
    },
    {
      "epoch": 8.509047734218685,
      "grad_norm": 0.09391575306653976,
      "learning_rate": 7.454871264923814e-06,
      "loss": 0.2689,
      "step": 387000
    },
    {
      "epoch": 8.520041335942482,
      "grad_norm": 12.588973045349121,
      "learning_rate": 7.399903256304831e-06,
      "loss": 0.2477,
      "step": 387500
    },
    {
      "epoch": 8.531034937666279,
      "grad_norm": 0.3182179927825928,
      "learning_rate": 7.344935247685847e-06,
      "loss": 0.2572,
      "step": 388000
    },
    {
      "epoch": 8.542028539390074,
      "grad_norm": 11.058145523071289,
      "learning_rate": 7.289967239066864e-06,
      "loss": 0.2665,
      "step": 388500
    },
    {
      "epoch": 8.553022141113871,
      "grad_norm": 0.515005886554718,
      "learning_rate": 7.23499923044788e-06,
      "loss": 0.2889,
      "step": 389000
    },
    {
      "epoch": 8.564015742837668,
      "grad_norm": 5.291982650756836,
      "learning_rate": 7.180031221828897e-06,
      "loss": 0.254,
      "step": 389500
    },
    {
      "epoch": 8.575009344561465,
      "grad_norm": 21.996017456054688,
      "learning_rate": 7.125063213209912e-06,
      "loss": 0.2542,
      "step": 390000
    },
    {
      "epoch": 8.586002946285262,
      "grad_norm": 0.10849342495203018,
      "learning_rate": 7.070095204590929e-06,
      "loss": 0.2485,
      "step": 390500
    },
    {
      "epoch": 8.59699654800906,
      "grad_norm": 0.7772354483604431,
      "learning_rate": 7.0151271959719445e-06,
      "loss": 0.2828,
      "step": 391000
    },
    {
      "epoch": 8.607990149732856,
      "grad_norm": 0.4593776762485504,
      "learning_rate": 6.9601591873529605e-06,
      "loss": 0.2545,
      "step": 391500
    },
    {
      "epoch": 8.618983751456652,
      "grad_norm": 0.6990107893943787,
      "learning_rate": 6.905191178733977e-06,
      "loss": 0.2642,
      "step": 392000
    },
    {
      "epoch": 8.629977353180449,
      "grad_norm": 0.6846628785133362,
      "learning_rate": 6.850223170114993e-06,
      "loss": 0.2794,
      "step": 392500
    },
    {
      "epoch": 8.640970954904246,
      "grad_norm": 1.8367207050323486,
      "learning_rate": 6.79525516149601e-06,
      "loss": 0.2654,
      "step": 393000
    },
    {
      "epoch": 8.651964556628043,
      "grad_norm": 6.812720775604248,
      "learning_rate": 6.740287152877026e-06,
      "loss": 0.2535,
      "step": 393500
    },
    {
      "epoch": 8.66295815835184,
      "grad_norm": 0.42749279737472534,
      "learning_rate": 6.685319144258043e-06,
      "loss": 0.2605,
      "step": 394000
    },
    {
      "epoch": 8.673951760075635,
      "grad_norm": 0.4507158100605011,
      "learning_rate": 6.630351135639058e-06,
      "loss": 0.2605,
      "step": 394500
    },
    {
      "epoch": 8.684945361799432,
      "grad_norm": 30.011442184448242,
      "learning_rate": 6.575383127020076e-06,
      "loss": 0.2742,
      "step": 395000
    },
    {
      "epoch": 8.69593896352323,
      "grad_norm": 7.4460062980651855,
      "learning_rate": 6.520415118401091e-06,
      "loss": 0.2577,
      "step": 395500
    },
    {
      "epoch": 8.706932565247026,
      "grad_norm": 18.759145736694336,
      "learning_rate": 6.465447109782107e-06,
      "loss": 0.2779,
      "step": 396000
    },
    {
      "epoch": 8.717926166970823,
      "grad_norm": 20.737743377685547,
      "learning_rate": 6.4104791011631235e-06,
      "loss": 0.2618,
      "step": 396500
    },
    {
      "epoch": 8.72891976869462,
      "grad_norm": 0.2285478711128235,
      "learning_rate": 6.3555110925441394e-06,
      "loss": 0.2645,
      "step": 397000
    },
    {
      "epoch": 8.739913370418417,
      "grad_norm": 20.223119735717773,
      "learning_rate": 6.300543083925156e-06,
      "loss": 0.2597,
      "step": 397500
    },
    {
      "epoch": 8.750906972142213,
      "grad_norm": 6.411953449249268,
      "learning_rate": 6.245575075306172e-06,
      "loss": 0.2632,
      "step": 398000
    },
    {
      "epoch": 8.76190057386601,
      "grad_norm": 15.867339134216309,
      "learning_rate": 6.190607066687188e-06,
      "loss": 0.2716,
      "step": 398500
    },
    {
      "epoch": 8.772894175589807,
      "grad_norm": 5.882462978363037,
      "learning_rate": 6.135639058068204e-06,
      "loss": 0.2544,
      "step": 399000
    },
    {
      "epoch": 8.783887777313604,
      "grad_norm": 4.823578834533691,
      "learning_rate": 6.080671049449221e-06,
      "loss": 0.2842,
      "step": 399500
    },
    {
      "epoch": 8.7948813790374,
      "grad_norm": 0.9170792698860168,
      "learning_rate": 6.025703040830237e-06,
      "loss": 0.2472,
      "step": 400000
    },
    {
      "epoch": 8.805874980761198,
      "grad_norm": 7.7260308265686035,
      "learning_rate": 5.970735032211254e-06,
      "loss": 0.2616,
      "step": 400500
    },
    {
      "epoch": 8.816868582484993,
      "grad_norm": 10.257424354553223,
      "learning_rate": 5.91576702359227e-06,
      "loss": 0.2567,
      "step": 401000
    },
    {
      "epoch": 8.82786218420879,
      "grad_norm": 8.020028114318848,
      "learning_rate": 5.860799014973286e-06,
      "loss": 0.2796,
      "step": 401500
    },
    {
      "epoch": 8.838855785932587,
      "grad_norm": 12.336654663085938,
      "learning_rate": 5.8058310063543024e-06,
      "loss": 0.2596,
      "step": 402000
    },
    {
      "epoch": 8.849849387656384,
      "grad_norm": 13.539714813232422,
      "learning_rate": 5.750862997735318e-06,
      "loss": 0.2502,
      "step": 402500
    },
    {
      "epoch": 8.860842989380181,
      "grad_norm": 0.7127130031585693,
      "learning_rate": 5.695894989116335e-06,
      "loss": 0.2709,
      "step": 403000
    },
    {
      "epoch": 8.871836591103978,
      "grad_norm": 0.20561730861663818,
      "learning_rate": 5.64092698049735e-06,
      "loss": 0.2568,
      "step": 403500
    },
    {
      "epoch": 8.882830192827774,
      "grad_norm": 0.4776766002178192,
      "learning_rate": 5.585958971878367e-06,
      "loss": 0.2506,
      "step": 404000
    },
    {
      "epoch": 8.89382379455157,
      "grad_norm": 20.48691749572754,
      "learning_rate": 5.530990963259383e-06,
      "loss": 0.2706,
      "step": 404500
    },
    {
      "epoch": 8.904817396275368,
      "grad_norm": 11.717845916748047,
      "learning_rate": 5.476022954640399e-06,
      "loss": 0.2486,
      "step": 405000
    },
    {
      "epoch": 8.915810997999165,
      "grad_norm": 13.339228630065918,
      "learning_rate": 5.421054946021416e-06,
      "loss": 0.2795,
      "step": 405500
    },
    {
      "epoch": 8.926804599722962,
      "grad_norm": 0.6927555799484253,
      "learning_rate": 5.366086937402432e-06,
      "loss": 0.245,
      "step": 406000
    },
    {
      "epoch": 8.937798201446759,
      "grad_norm": 31.68856430053711,
      "learning_rate": 5.311118928783449e-06,
      "loss": 0.2423,
      "step": 406500
    },
    {
      "epoch": 8.948791803170554,
      "grad_norm": 0.3357219398021698,
      "learning_rate": 5.256150920164465e-06,
      "loss": 0.2516,
      "step": 407000
    },
    {
      "epoch": 8.959785404894351,
      "grad_norm": 15.466933250427246,
      "learning_rate": 5.201182911545481e-06,
      "loss": 0.2482,
      "step": 407500
    },
    {
      "epoch": 8.970779006618148,
      "grad_norm": 24.34349822998047,
      "learning_rate": 5.146214902926497e-06,
      "loss": 0.2797,
      "step": 408000
    },
    {
      "epoch": 8.981772608341945,
      "grad_norm": 0.42251700162887573,
      "learning_rate": 5.091246894307513e-06,
      "loss": 0.2541,
      "step": 408500
    },
    {
      "epoch": 8.992766210065742,
      "grad_norm": 17.595285415649414,
      "learning_rate": 5.036278885688529e-06,
      "loss": 0.2752,
      "step": 409000
    },
    {
      "epoch": 9.00375981178954,
      "grad_norm": 33.853939056396484,
      "learning_rate": 4.981310877069545e-06,
      "loss": 0.2862,
      "step": 409500
    },
    {
      "epoch": 9.014753413513334,
      "grad_norm": 8.003698348999023,
      "learning_rate": 4.926342868450562e-06,
      "loss": 0.2535,
      "step": 410000
    },
    {
      "epoch": 9.025747015237132,
      "grad_norm": 0.16239456832408905,
      "learning_rate": 4.871374859831578e-06,
      "loss": 0.2489,
      "step": 410500
    },
    {
      "epoch": 9.036740616960929,
      "grad_norm": 24.668867111206055,
      "learning_rate": 4.816406851212595e-06,
      "loss": 0.2681,
      "step": 411000
    },
    {
      "epoch": 9.047734218684726,
      "grad_norm": 19.58768081665039,
      "learning_rate": 4.761438842593611e-06,
      "loss": 0.2531,
      "step": 411500
    },
    {
      "epoch": 9.058727820408523,
      "grad_norm": 15.512588500976562,
      "learning_rate": 4.706470833974628e-06,
      "loss": 0.2296,
      "step": 412000
    },
    {
      "epoch": 9.06972142213232,
      "grad_norm": 14.990845680236816,
      "learning_rate": 4.6515028253556436e-06,
      "loss": 0.2399,
      "step": 412500
    },
    {
      "epoch": 9.080715023856115,
      "grad_norm": 0.33715370297431946,
      "learning_rate": 4.5965348167366595e-06,
      "loss": 0.2446,
      "step": 413000
    },
    {
      "epoch": 9.091708625579912,
      "grad_norm": 5.429749965667725,
      "learning_rate": 4.5415668081176755e-06,
      "loss": 0.2779,
      "step": 413500
    },
    {
      "epoch": 9.102702227303709,
      "grad_norm": 0.28419026732444763,
      "learning_rate": 4.4865987994986914e-06,
      "loss": 0.2545,
      "step": 414000
    },
    {
      "epoch": 9.113695829027506,
      "grad_norm": 0.5488486289978027,
      "learning_rate": 4.431630790879708e-06,
      "loss": 0.2344,
      "step": 414500
    },
    {
      "epoch": 9.124689430751303,
      "grad_norm": 0.2870948612689972,
      "learning_rate": 4.376662782260724e-06,
      "loss": 0.2632,
      "step": 415000
    },
    {
      "epoch": 9.1356830324751,
      "grad_norm": 1.2778656482696533,
      "learning_rate": 4.321694773641741e-06,
      "loss": 0.2718,
      "step": 415500
    },
    {
      "epoch": 9.146676634198895,
      "grad_norm": 0.7827711701393127,
      "learning_rate": 4.266726765022757e-06,
      "loss": 0.2445,
      "step": 416000
    },
    {
      "epoch": 9.157670235922692,
      "grad_norm": 4.5229268074035645,
      "learning_rate": 4.211758756403773e-06,
      "loss": 0.2481,
      "step": 416500
    },
    {
      "epoch": 9.16866383764649,
      "grad_norm": 7.380736827850342,
      "learning_rate": 4.15679074778479e-06,
      "loss": 0.2528,
      "step": 417000
    },
    {
      "epoch": 9.179657439370287,
      "grad_norm": 3.2762579917907715,
      "learning_rate": 4.101822739165806e-06,
      "loss": 0.2581,
      "step": 417500
    },
    {
      "epoch": 9.190651041094084,
      "grad_norm": 5.5503106117248535,
      "learning_rate": 4.0468547305468225e-06,
      "loss": 0.2763,
      "step": 418000
    },
    {
      "epoch": 9.20164464281788,
      "grad_norm": 22.056472778320312,
      "learning_rate": 3.991886721927838e-06,
      "loss": 0.2601,
      "step": 418500
    },
    {
      "epoch": 9.212638244541676,
      "grad_norm": 0.2935006618499756,
      "learning_rate": 3.9369187133088544e-06,
      "loss": 0.2485,
      "step": 419000
    },
    {
      "epoch": 9.223631846265473,
      "grad_norm": 16.43678855895996,
      "learning_rate": 3.88195070468987e-06,
      "loss": 0.2568,
      "step": 419500
    },
    {
      "epoch": 9.23462544798927,
      "grad_norm": 2.782071352005005,
      "learning_rate": 3.826982696070887e-06,
      "loss": 0.2763,
      "step": 420000
    },
    {
      "epoch": 9.245619049713067,
      "grad_norm": 12.970808029174805,
      "learning_rate": 3.772014687451903e-06,
      "loss": 0.2638,
      "step": 420500
    },
    {
      "epoch": 9.256612651436864,
      "grad_norm": 13.580662727355957,
      "learning_rate": 3.7170466788329195e-06,
      "loss": 0.284,
      "step": 421000
    },
    {
      "epoch": 9.267606253160661,
      "grad_norm": 0.16665053367614746,
      "learning_rate": 3.662078670213936e-06,
      "loss": 0.2421,
      "step": 421500
    },
    {
      "epoch": 9.278599854884458,
      "grad_norm": 9.936975479125977,
      "learning_rate": 3.607110661594952e-06,
      "loss": 0.2441,
      "step": 422000
    },
    {
      "epoch": 9.289593456608253,
      "grad_norm": 0.7904689311981201,
      "learning_rate": 3.5521426529759683e-06,
      "loss": 0.2323,
      "step": 422500
    },
    {
      "epoch": 9.30058705833205,
      "grad_norm": 6.64162015914917,
      "learning_rate": 3.4971746443569847e-06,
      "loss": 0.2663,
      "step": 423000
    },
    {
      "epoch": 9.311580660055848,
      "grad_norm": 0.4328213036060333,
      "learning_rate": 3.442206635738e-06,
      "loss": 0.2739,
      "step": 423500
    },
    {
      "epoch": 9.322574261779645,
      "grad_norm": 10.342379570007324,
      "learning_rate": 3.3872386271190166e-06,
      "loss": 0.2403,
      "step": 424000
    },
    {
      "epoch": 9.333567863503442,
      "grad_norm": 9.556550979614258,
      "learning_rate": 3.332270618500033e-06,
      "loss": 0.264,
      "step": 424500
    },
    {
      "epoch": 9.344561465227237,
      "grad_norm": 14.406665802001953,
      "learning_rate": 3.2773026098810494e-06,
      "loss": 0.2585,
      "step": 425000
    },
    {
      "epoch": 9.355555066951034,
      "grad_norm": 16.582326889038086,
      "learning_rate": 3.2223346012620657e-06,
      "loss": 0.2645,
      "step": 425500
    },
    {
      "epoch": 9.366548668674831,
      "grad_norm": 13.73993968963623,
      "learning_rate": 3.167366592643082e-06,
      "loss": 0.2511,
      "step": 426000
    },
    {
      "epoch": 9.377542270398628,
      "grad_norm": 11.94059944152832,
      "learning_rate": 3.112398584024098e-06,
      "loss": 0.2719,
      "step": 426500
    },
    {
      "epoch": 9.388535872122425,
      "grad_norm": 0.5527501702308655,
      "learning_rate": 3.057430575405114e-06,
      "loss": 0.2521,
      "step": 427000
    },
    {
      "epoch": 9.399529473846222,
      "grad_norm": 21.968013763427734,
      "learning_rate": 3.0024625667861304e-06,
      "loss": 0.2468,
      "step": 427500
    },
    {
      "epoch": 9.410523075570019,
      "grad_norm": 16.56879234313965,
      "learning_rate": 2.947494558167147e-06,
      "loss": 0.2576,
      "step": 428000
    },
    {
      "epoch": 9.421516677293814,
      "grad_norm": 0.2280309796333313,
      "learning_rate": 2.892526549548163e-06,
      "loss": 0.2503,
      "step": 428500
    },
    {
      "epoch": 9.432510279017611,
      "grad_norm": 29.146259307861328,
      "learning_rate": 2.8375585409291796e-06,
      "loss": 0.2804,
      "step": 429000
    },
    {
      "epoch": 9.443503880741408,
      "grad_norm": 33.32585144042969,
      "learning_rate": 2.782590532310196e-06,
      "loss": 0.2578,
      "step": 429500
    },
    {
      "epoch": 9.454497482465205,
      "grad_norm": 0.5161885023117065,
      "learning_rate": 2.727622523691212e-06,
      "loss": 0.2658,
      "step": 430000
    },
    {
      "epoch": 9.465491084189003,
      "grad_norm": 9.948553085327148,
      "learning_rate": 2.672654515072228e-06,
      "loss": 0.2587,
      "step": 430500
    },
    {
      "epoch": 9.4764846859128,
      "grad_norm": 14.141880989074707,
      "learning_rate": 2.6176865064532443e-06,
      "loss": 0.2586,
      "step": 431000
    },
    {
      "epoch": 9.487478287636595,
      "grad_norm": 1.5758737325668335,
      "learning_rate": 2.5627184978342607e-06,
      "loss": 0.2576,
      "step": 431500
    },
    {
      "epoch": 9.498471889360392,
      "grad_norm": 0.8606505990028381,
      "learning_rate": 2.507750489215277e-06,
      "loss": 0.2728,
      "step": 432000
    },
    {
      "epoch": 9.509465491084189,
      "grad_norm": 0.3146405518054962,
      "learning_rate": 2.452782480596293e-06,
      "loss": 0.255,
      "step": 432500
    },
    {
      "epoch": 9.520459092807986,
      "grad_norm": 8.267414093017578,
      "learning_rate": 2.3978144719773094e-06,
      "loss": 0.2618,
      "step": 433000
    },
    {
      "epoch": 9.531452694531783,
      "grad_norm": 0.39820247888565063,
      "learning_rate": 2.3428464633583258e-06,
      "loss": 0.279,
      "step": 433500
    },
    {
      "epoch": 9.54244629625558,
      "grad_norm": 0.5973173975944519,
      "learning_rate": 2.2878784547393417e-06,
      "loss": 0.2645,
      "step": 434000
    },
    {
      "epoch": 9.553439897979375,
      "grad_norm": 7.833223819732666,
      "learning_rate": 2.232910446120358e-06,
      "loss": 0.2535,
      "step": 434500
    },
    {
      "epoch": 9.564433499703172,
      "grad_norm": 1.1278529167175293,
      "learning_rate": 2.177942437501374e-06,
      "loss": 0.2589,
      "step": 435000
    },
    {
      "epoch": 9.57542710142697,
      "grad_norm": 0.5806137323379517,
      "learning_rate": 2.1229744288823905e-06,
      "loss": 0.254,
      "step": 435500
    },
    {
      "epoch": 9.586420703150766,
      "grad_norm": 0.5677410960197449,
      "learning_rate": 2.068006420263407e-06,
      "loss": 0.2491,
      "step": 436000
    },
    {
      "epoch": 9.597414304874563,
      "grad_norm": 10.168159484863281,
      "learning_rate": 2.0130384116444232e-06,
      "loss": 0.2367,
      "step": 436500
    },
    {
      "epoch": 9.60840790659836,
      "grad_norm": 0.5643631219863892,
      "learning_rate": 1.9580704030254396e-06,
      "loss": 0.2628,
      "step": 437000
    },
    {
      "epoch": 9.619401508322156,
      "grad_norm": 6.0933685302734375,
      "learning_rate": 1.9031023944064554e-06,
      "loss": 0.2491,
      "step": 437500
    },
    {
      "epoch": 9.630395110045953,
      "grad_norm": 1.0765403509140015,
      "learning_rate": 1.8481343857874718e-06,
      "loss": 0.2625,
      "step": 438000
    },
    {
      "epoch": 9.64138871176975,
      "grad_norm": 50.986915588378906,
      "learning_rate": 1.7931663771684881e-06,
      "loss": 0.237,
      "step": 438500
    },
    {
      "epoch": 9.652382313493547,
      "grad_norm": 1.058186411857605,
      "learning_rate": 1.7381983685495043e-06,
      "loss": 0.2419,
      "step": 439000
    },
    {
      "epoch": 9.663375915217344,
      "grad_norm": 8.999154090881348,
      "learning_rate": 1.6832303599305207e-06,
      "loss": 0.2553,
      "step": 439500
    },
    {
      "epoch": 9.674369516941141,
      "grad_norm": 0.4857161045074463,
      "learning_rate": 1.6282623513115367e-06,
      "loss": 0.2457,
      "step": 440000
    },
    {
      "epoch": 9.685363118664936,
      "grad_norm": 16.875837326049805,
      "learning_rate": 1.573294342692553e-06,
      "loss": 0.2435,
      "step": 440500
    },
    {
      "epoch": 9.696356720388733,
      "grad_norm": 9.241517066955566,
      "learning_rate": 1.5183263340735692e-06,
      "loss": 0.2507,
      "step": 441000
    },
    {
      "epoch": 9.70735032211253,
      "grad_norm": 0.3739914894104004,
      "learning_rate": 1.4633583254545854e-06,
      "loss": 0.2446,
      "step": 441500
    },
    {
      "epoch": 9.718343923836327,
      "grad_norm": 14.115691184997559,
      "learning_rate": 1.4083903168356018e-06,
      "loss": 0.2496,
      "step": 442000
    },
    {
      "epoch": 9.729337525560124,
      "grad_norm": 0.2773579955101013,
      "learning_rate": 1.3534223082166182e-06,
      "loss": 0.2786,
      "step": 442500
    },
    {
      "epoch": 9.740331127283921,
      "grad_norm": 16.069761276245117,
      "learning_rate": 1.2984542995976341e-06,
      "loss": 0.2559,
      "step": 443000
    },
    {
      "epoch": 9.751324729007717,
      "grad_norm": 22.988222122192383,
      "learning_rate": 1.2434862909786505e-06,
      "loss": 0.26,
      "step": 443500
    },
    {
      "epoch": 9.762318330731514,
      "grad_norm": 16.46109962463379,
      "learning_rate": 1.1885182823596667e-06,
      "loss": 0.2735,
      "step": 444000
    },
    {
      "epoch": 9.77331193245531,
      "grad_norm": 8.947617530822754,
      "learning_rate": 1.133550273740683e-06,
      "loss": 0.261,
      "step": 444500
    },
    {
      "epoch": 9.784305534179108,
      "grad_norm": 14.851780891418457,
      "learning_rate": 1.0785822651216992e-06,
      "loss": 0.2791,
      "step": 445000
    },
    {
      "epoch": 9.795299135902905,
      "grad_norm": 12.917631149291992,
      "learning_rate": 1.0236142565027154e-06,
      "loss": 0.2705,
      "step": 445500
    },
    {
      "epoch": 9.806292737626702,
      "grad_norm": 16.865314483642578,
      "learning_rate": 9.686462478837318e-07,
      "loss": 0.2545,
      "step": 446000
    },
    {
      "epoch": 9.817286339350497,
      "grad_norm": 0.3257238566875458,
      "learning_rate": 9.136782392647479e-07,
      "loss": 0.2917,
      "step": 446500
    },
    {
      "epoch": 9.828279941074294,
      "grad_norm": 2.3616271018981934,
      "learning_rate": 8.587102306457642e-07,
      "loss": 0.2526,
      "step": 447000
    },
    {
      "epoch": 9.839273542798091,
      "grad_norm": 1.4025428295135498,
      "learning_rate": 8.037422220267805e-07,
      "loss": 0.2756,
      "step": 447500
    },
    {
      "epoch": 9.850267144521888,
      "grad_norm": 14.239948272705078,
      "learning_rate": 7.487742134077967e-07,
      "loss": 0.2723,
      "step": 448000
    },
    {
      "epoch": 9.861260746245685,
      "grad_norm": 0.6069947481155396,
      "learning_rate": 6.93806204788813e-07,
      "loss": 0.2567,
      "step": 448500
    },
    {
      "epoch": 9.872254347969482,
      "grad_norm": 0.5954636931419373,
      "learning_rate": 6.388381961698291e-07,
      "loss": 0.2525,
      "step": 449000
    },
    {
      "epoch": 9.883247949693278,
      "grad_norm": 9.15565299987793,
      "learning_rate": 5.838701875508454e-07,
      "loss": 0.2325,
      "step": 449500
    },
    {
      "epoch": 9.894241551417075,
      "grad_norm": 5.892383098602295,
      "learning_rate": 5.289021789318616e-07,
      "loss": 0.2414,
      "step": 450000
    },
    {
      "epoch": 9.905235153140872,
      "grad_norm": 6.890552043914795,
      "learning_rate": 4.73934170312878e-07,
      "loss": 0.247,
      "step": 450500
    },
    {
      "epoch": 9.916228754864669,
      "grad_norm": 0.23768125474452972,
      "learning_rate": 4.189661616938942e-07,
      "loss": 0.2507,
      "step": 451000
    },
    {
      "epoch": 9.927222356588466,
      "grad_norm": 3.531796455383301,
      "learning_rate": 3.6399815307491043e-07,
      "loss": 0.2599,
      "step": 451500
    },
    {
      "epoch": 9.938215958312263,
      "grad_norm": 0.5906288623809814,
      "learning_rate": 3.0903014445592666e-07,
      "loss": 0.2439,
      "step": 452000
    },
    {
      "epoch": 9.94920956003606,
      "grad_norm": 0.7303103804588318,
      "learning_rate": 2.5406213583694293e-07,
      "loss": 0.2654,
      "step": 452500
    },
    {
      "epoch": 9.960203161759855,
      "grad_norm": 1.5228626728057861,
      "learning_rate": 1.9909412721795913e-07,
      "loss": 0.2388,
      "step": 453000
    },
    {
      "epoch": 9.971196763483652,
      "grad_norm": 15.961101531982422,
      "learning_rate": 1.4412611859897539e-07,
      "loss": 0.2446,
      "step": 453500
    },
    {
      "epoch": 9.98219036520745,
      "grad_norm": 0.3052166700363159,
      "learning_rate": 8.915810997999165e-08,
      "loss": 0.2522,
      "step": 454000
    },
    {
      "epoch": 9.993183966931246,
      "grad_norm": 9.619818687438965,
      "learning_rate": 3.419010136100789e-08,
      "loss": 0.2519,
      "step": 454500
    },
    {
      "epoch": 10.0,
      "step": 454810,
      "total_flos": 8.493526426368614e+17,
      "train_loss": 0.2890971097658687,
      "train_runtime": 69786.2745,
      "train_samples_per_second": 52.137,
      "train_steps_per_second": 6.517
    }
  ],
  "logging_steps": 500,
  "max_steps": 454810,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8.493526426368614e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
