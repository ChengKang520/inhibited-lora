{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 454810,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01099360172379675,
      "grad_norm": 7.6852707862854,
      "learning_rate": 4.994514192739826e-05,
      "loss": 0.5832,
      "step": 500
    },
    {
      "epoch": 0.0219872034475935,
      "grad_norm": 6.077682018280029,
      "learning_rate": 4.989017391877927e-05,
      "loss": 0.5124,
      "step": 1000
    },
    {
      "epoch": 0.03298080517139025,
      "grad_norm": 6.411675453186035,
      "learning_rate": 4.983520591016029e-05,
      "loss": 0.4702,
      "step": 1500
    },
    {
      "epoch": 0.043974406895187,
      "grad_norm": 5.46589994430542,
      "learning_rate": 4.97802379015413e-05,
      "loss": 0.4501,
      "step": 2000
    },
    {
      "epoch": 0.05496800861898375,
      "grad_norm": 7.798507213592529,
      "learning_rate": 4.9725269892922324e-05,
      "loss": 0.4533,
      "step": 2500
    },
    {
      "epoch": 0.0659616103427805,
      "grad_norm": 9.175281524658203,
      "learning_rate": 4.967030188430334e-05,
      "loss": 0.4251,
      "step": 3000
    },
    {
      "epoch": 0.07695521206657725,
      "grad_norm": 4.6734442710876465,
      "learning_rate": 4.9615333875684354e-05,
      "loss": 0.4296,
      "step": 3500
    },
    {
      "epoch": 0.087948813790374,
      "grad_norm": 8.107154846191406,
      "learning_rate": 4.956036586706537e-05,
      "loss": 0.3943,
      "step": 4000
    },
    {
      "epoch": 0.09894241551417075,
      "grad_norm": 5.0976152420043945,
      "learning_rate": 4.9505397858446385e-05,
      "loss": 0.4105,
      "step": 4500
    },
    {
      "epoch": 0.1099360172379675,
      "grad_norm": 9.972021102905273,
      "learning_rate": 4.94504298498274e-05,
      "loss": 0.4079,
      "step": 5000
    },
    {
      "epoch": 0.12092961896176425,
      "grad_norm": 8.496883392333984,
      "learning_rate": 4.9395461841208415e-05,
      "loss": 0.3951,
      "step": 5500
    },
    {
      "epoch": 0.131923220685561,
      "grad_norm": 7.848744869232178,
      "learning_rate": 4.934049383258944e-05,
      "loss": 0.3917,
      "step": 6000
    },
    {
      "epoch": 0.14291682240935774,
      "grad_norm": 9.015657424926758,
      "learning_rate": 4.928552582397045e-05,
      "loss": 0.4088,
      "step": 6500
    },
    {
      "epoch": 0.1539104241331545,
      "grad_norm": 12.613455772399902,
      "learning_rate": 4.923055781535147e-05,
      "loss": 0.379,
      "step": 7000
    },
    {
      "epoch": 0.16490402585695124,
      "grad_norm": 5.95583963394165,
      "learning_rate": 4.917558980673249e-05,
      "loss": 0.3745,
      "step": 7500
    },
    {
      "epoch": 0.175897627580748,
      "grad_norm": 6.662814140319824,
      "learning_rate": 4.9120621798113504e-05,
      "loss": 0.3891,
      "step": 8000
    },
    {
      "epoch": 0.18689122930454474,
      "grad_norm": 4.061456680297852,
      "learning_rate": 4.906565378949451e-05,
      "loss": 0.389,
      "step": 8500
    },
    {
      "epoch": 0.1978848310283415,
      "grad_norm": 2.0106022357940674,
      "learning_rate": 4.9010685780875534e-05,
      "loss": 0.3854,
      "step": 9000
    },
    {
      "epoch": 0.20887843275213824,
      "grad_norm": 13.872797012329102,
      "learning_rate": 4.895571777225655e-05,
      "loss": 0.3788,
      "step": 9500
    },
    {
      "epoch": 0.219872034475935,
      "grad_norm": 3.976120948791504,
      "learning_rate": 4.8900749763637565e-05,
      "loss": 0.3839,
      "step": 10000
    },
    {
      "epoch": 0.23086563619973174,
      "grad_norm": 5.149259567260742,
      "learning_rate": 4.884578175501858e-05,
      "loss": 0.3789,
      "step": 10500
    },
    {
      "epoch": 0.2418592379235285,
      "grad_norm": 4.654041290283203,
      "learning_rate": 4.87908137463996e-05,
      "loss": 0.3793,
      "step": 11000
    },
    {
      "epoch": 0.25285283964732524,
      "grad_norm": 6.604604244232178,
      "learning_rate": 4.873584573778062e-05,
      "loss": 0.3775,
      "step": 11500
    },
    {
      "epoch": 0.263846441371122,
      "grad_norm": 4.747637748718262,
      "learning_rate": 4.8680877729161625e-05,
      "loss": 0.3678,
      "step": 12000
    },
    {
      "epoch": 0.2748400430949188,
      "grad_norm": 11.559839248657227,
      "learning_rate": 4.862590972054265e-05,
      "loss": 0.3703,
      "step": 12500
    },
    {
      "epoch": 0.2858336448187155,
      "grad_norm": 12.431077003479004,
      "learning_rate": 4.857094171192366e-05,
      "loss": 0.3675,
      "step": 13000
    },
    {
      "epoch": 0.29682724654251225,
      "grad_norm": 7.949130535125732,
      "learning_rate": 4.851597370330468e-05,
      "loss": 0.3832,
      "step": 13500
    },
    {
      "epoch": 0.307820848266309,
      "grad_norm": 7.031922817230225,
      "learning_rate": 4.84610056946857e-05,
      "loss": 0.3644,
      "step": 14000
    },
    {
      "epoch": 0.3188144499901058,
      "grad_norm": 4.965152740478516,
      "learning_rate": 4.8406037686066714e-05,
      "loss": 0.3714,
      "step": 14500
    },
    {
      "epoch": 0.3298080517139025,
      "grad_norm": 3.257629871368408,
      "learning_rate": 4.835106967744773e-05,
      "loss": 0.3915,
      "step": 15000
    },
    {
      "epoch": 0.34080165343769925,
      "grad_norm": 4.243026256561279,
      "learning_rate": 4.8296101668828744e-05,
      "loss": 0.366,
      "step": 15500
    },
    {
      "epoch": 0.351795255161496,
      "grad_norm": 7.148746967315674,
      "learning_rate": 4.824113366020976e-05,
      "loss": 0.364,
      "step": 16000
    },
    {
      "epoch": 0.3627888568852928,
      "grad_norm": 9.659345626831055,
      "learning_rate": 4.8186165651590775e-05,
      "loss": 0.3681,
      "step": 16500
    },
    {
      "epoch": 0.3737824586090895,
      "grad_norm": 4.289309978485107,
      "learning_rate": 4.813119764297179e-05,
      "loss": 0.3751,
      "step": 17000
    },
    {
      "epoch": 0.38477606033288625,
      "grad_norm": 9.50528621673584,
      "learning_rate": 4.807622963435281e-05,
      "loss": 0.3639,
      "step": 17500
    },
    {
      "epoch": 0.395769662056683,
      "grad_norm": 7.365665435791016,
      "learning_rate": 4.802126162573383e-05,
      "loss": 0.3574,
      "step": 18000
    },
    {
      "epoch": 0.4067632637804798,
      "grad_norm": 1.3965046405792236,
      "learning_rate": 4.796629361711484e-05,
      "loss": 0.3644,
      "step": 18500
    },
    {
      "epoch": 0.4177568655042765,
      "grad_norm": 14.387591361999512,
      "learning_rate": 4.791132560849586e-05,
      "loss": 0.368,
      "step": 19000
    },
    {
      "epoch": 0.42875046722807325,
      "grad_norm": 6.191346645355225,
      "learning_rate": 4.785635759987687e-05,
      "loss": 0.3464,
      "step": 19500
    },
    {
      "epoch": 0.43974406895187,
      "grad_norm": 3.3062357902526855,
      "learning_rate": 4.780138959125789e-05,
      "loss": 0.3511,
      "step": 20000
    },
    {
      "epoch": 0.4507376706756668,
      "grad_norm": 3.9582483768463135,
      "learning_rate": 4.774642158263891e-05,
      "loss": 0.3619,
      "step": 20500
    },
    {
      "epoch": 0.4617312723994635,
      "grad_norm": 8.021829605102539,
      "learning_rate": 4.7691453574019924e-05,
      "loss": 0.3567,
      "step": 21000
    },
    {
      "epoch": 0.47272487412326025,
      "grad_norm": 4.2584943771362305,
      "learning_rate": 4.763648556540094e-05,
      "loss": 0.3583,
      "step": 21500
    },
    {
      "epoch": 0.483718475847057,
      "grad_norm": 3.815525770187378,
      "learning_rate": 4.7581517556781954e-05,
      "loss": 0.3653,
      "step": 22000
    },
    {
      "epoch": 0.4947120775708538,
      "grad_norm": 12.823688507080078,
      "learning_rate": 4.7526549548162976e-05,
      "loss": 0.3442,
      "step": 22500
    },
    {
      "epoch": 0.5057056792946505,
      "grad_norm": 6.614245414733887,
      "learning_rate": 4.7471581539543985e-05,
      "loss": 0.3663,
      "step": 23000
    },
    {
      "epoch": 0.5166992810184473,
      "grad_norm": 14.754914283752441,
      "learning_rate": 4.7416613530925e-05,
      "loss": 0.3681,
      "step": 23500
    },
    {
      "epoch": 0.527692882742244,
      "grad_norm": 8.4029541015625,
      "learning_rate": 4.736164552230602e-05,
      "loss": 0.339,
      "step": 24000
    },
    {
      "epoch": 0.5386864844660407,
      "grad_norm": 5.317979335784912,
      "learning_rate": 4.730667751368704e-05,
      "loss": 0.3503,
      "step": 24500
    },
    {
      "epoch": 0.5496800861898375,
      "grad_norm": 5.995444297790527,
      "learning_rate": 4.725170950506805e-05,
      "loss": 0.3634,
      "step": 25000
    },
    {
      "epoch": 0.5606736879136343,
      "grad_norm": 9.527183532714844,
      "learning_rate": 4.7196741496449074e-05,
      "loss": 0.3496,
      "step": 25500
    },
    {
      "epoch": 0.571667289637431,
      "grad_norm": 11.391362190246582,
      "learning_rate": 4.714177348783009e-05,
      "loss": 0.3729,
      "step": 26000
    },
    {
      "epoch": 0.5826608913612278,
      "grad_norm": 6.251795291900635,
      "learning_rate": 4.70868054792111e-05,
      "loss": 0.3675,
      "step": 26500
    },
    {
      "epoch": 0.5936544930850245,
      "grad_norm": 7.058574676513672,
      "learning_rate": 4.703183747059212e-05,
      "loss": 0.3428,
      "step": 27000
    },
    {
      "epoch": 0.6046480948088213,
      "grad_norm": 9.008706092834473,
      "learning_rate": 4.6976869461973134e-05,
      "loss": 0.3319,
      "step": 27500
    },
    {
      "epoch": 0.615641696532618,
      "grad_norm": 4.3672051429748535,
      "learning_rate": 4.692190145335415e-05,
      "loss": 0.3499,
      "step": 28000
    },
    {
      "epoch": 0.6266352982564147,
      "grad_norm": 0.8669804930686951,
      "learning_rate": 4.6866933444735164e-05,
      "loss": 0.3548,
      "step": 28500
    },
    {
      "epoch": 0.6376288999802115,
      "grad_norm": 4.520936965942383,
      "learning_rate": 4.6811965436116186e-05,
      "loss": 0.3363,
      "step": 29000
    },
    {
      "epoch": 0.6486225017040083,
      "grad_norm": 8.921101570129395,
      "learning_rate": 4.67569974274972e-05,
      "loss": 0.3455,
      "step": 29500
    },
    {
      "epoch": 0.659616103427805,
      "grad_norm": 10.08314037322998,
      "learning_rate": 4.670202941887821e-05,
      "loss": 0.3591,
      "step": 30000
    },
    {
      "epoch": 0.6706097051516018,
      "grad_norm": 6.297346115112305,
      "learning_rate": 4.664706141025923e-05,
      "loss": 0.3482,
      "step": 30500
    },
    {
      "epoch": 0.6816033068753985,
      "grad_norm": 2.4979209899902344,
      "learning_rate": 4.659209340164025e-05,
      "loss": 0.3464,
      "step": 31000
    },
    {
      "epoch": 0.6925969085991953,
      "grad_norm": 4.285398960113525,
      "learning_rate": 4.653712539302126e-05,
      "loss": 0.3512,
      "step": 31500
    },
    {
      "epoch": 0.703590510322992,
      "grad_norm": 7.63059663772583,
      "learning_rate": 4.6482157384402284e-05,
      "loss": 0.3356,
      "step": 32000
    },
    {
      "epoch": 0.7145841120467887,
      "grad_norm": 4.101661205291748,
      "learning_rate": 4.64271893757833e-05,
      "loss": 0.3351,
      "step": 32500
    },
    {
      "epoch": 0.7255777137705856,
      "grad_norm": 4.738336563110352,
      "learning_rate": 4.6372221367164314e-05,
      "loss": 0.3471,
      "step": 33000
    },
    {
      "epoch": 0.7365713154943823,
      "grad_norm": 6.449936866760254,
      "learning_rate": 4.631725335854533e-05,
      "loss": 0.3348,
      "step": 33500
    },
    {
      "epoch": 0.747564917218179,
      "grad_norm": 4.768892765045166,
      "learning_rate": 4.6262285349926344e-05,
      "loss": 0.347,
      "step": 34000
    },
    {
      "epoch": 0.7585585189419758,
      "grad_norm": 5.954689025878906,
      "learning_rate": 4.620731734130736e-05,
      "loss": 0.3576,
      "step": 34500
    },
    {
      "epoch": 0.7695521206657725,
      "grad_norm": 5.610799312591553,
      "learning_rate": 4.6152349332688374e-05,
      "loss": 0.3237,
      "step": 35000
    },
    {
      "epoch": 0.7805457223895693,
      "grad_norm": 5.825622081756592,
      "learning_rate": 4.6097381324069396e-05,
      "loss": 0.3444,
      "step": 35500
    },
    {
      "epoch": 0.791539324113366,
      "grad_norm": 8.716038703918457,
      "learning_rate": 4.604241331545041e-05,
      "loss": 0.3456,
      "step": 36000
    },
    {
      "epoch": 0.8025329258371627,
      "grad_norm": 16.02817153930664,
      "learning_rate": 4.5987445306831427e-05,
      "loss": 0.3231,
      "step": 36500
    },
    {
      "epoch": 0.8135265275609596,
      "grad_norm": 6.176446914672852,
      "learning_rate": 4.593247729821245e-05,
      "loss": 0.3477,
      "step": 37000
    },
    {
      "epoch": 0.8245201292847563,
      "grad_norm": 7.688817977905273,
      "learning_rate": 4.587750928959346e-05,
      "loss": 0.3526,
      "step": 37500
    },
    {
      "epoch": 0.835513731008553,
      "grad_norm": 9.495223999023438,
      "learning_rate": 4.582254128097447e-05,
      "loss": 0.3498,
      "step": 38000
    },
    {
      "epoch": 0.8465073327323498,
      "grad_norm": 4.457366466522217,
      "learning_rate": 4.5767573272355494e-05,
      "loss": 0.3468,
      "step": 38500
    },
    {
      "epoch": 0.8575009344561465,
      "grad_norm": 2.594508647918701,
      "learning_rate": 4.571260526373651e-05,
      "loss": 0.3283,
      "step": 39000
    },
    {
      "epoch": 0.8684945361799433,
      "grad_norm": 5.69793701171875,
      "learning_rate": 4.5657637255117524e-05,
      "loss": 0.3367,
      "step": 39500
    },
    {
      "epoch": 0.87948813790374,
      "grad_norm": 7.199739933013916,
      "learning_rate": 4.560266924649854e-05,
      "loss": 0.3508,
      "step": 40000
    },
    {
      "epoch": 0.8904817396275367,
      "grad_norm": 10.770628929138184,
      "learning_rate": 4.554770123787956e-05,
      "loss": 0.3277,
      "step": 40500
    },
    {
      "epoch": 0.9014753413513336,
      "grad_norm": 7.6080708503723145,
      "learning_rate": 4.549273322926057e-05,
      "loss": 0.3383,
      "step": 41000
    },
    {
      "epoch": 0.9124689430751303,
      "grad_norm": 1.5242398977279663,
      "learning_rate": 4.5437765220641584e-05,
      "loss": 0.3278,
      "step": 41500
    },
    {
      "epoch": 0.923462544798927,
      "grad_norm": 4.5255303382873535,
      "learning_rate": 4.5382797212022606e-05,
      "loss": 0.3527,
      "step": 42000
    },
    {
      "epoch": 0.9344561465227238,
      "grad_norm": 3.017644166946411,
      "learning_rate": 4.532782920340362e-05,
      "loss": 0.3464,
      "step": 42500
    },
    {
      "epoch": 0.9454497482465205,
      "grad_norm": 12.017179489135742,
      "learning_rate": 4.5272861194784637e-05,
      "loss": 0.344,
      "step": 43000
    },
    {
      "epoch": 0.9564433499703173,
      "grad_norm": 6.5004472732543945,
      "learning_rate": 4.521789318616566e-05,
      "loss": 0.3298,
      "step": 43500
    },
    {
      "epoch": 0.967436951694114,
      "grad_norm": 5.65278959274292,
      "learning_rate": 4.5162925177546674e-05,
      "loss": 0.3386,
      "step": 44000
    },
    {
      "epoch": 0.9784305534179107,
      "grad_norm": 4.142390727996826,
      "learning_rate": 4.510795716892768e-05,
      "loss": 0.3198,
      "step": 44500
    },
    {
      "epoch": 0.9894241551417076,
      "grad_norm": 6.803889751434326,
      "learning_rate": 4.5052989160308704e-05,
      "loss": 0.343,
      "step": 45000
    },
    {
      "epoch": 1.0004177568655044,
      "grad_norm": 8.135403633117676,
      "learning_rate": 4.499802115168972e-05,
      "loss": 0.3407,
      "step": 45500
    },
    {
      "epoch": 1.011411358589301,
      "grad_norm": 5.691091060638428,
      "learning_rate": 4.4943053143070734e-05,
      "loss": 0.3199,
      "step": 46000
    },
    {
      "epoch": 1.0224049603130978,
      "grad_norm": 5.762421131134033,
      "learning_rate": 4.488808513445175e-05,
      "loss": 0.3275,
      "step": 46500
    },
    {
      "epoch": 1.0333985620368946,
      "grad_norm": 9.093606948852539,
      "learning_rate": 4.483311712583277e-05,
      "loss": 0.3387,
      "step": 47000
    },
    {
      "epoch": 1.0443921637606912,
      "grad_norm": 2.569868803024292,
      "learning_rate": 4.4778149117213786e-05,
      "loss": 0.3186,
      "step": 47500
    },
    {
      "epoch": 1.055385765484488,
      "grad_norm": 7.055373668670654,
      "learning_rate": 4.47231811085948e-05,
      "loss": 0.357,
      "step": 48000
    },
    {
      "epoch": 1.0663793672082849,
      "grad_norm": 5.101191520690918,
      "learning_rate": 4.4668213099975816e-05,
      "loss": 0.3328,
      "step": 48500
    },
    {
      "epoch": 1.0773729689320815,
      "grad_norm": 9.26540470123291,
      "learning_rate": 4.461324509135683e-05,
      "loss": 0.3272,
      "step": 49000
    },
    {
      "epoch": 1.0883665706558783,
      "grad_norm": 4.702136993408203,
      "learning_rate": 4.4558277082737847e-05,
      "loss": 0.3189,
      "step": 49500
    },
    {
      "epoch": 1.099360172379675,
      "grad_norm": 3.607656717300415,
      "learning_rate": 4.450330907411887e-05,
      "loss": 0.3297,
      "step": 50000
    },
    {
      "epoch": 1.1103537741034717,
      "grad_norm": 6.706134796142578,
      "learning_rate": 4.4448341065499884e-05,
      "loss": 0.3381,
      "step": 50500
    },
    {
      "epoch": 1.1213473758272685,
      "grad_norm": 3.9693987369537354,
      "learning_rate": 4.43933730568809e-05,
      "loss": 0.3124,
      "step": 51000
    },
    {
      "epoch": 1.1323409775510653,
      "grad_norm": 6.188086986541748,
      "learning_rate": 4.4338405048261914e-05,
      "loss": 0.3291,
      "step": 51500
    },
    {
      "epoch": 1.1433345792748622,
      "grad_norm": 8.857538223266602,
      "learning_rate": 4.428343703964293e-05,
      "loss": 0.3275,
      "step": 52000
    },
    {
      "epoch": 1.1543281809986587,
      "grad_norm": 1.032089114189148,
      "learning_rate": 4.4228469031023944e-05,
      "loss": 0.3286,
      "step": 52500
    },
    {
      "epoch": 1.1653217827224556,
      "grad_norm": 7.439447402954102,
      "learning_rate": 4.417350102240496e-05,
      "loss": 0.3304,
      "step": 53000
    },
    {
      "epoch": 1.1763153844462524,
      "grad_norm": 7.201578617095947,
      "learning_rate": 4.411853301378598e-05,
      "loss": 0.3254,
      "step": 53500
    },
    {
      "epoch": 1.187308986170049,
      "grad_norm": 5.040529251098633,
      "learning_rate": 4.4063565005166996e-05,
      "loss": 0.3121,
      "step": 54000
    },
    {
      "epoch": 1.1983025878938458,
      "grad_norm": 10.489144325256348,
      "learning_rate": 4.400859699654801e-05,
      "loss": 0.3261,
      "step": 54500
    },
    {
      "epoch": 1.2092961896176426,
      "grad_norm": 6.053440093994141,
      "learning_rate": 4.3953628987929026e-05,
      "loss": 0.3249,
      "step": 55000
    },
    {
      "epoch": 1.2202897913414392,
      "grad_norm": 1.7133671045303345,
      "learning_rate": 4.389866097931004e-05,
      "loss": 0.3117,
      "step": 55500
    },
    {
      "epoch": 1.231283393065236,
      "grad_norm": 12.457589149475098,
      "learning_rate": 4.384369297069106e-05,
      "loss": 0.3248,
      "step": 56000
    },
    {
      "epoch": 1.2422769947890329,
      "grad_norm": 2.3652970790863037,
      "learning_rate": 4.378872496207208e-05,
      "loss": 0.3302,
      "step": 56500
    },
    {
      "epoch": 1.2532705965128295,
      "grad_norm": 6.755651473999023,
      "learning_rate": 4.3733756953453094e-05,
      "loss": 0.3223,
      "step": 57000
    },
    {
      "epoch": 1.2642641982366263,
      "grad_norm": 11.387423515319824,
      "learning_rate": 4.367878894483411e-05,
      "loss": 0.3201,
      "step": 57500
    },
    {
      "epoch": 1.275257799960423,
      "grad_norm": 0.5141758918762207,
      "learning_rate": 4.3623820936215124e-05,
      "loss": 0.3148,
      "step": 58000
    },
    {
      "epoch": 1.2862514016842197,
      "grad_norm": 1.3489047288894653,
      "learning_rate": 4.3568852927596146e-05,
      "loss": 0.3211,
      "step": 58500
    },
    {
      "epoch": 1.2972450034080165,
      "grad_norm": 8.246599197387695,
      "learning_rate": 4.3513884918977154e-05,
      "loss": 0.325,
      "step": 59000
    },
    {
      "epoch": 1.3082386051318133,
      "grad_norm": 4.41802978515625,
      "learning_rate": 4.345891691035817e-05,
      "loss": 0.3224,
      "step": 59500
    },
    {
      "epoch": 1.31923220685561,
      "grad_norm": 10.393637657165527,
      "learning_rate": 4.340394890173919e-05,
      "loss": 0.3174,
      "step": 60000
    },
    {
      "epoch": 1.3302258085794068,
      "grad_norm": 15.993583679199219,
      "learning_rate": 4.3348980893120206e-05,
      "loss": 0.3265,
      "step": 60500
    },
    {
      "epoch": 1.3412194103032036,
      "grad_norm": 4.756715774536133,
      "learning_rate": 4.329401288450122e-05,
      "loss": 0.3026,
      "step": 61000
    },
    {
      "epoch": 1.3522130120270002,
      "grad_norm": 13.870113372802734,
      "learning_rate": 4.3239044875882236e-05,
      "loss": 0.335,
      "step": 61500
    },
    {
      "epoch": 1.363206613750797,
      "grad_norm": 4.736790180206299,
      "learning_rate": 4.318407686726326e-05,
      "loss": 0.3202,
      "step": 62000
    },
    {
      "epoch": 1.3742002154745938,
      "grad_norm": 7.43394660949707,
      "learning_rate": 4.3129108858644273e-05,
      "loss": 0.3364,
      "step": 62500
    },
    {
      "epoch": 1.3851938171983904,
      "grad_norm": 7.1568098068237305,
      "learning_rate": 4.307414085002529e-05,
      "loss": 0.3285,
      "step": 63000
    },
    {
      "epoch": 1.3961874189221872,
      "grad_norm": 10.629501342773438,
      "learning_rate": 4.3019172841406304e-05,
      "loss": 0.3255,
      "step": 63500
    },
    {
      "epoch": 1.407181020645984,
      "grad_norm": 6.8368730545043945,
      "learning_rate": 4.296420483278732e-05,
      "loss": 0.3273,
      "step": 64000
    },
    {
      "epoch": 1.4181746223697809,
      "grad_norm": 9.72579574584961,
      "learning_rate": 4.2909236824168334e-05,
      "loss": 0.3249,
      "step": 64500
    },
    {
      "epoch": 1.4291682240935775,
      "grad_norm": 7.878532409667969,
      "learning_rate": 4.2854268815549356e-05,
      "loss": 0.3176,
      "step": 65000
    },
    {
      "epoch": 1.4401618258173743,
      "grad_norm": 6.779845237731934,
      "learning_rate": 4.279930080693037e-05,
      "loss": 0.3225,
      "step": 65500
    },
    {
      "epoch": 1.451155427541171,
      "grad_norm": 9.807607650756836,
      "learning_rate": 4.2744332798311386e-05,
      "loss": 0.3074,
      "step": 66000
    },
    {
      "epoch": 1.462149029264968,
      "grad_norm": 7.541809558868408,
      "learning_rate": 4.26893647896924e-05,
      "loss": 0.3255,
      "step": 66500
    },
    {
      "epoch": 1.4731426309887645,
      "grad_norm": 11.020650863647461,
      "learning_rate": 4.2634396781073416e-05,
      "loss": 0.3253,
      "step": 67000
    },
    {
      "epoch": 1.4841362327125613,
      "grad_norm": 3.203883647918701,
      "learning_rate": 4.257942877245443e-05,
      "loss": 0.3283,
      "step": 67500
    },
    {
      "epoch": 1.4951298344363582,
      "grad_norm": 8.174692153930664,
      "learning_rate": 4.2524460763835446e-05,
      "loss": 0.3117,
      "step": 68000
    },
    {
      "epoch": 1.5061234361601548,
      "grad_norm": 7.368521690368652,
      "learning_rate": 4.246949275521647e-05,
      "loss": 0.3002,
      "step": 68500
    },
    {
      "epoch": 1.5171170378839516,
      "grad_norm": 10.585065841674805,
      "learning_rate": 4.2414524746597483e-05,
      "loss": 0.3306,
      "step": 69000
    },
    {
      "epoch": 1.5281106396077484,
      "grad_norm": 5.0981245040893555,
      "learning_rate": 4.23595567379785e-05,
      "loss": 0.3131,
      "step": 69500
    },
    {
      "epoch": 1.539104241331545,
      "grad_norm": 9.440864562988281,
      "learning_rate": 4.2304588729359514e-05,
      "loss": 0.3043,
      "step": 70000
    },
    {
      "epoch": 1.5500978430553418,
      "grad_norm": 10.246316909790039,
      "learning_rate": 4.224962072074053e-05,
      "loss": 0.3283,
      "step": 70500
    },
    {
      "epoch": 1.5610914447791386,
      "grad_norm": 3.78672194480896,
      "learning_rate": 4.2194652712121544e-05,
      "loss": 0.3047,
      "step": 71000
    },
    {
      "epoch": 1.5720850465029352,
      "grad_norm": 10.5682373046875,
      "learning_rate": 4.2139684703502566e-05,
      "loss": 0.3267,
      "step": 71500
    },
    {
      "epoch": 1.583078648226732,
      "grad_norm": 4.071053981781006,
      "learning_rate": 4.208471669488358e-05,
      "loss": 0.3014,
      "step": 72000
    },
    {
      "epoch": 1.5940722499505289,
      "grad_norm": 7.787253379821777,
      "learning_rate": 4.2029748686264596e-05,
      "loss": 0.3339,
      "step": 72500
    },
    {
      "epoch": 1.6050658516743255,
      "grad_norm": 7.807453155517578,
      "learning_rate": 4.197478067764561e-05,
      "loss": 0.3262,
      "step": 73000
    },
    {
      "epoch": 1.6160594533981223,
      "grad_norm": 13.344776153564453,
      "learning_rate": 4.1919812669026626e-05,
      "loss": 0.3028,
      "step": 73500
    },
    {
      "epoch": 1.6270530551219191,
      "grad_norm": 6.042803764343262,
      "learning_rate": 4.186484466040764e-05,
      "loss": 0.3025,
      "step": 74000
    },
    {
      "epoch": 1.6380466568457157,
      "grad_norm": 2.3158645629882812,
      "learning_rate": 4.1809876651788657e-05,
      "loss": 0.3357,
      "step": 74500
    },
    {
      "epoch": 1.6490402585695125,
      "grad_norm": 3.382023572921753,
      "learning_rate": 4.175490864316968e-05,
      "loss": 0.3187,
      "step": 75000
    },
    {
      "epoch": 1.6600338602933093,
      "grad_norm": 3.0194923877716064,
      "learning_rate": 4.1699940634550694e-05,
      "loss": 0.3191,
      "step": 75500
    },
    {
      "epoch": 1.671027462017106,
      "grad_norm": 9.115680694580078,
      "learning_rate": 4.164497262593171e-05,
      "loss": 0.3175,
      "step": 76000
    },
    {
      "epoch": 1.6820210637409028,
      "grad_norm": 0.8314847350120544,
      "learning_rate": 4.159000461731273e-05,
      "loss": 0.3113,
      "step": 76500
    },
    {
      "epoch": 1.6930146654646996,
      "grad_norm": 2.6523895263671875,
      "learning_rate": 4.1535036608693746e-05,
      "loss": 0.2964,
      "step": 77000
    },
    {
      "epoch": 1.7040082671884962,
      "grad_norm": 2.4111762046813965,
      "learning_rate": 4.1480068600074754e-05,
      "loss": 0.3201,
      "step": 77500
    },
    {
      "epoch": 1.715001868912293,
      "grad_norm": 14.64544677734375,
      "learning_rate": 4.1425100591455776e-05,
      "loss": 0.3268,
      "step": 78000
    },
    {
      "epoch": 1.7259954706360898,
      "grad_norm": 8.473756790161133,
      "learning_rate": 4.137013258283679e-05,
      "loss": 0.3084,
      "step": 78500
    },
    {
      "epoch": 1.7369890723598864,
      "grad_norm": 6.914072513580322,
      "learning_rate": 4.1315164574217806e-05,
      "loss": 0.3217,
      "step": 79000
    },
    {
      "epoch": 1.7479826740836832,
      "grad_norm": 1.459900140762329,
      "learning_rate": 4.126019656559882e-05,
      "loss": 0.3198,
      "step": 79500
    },
    {
      "epoch": 1.75897627580748,
      "grad_norm": 5.305031776428223,
      "learning_rate": 4.120522855697984e-05,
      "loss": 0.3257,
      "step": 80000
    },
    {
      "epoch": 1.7699698775312767,
      "grad_norm": 8.063501358032227,
      "learning_rate": 4.115026054836086e-05,
      "loss": 0.3117,
      "step": 80500
    },
    {
      "epoch": 1.7809634792550737,
      "grad_norm": 12.318089485168457,
      "learning_rate": 4.1095292539741867e-05,
      "loss": 0.3143,
      "step": 81000
    },
    {
      "epoch": 1.7919570809788703,
      "grad_norm": 1.3642578125,
      "learning_rate": 4.104032453112289e-05,
      "loss": 0.3099,
      "step": 81500
    },
    {
      "epoch": 1.802950682702667,
      "grad_norm": 4.656307697296143,
      "learning_rate": 4.0985356522503904e-05,
      "loss": 0.3188,
      "step": 82000
    },
    {
      "epoch": 1.813944284426464,
      "grad_norm": 6.489762783050537,
      "learning_rate": 4.093038851388492e-05,
      "loss": 0.3107,
      "step": 82500
    },
    {
      "epoch": 1.8249378861502605,
      "grad_norm": 8.385287284851074,
      "learning_rate": 4.087542050526594e-05,
      "loss": 0.3243,
      "step": 83000
    },
    {
      "epoch": 1.8359314878740571,
      "grad_norm": 12.894210815429688,
      "learning_rate": 4.0820452496646956e-05,
      "loss": 0.3203,
      "step": 83500
    },
    {
      "epoch": 1.8469250895978542,
      "grad_norm": 6.233978271484375,
      "learning_rate": 4.076548448802797e-05,
      "loss": 0.3078,
      "step": 84000
    },
    {
      "epoch": 1.8579186913216508,
      "grad_norm": 7.923521518707275,
      "learning_rate": 4.0710516479408986e-05,
      "loss": 0.3136,
      "step": 84500
    },
    {
      "epoch": 1.8689122930454476,
      "grad_norm": 5.962789058685303,
      "learning_rate": 4.065554847079e-05,
      "loss": 0.3376,
      "step": 85000
    },
    {
      "epoch": 1.8799058947692444,
      "grad_norm": 13.194419860839844,
      "learning_rate": 4.0600580462171016e-05,
      "loss": 0.3066,
      "step": 85500
    },
    {
      "epoch": 1.890899496493041,
      "grad_norm": 3.3958144187927246,
      "learning_rate": 4.054561245355203e-05,
      "loss": 0.3264,
      "step": 86000
    },
    {
      "epoch": 1.9018930982168378,
      "grad_norm": 8.471597671508789,
      "learning_rate": 4.049064444493305e-05,
      "loss": 0.3039,
      "step": 86500
    },
    {
      "epoch": 1.9128866999406346,
      "grad_norm": 2.0022053718566895,
      "learning_rate": 4.043567643631407e-05,
      "loss": 0.2996,
      "step": 87000
    },
    {
      "epoch": 1.9238803016644312,
      "grad_norm": 9.411881446838379,
      "learning_rate": 4.038070842769508e-05,
      "loss": 0.3045,
      "step": 87500
    },
    {
      "epoch": 1.934873903388228,
      "grad_norm": 9.74155044555664,
      "learning_rate": 4.0325740419076105e-05,
      "loss": 0.3303,
      "step": 88000
    },
    {
      "epoch": 1.9458675051120249,
      "grad_norm": 2.008167266845703,
      "learning_rate": 4.0270772410457114e-05,
      "loss": 0.3185,
      "step": 88500
    },
    {
      "epoch": 1.9568611068358215,
      "grad_norm": 9.984036445617676,
      "learning_rate": 4.021580440183813e-05,
      "loss": 0.3099,
      "step": 89000
    },
    {
      "epoch": 1.9678547085596183,
      "grad_norm": 9.262005805969238,
      "learning_rate": 4.016083639321915e-05,
      "loss": 0.3169,
      "step": 89500
    },
    {
      "epoch": 1.9788483102834151,
      "grad_norm": 0.9796985983848572,
      "learning_rate": 4.0105868384600166e-05,
      "loss": 0.3435,
      "step": 90000
    },
    {
      "epoch": 1.9898419120072117,
      "grad_norm": 0.6786307692527771,
      "learning_rate": 4.005090037598118e-05,
      "loss": 0.3035,
      "step": 90500
    },
    {
      "epoch": 2.0008355137310088,
      "grad_norm": 9.989644050598145,
      "learning_rate": 3.9995932367362196e-05,
      "loss": 0.3164,
      "step": 91000
    },
    {
      "epoch": 2.0118291154548054,
      "grad_norm": 9.043258666992188,
      "learning_rate": 3.994096435874322e-05,
      "loss": 0.3037,
      "step": 91500
    },
    {
      "epoch": 2.022822717178602,
      "grad_norm": 2.6026265621185303,
      "learning_rate": 3.9885996350124226e-05,
      "loss": 0.3028,
      "step": 92000
    },
    {
      "epoch": 2.033816318902399,
      "grad_norm": 6.789029121398926,
      "learning_rate": 3.983102834150524e-05,
      "loss": 0.3115,
      "step": 92500
    },
    {
      "epoch": 2.0448099206261956,
      "grad_norm": 7.926789283752441,
      "learning_rate": 3.977606033288626e-05,
      "loss": 0.311,
      "step": 93000
    },
    {
      "epoch": 2.055803522349992,
      "grad_norm": 6.2505106925964355,
      "learning_rate": 3.972109232426728e-05,
      "loss": 0.3076,
      "step": 93500
    },
    {
      "epoch": 2.0667971240737892,
      "grad_norm": 10.552349090576172,
      "learning_rate": 3.9666124315648293e-05,
      "loss": 0.3069,
      "step": 94000
    },
    {
      "epoch": 2.077790725797586,
      "grad_norm": 10.104166984558105,
      "learning_rate": 3.9611156307029315e-05,
      "loss": 0.3086,
      "step": 94500
    },
    {
      "epoch": 2.0887843275213824,
      "grad_norm": 1.4061641693115234,
      "learning_rate": 3.955618829841033e-05,
      "loss": 0.3099,
      "step": 95000
    },
    {
      "epoch": 2.0997779292451795,
      "grad_norm": 2.4948506355285645,
      "learning_rate": 3.950122028979134e-05,
      "loss": 0.3261,
      "step": 95500
    },
    {
      "epoch": 2.110771530968976,
      "grad_norm": 13.696976661682129,
      "learning_rate": 3.944625228117236e-05,
      "loss": 0.294,
      "step": 96000
    },
    {
      "epoch": 2.1217651326927727,
      "grad_norm": 10.251286506652832,
      "learning_rate": 3.9391284272553376e-05,
      "loss": 0.3214,
      "step": 96500
    },
    {
      "epoch": 2.1327587344165697,
      "grad_norm": 0.9146378636360168,
      "learning_rate": 3.933631626393439e-05,
      "loss": 0.3046,
      "step": 97000
    },
    {
      "epoch": 2.1437523361403663,
      "grad_norm": 9.13562297821045,
      "learning_rate": 3.9281348255315406e-05,
      "loss": 0.3011,
      "step": 97500
    },
    {
      "epoch": 2.154745937864163,
      "grad_norm": 7.935980796813965,
      "learning_rate": 3.922638024669643e-05,
      "loss": 0.3177,
      "step": 98000
    },
    {
      "epoch": 2.16573953958796,
      "grad_norm": 1.8780895471572876,
      "learning_rate": 3.917141223807744e-05,
      "loss": 0.3156,
      "step": 98500
    },
    {
      "epoch": 2.1767331413117565,
      "grad_norm": 1.0427724123001099,
      "learning_rate": 3.911644422945845e-05,
      "loss": 0.2988,
      "step": 99000
    },
    {
      "epoch": 2.187726743035553,
      "grad_norm": 1.5347665548324585,
      "learning_rate": 3.906147622083947e-05,
      "loss": 0.3098,
      "step": 99500
    },
    {
      "epoch": 2.19872034475935,
      "grad_norm": 1.5558490753173828,
      "learning_rate": 3.900650821222049e-05,
      "loss": 0.3012,
      "step": 100000
    },
    {
      "epoch": 2.209713946483147,
      "grad_norm": 1.572874665260315,
      "learning_rate": 3.8951540203601503e-05,
      "loss": 0.2996,
      "step": 100500
    },
    {
      "epoch": 2.2207075482069434,
      "grad_norm": 16.95810317993164,
      "learning_rate": 3.8896572194982525e-05,
      "loss": 0.3003,
      "step": 101000
    },
    {
      "epoch": 2.2317011499307404,
      "grad_norm": 4.004517078399658,
      "learning_rate": 3.884160418636354e-05,
      "loss": 0.3142,
      "step": 101500
    },
    {
      "epoch": 2.242694751654537,
      "grad_norm": 2.236243486404419,
      "learning_rate": 3.8786636177744556e-05,
      "loss": 0.305,
      "step": 102000
    },
    {
      "epoch": 2.2536883533783336,
      "grad_norm": 9.960746765136719,
      "learning_rate": 3.873166816912557e-05,
      "loss": 0.2827,
      "step": 102500
    },
    {
      "epoch": 2.2646819551021307,
      "grad_norm": 2.646700143814087,
      "learning_rate": 3.8676700160506586e-05,
      "loss": 0.3291,
      "step": 103000
    },
    {
      "epoch": 2.2756755568259273,
      "grad_norm": 7.781841278076172,
      "learning_rate": 3.86217321518876e-05,
      "loss": 0.3115,
      "step": 103500
    },
    {
      "epoch": 2.2866691585497243,
      "grad_norm": 9.663519859313965,
      "learning_rate": 3.8566764143268616e-05,
      "loss": 0.3099,
      "step": 104000
    },
    {
      "epoch": 2.297662760273521,
      "grad_norm": 4.777220249176025,
      "learning_rate": 3.851179613464964e-05,
      "loss": 0.3013,
      "step": 104500
    },
    {
      "epoch": 2.3086563619973175,
      "grad_norm": 10.792736053466797,
      "learning_rate": 3.845682812603065e-05,
      "loss": 0.3024,
      "step": 105000
    },
    {
      "epoch": 2.3196499637211145,
      "grad_norm": 5.364138126373291,
      "learning_rate": 3.840186011741167e-05,
      "loss": 0.2946,
      "step": 105500
    },
    {
      "epoch": 2.330643565444911,
      "grad_norm": 14.293880462646484,
      "learning_rate": 3.834689210879269e-05,
      "loss": 0.3009,
      "step": 106000
    },
    {
      "epoch": 2.3416371671687077,
      "grad_norm": 2.8211841583251953,
      "learning_rate": 3.82919241001737e-05,
      "loss": 0.3123,
      "step": 106500
    },
    {
      "epoch": 2.3526307688925048,
      "grad_norm": 1.526916265487671,
      "learning_rate": 3.8236956091554713e-05,
      "loss": 0.3097,
      "step": 107000
    },
    {
      "epoch": 2.3636243706163014,
      "grad_norm": 3.566422700881958,
      "learning_rate": 3.8181988082935735e-05,
      "loss": 0.2979,
      "step": 107500
    },
    {
      "epoch": 2.374617972340098,
      "grad_norm": 5.705004692077637,
      "learning_rate": 3.812702007431675e-05,
      "loss": 0.3268,
      "step": 108000
    },
    {
      "epoch": 2.385611574063895,
      "grad_norm": 0.8043462634086609,
      "learning_rate": 3.8072052065697766e-05,
      "loss": 0.2905,
      "step": 108500
    },
    {
      "epoch": 2.3966051757876916,
      "grad_norm": 1.686533808708191,
      "learning_rate": 3.801708405707878e-05,
      "loss": 0.288,
      "step": 109000
    },
    {
      "epoch": 2.407598777511488,
      "grad_norm": 7.4631028175354,
      "learning_rate": 3.79621160484598e-05,
      "loss": 0.3098,
      "step": 109500
    },
    {
      "epoch": 2.4185923792352853,
      "grad_norm": 0.9685028791427612,
      "learning_rate": 3.790714803984081e-05,
      "loss": 0.2925,
      "step": 110000
    },
    {
      "epoch": 2.429585980959082,
      "grad_norm": 5.553544998168945,
      "learning_rate": 3.7852180031221826e-05,
      "loss": 0.2823,
      "step": 110500
    },
    {
      "epoch": 2.4405795826828784,
      "grad_norm": 0.8437844514846802,
      "learning_rate": 3.779721202260285e-05,
      "loss": 0.3027,
      "step": 111000
    },
    {
      "epoch": 2.4515731844066755,
      "grad_norm": 7.926551818847656,
      "learning_rate": 3.774224401398386e-05,
      "loss": 0.3263,
      "step": 111500
    },
    {
      "epoch": 2.462566786130472,
      "grad_norm": 1.1062408685684204,
      "learning_rate": 3.768727600536488e-05,
      "loss": 0.3048,
      "step": 112000
    },
    {
      "epoch": 2.4735603878542687,
      "grad_norm": 16.668304443359375,
      "learning_rate": 3.76323079967459e-05,
      "loss": 0.2942,
      "step": 112500
    },
    {
      "epoch": 2.4845539895780657,
      "grad_norm": 0.8996566534042358,
      "learning_rate": 3.7577339988126915e-05,
      "loss": 0.286,
      "step": 113000
    },
    {
      "epoch": 2.4955475913018623,
      "grad_norm": 3.935241222381592,
      "learning_rate": 3.752237197950793e-05,
      "loss": 0.3256,
      "step": 113500
    },
    {
      "epoch": 2.506541193025659,
      "grad_norm": 9.637289047241211,
      "learning_rate": 3.7467403970888945e-05,
      "loss": 0.2972,
      "step": 114000
    },
    {
      "epoch": 2.517534794749456,
      "grad_norm": 0.8611299991607666,
      "learning_rate": 3.741243596226996e-05,
      "loss": 0.3188,
      "step": 114500
    },
    {
      "epoch": 2.5285283964732526,
      "grad_norm": 4.2578301429748535,
      "learning_rate": 3.7357467953650976e-05,
      "loss": 0.3034,
      "step": 115000
    },
    {
      "epoch": 2.539521998197049,
      "grad_norm": 7.833805084228516,
      "learning_rate": 3.730249994503199e-05,
      "loss": 0.2938,
      "step": 115500
    },
    {
      "epoch": 2.550515599920846,
      "grad_norm": 1.7365891933441162,
      "learning_rate": 3.724753193641301e-05,
      "loss": 0.2964,
      "step": 116000
    },
    {
      "epoch": 2.561509201644643,
      "grad_norm": 2.942410469055176,
      "learning_rate": 3.719256392779403e-05,
      "loss": 0.2979,
      "step": 116500
    },
    {
      "epoch": 2.5725028033684394,
      "grad_norm": 9.47018051147461,
      "learning_rate": 3.713759591917504e-05,
      "loss": 0.2985,
      "step": 117000
    },
    {
      "epoch": 2.5834964050922364,
      "grad_norm": 8.860272407531738,
      "learning_rate": 3.708262791055606e-05,
      "loss": 0.3035,
      "step": 117500
    },
    {
      "epoch": 2.594490006816033,
      "grad_norm": 6.905698776245117,
      "learning_rate": 3.702765990193707e-05,
      "loss": 0.3013,
      "step": 118000
    },
    {
      "epoch": 2.6054836085398296,
      "grad_norm": 6.705261707305908,
      "learning_rate": 3.697269189331809e-05,
      "loss": 0.2838,
      "step": 118500
    },
    {
      "epoch": 2.6164772102636267,
      "grad_norm": 21.083621978759766,
      "learning_rate": 3.691772388469911e-05,
      "loss": 0.3294,
      "step": 119000
    },
    {
      "epoch": 2.6274708119874233,
      "grad_norm": 6.337228298187256,
      "learning_rate": 3.6862755876080125e-05,
      "loss": 0.2985,
      "step": 119500
    },
    {
      "epoch": 2.63846441371122,
      "grad_norm": 6.2962188720703125,
      "learning_rate": 3.680778786746114e-05,
      "loss": 0.3019,
      "step": 120000
    },
    {
      "epoch": 2.649458015435017,
      "grad_norm": 9.194127082824707,
      "learning_rate": 3.6752819858842155e-05,
      "loss": 0.3032,
      "step": 120500
    },
    {
      "epoch": 2.6604516171588135,
      "grad_norm": 10.830204010009766,
      "learning_rate": 3.669785185022317e-05,
      "loss": 0.308,
      "step": 121000
    },
    {
      "epoch": 2.67144521888261,
      "grad_norm": 6.978017807006836,
      "learning_rate": 3.6642883841604186e-05,
      "loss": 0.3061,
      "step": 121500
    },
    {
      "epoch": 2.682438820606407,
      "grad_norm": 11.956432342529297,
      "learning_rate": 3.65879158329852e-05,
      "loss": 0.3182,
      "step": 122000
    },
    {
      "epoch": 2.6934324223302037,
      "grad_norm": 3.0290937423706055,
      "learning_rate": 3.653294782436622e-05,
      "loss": 0.3092,
      "step": 122500
    },
    {
      "epoch": 2.7044260240540003,
      "grad_norm": 18.400428771972656,
      "learning_rate": 3.647797981574724e-05,
      "loss": 0.2987,
      "step": 123000
    },
    {
      "epoch": 2.7154196257777974,
      "grad_norm": 7.220609664916992,
      "learning_rate": 3.642301180712825e-05,
      "loss": 0.284,
      "step": 123500
    },
    {
      "epoch": 2.726413227501594,
      "grad_norm": 13.259928703308105,
      "learning_rate": 3.6368043798509275e-05,
      "loss": 0.3211,
      "step": 124000
    },
    {
      "epoch": 2.7374068292253906,
      "grad_norm": 10.28958511352539,
      "learning_rate": 3.631307578989028e-05,
      "loss": 0.2974,
      "step": 124500
    },
    {
      "epoch": 2.7484004309491876,
      "grad_norm": 5.506535053253174,
      "learning_rate": 3.62581077812713e-05,
      "loss": 0.3112,
      "step": 125000
    },
    {
      "epoch": 2.759394032672984,
      "grad_norm": 16.35608673095703,
      "learning_rate": 3.620313977265232e-05,
      "loss": 0.2975,
      "step": 125500
    },
    {
      "epoch": 2.770387634396781,
      "grad_norm": 13.450288772583008,
      "learning_rate": 3.6148171764033335e-05,
      "loss": 0.3044,
      "step": 126000
    },
    {
      "epoch": 2.781381236120578,
      "grad_norm": 9.631340026855469,
      "learning_rate": 3.609320375541435e-05,
      "loss": 0.3067,
      "step": 126500
    },
    {
      "epoch": 2.7923748378443745,
      "grad_norm": 10.105132102966309,
      "learning_rate": 3.6038235746795365e-05,
      "loss": 0.2945,
      "step": 127000
    },
    {
      "epoch": 2.803368439568171,
      "grad_norm": 9.143454551696777,
      "learning_rate": 3.598326773817639e-05,
      "loss": 0.3147,
      "step": 127500
    },
    {
      "epoch": 2.814362041291968,
      "grad_norm": 2.2116432189941406,
      "learning_rate": 3.59282997295574e-05,
      "loss": 0.2855,
      "step": 128000
    },
    {
      "epoch": 2.8253556430157647,
      "grad_norm": 4.285165786743164,
      "learning_rate": 3.587333172093841e-05,
      "loss": 0.2936,
      "step": 128500
    },
    {
      "epoch": 2.8363492447395617,
      "grad_norm": 5.547806262969971,
      "learning_rate": 3.581836371231943e-05,
      "loss": 0.301,
      "step": 129000
    },
    {
      "epoch": 2.8473428464633583,
      "grad_norm": 6.723481178283691,
      "learning_rate": 3.576339570370045e-05,
      "loss": 0.2935,
      "step": 129500
    },
    {
      "epoch": 2.858336448187155,
      "grad_norm": 5.84812068939209,
      "learning_rate": 3.570842769508146e-05,
      "loss": 0.3056,
      "step": 130000
    },
    {
      "epoch": 2.869330049910952,
      "grad_norm": 8.834454536437988,
      "learning_rate": 3.5653459686462485e-05,
      "loss": 0.283,
      "step": 130500
    },
    {
      "epoch": 2.8803236516347486,
      "grad_norm": 3.3290088176727295,
      "learning_rate": 3.55984916778435e-05,
      "loss": 0.2929,
      "step": 131000
    },
    {
      "epoch": 2.891317253358545,
      "grad_norm": 1.3517444133758545,
      "learning_rate": 3.5543523669224515e-05,
      "loss": 0.2966,
      "step": 131500
    },
    {
      "epoch": 2.902310855082342,
      "grad_norm": 4.787579536437988,
      "learning_rate": 3.548855566060553e-05,
      "loss": 0.2868,
      "step": 132000
    },
    {
      "epoch": 2.913304456806139,
      "grad_norm": 9.924245834350586,
      "learning_rate": 3.5433587651986545e-05,
      "loss": 0.3048,
      "step": 132500
    },
    {
      "epoch": 2.924298058529936,
      "grad_norm": 0.5383116602897644,
      "learning_rate": 3.537861964336756e-05,
      "loss": 0.2828,
      "step": 133000
    },
    {
      "epoch": 2.9352916602537324,
      "grad_norm": 0.9611040949821472,
      "learning_rate": 3.5323651634748575e-05,
      "loss": 0.2997,
      "step": 133500
    },
    {
      "epoch": 2.946285261977529,
      "grad_norm": 7.56805419921875,
      "learning_rate": 3.52686836261296e-05,
      "loss": 0.2957,
      "step": 134000
    },
    {
      "epoch": 2.957278863701326,
      "grad_norm": 9.213335037231445,
      "learning_rate": 3.521371561751061e-05,
      "loss": 0.2953,
      "step": 134500
    },
    {
      "epoch": 2.9682724654251227,
      "grad_norm": 8.919504165649414,
      "learning_rate": 3.515874760889163e-05,
      "loss": 0.304,
      "step": 135000
    },
    {
      "epoch": 2.9792660671489193,
      "grad_norm": 10.263203620910645,
      "learning_rate": 3.510377960027264e-05,
      "loss": 0.2993,
      "step": 135500
    },
    {
      "epoch": 2.9902596688727163,
      "grad_norm": 8.062067031860352,
      "learning_rate": 3.504881159165366e-05,
      "loss": 0.2976,
      "step": 136000
    },
    {
      "epoch": 3.001253270596513,
      "grad_norm": 14.30807876586914,
      "learning_rate": 3.499384358303467e-05,
      "loss": 0.2833,
      "step": 136500
    },
    {
      "epoch": 3.0122468723203095,
      "grad_norm": 0.4927581548690796,
      "learning_rate": 3.4938875574415695e-05,
      "loss": 0.3009,
      "step": 137000
    },
    {
      "epoch": 3.023240474044106,
      "grad_norm": 6.926885604858398,
      "learning_rate": 3.488390756579671e-05,
      "loss": 0.2988,
      "step": 137500
    },
    {
      "epoch": 3.034234075767903,
      "grad_norm": 17.89851951599121,
      "learning_rate": 3.4828939557177725e-05,
      "loss": 0.2954,
      "step": 138000
    },
    {
      "epoch": 3.0452276774916998,
      "grad_norm": 15.316155433654785,
      "learning_rate": 3.477397154855874e-05,
      "loss": 0.2959,
      "step": 138500
    },
    {
      "epoch": 3.056221279215497,
      "grad_norm": 1.7783409357070923,
      "learning_rate": 3.4719003539939755e-05,
      "loss": 0.2963,
      "step": 139000
    },
    {
      "epoch": 3.0672148809392934,
      "grad_norm": 10.706584930419922,
      "learning_rate": 3.466403553132077e-05,
      "loss": 0.2909,
      "step": 139500
    },
    {
      "epoch": 3.07820848266309,
      "grad_norm": 5.2575483322143555,
      "learning_rate": 3.4609067522701786e-05,
      "loss": 0.2848,
      "step": 140000
    },
    {
      "epoch": 3.089202084386887,
      "grad_norm": 0.24055998027324677,
      "learning_rate": 3.455409951408281e-05,
      "loss": 0.3059,
      "step": 140500
    },
    {
      "epoch": 3.1001956861106836,
      "grad_norm": 6.349670886993408,
      "learning_rate": 3.449913150546382e-05,
      "loss": 0.2987,
      "step": 141000
    },
    {
      "epoch": 3.1111892878344802,
      "grad_norm": 14.964327812194824,
      "learning_rate": 3.444416349684484e-05,
      "loss": 0.2848,
      "step": 141500
    },
    {
      "epoch": 3.1221828895582773,
      "grad_norm": 6.732614994049072,
      "learning_rate": 3.438919548822585e-05,
      "loss": 0.2842,
      "step": 142000
    },
    {
      "epoch": 3.133176491282074,
      "grad_norm": 9.918999671936035,
      "learning_rate": 3.4334227479606875e-05,
      "loss": 0.2905,
      "step": 142500
    },
    {
      "epoch": 3.1441700930058705,
      "grad_norm": 7.882229328155518,
      "learning_rate": 3.427925947098788e-05,
      "loss": 0.2961,
      "step": 143000
    },
    {
      "epoch": 3.1551636947296675,
      "grad_norm": 2.1625654697418213,
      "learning_rate": 3.42242914623689e-05,
      "loss": 0.2831,
      "step": 143500
    },
    {
      "epoch": 3.166157296453464,
      "grad_norm": 12.269408226013184,
      "learning_rate": 3.416932345374992e-05,
      "loss": 0.2892,
      "step": 144000
    },
    {
      "epoch": 3.1771508981772607,
      "grad_norm": 3.173478364944458,
      "learning_rate": 3.4114355445130935e-05,
      "loss": 0.2983,
      "step": 144500
    },
    {
      "epoch": 3.1881444999010577,
      "grad_norm": 1.3612077236175537,
      "learning_rate": 3.405938743651195e-05,
      "loss": 0.2968,
      "step": 145000
    },
    {
      "epoch": 3.1991381016248543,
      "grad_norm": 0.31188786029815674,
      "learning_rate": 3.400441942789297e-05,
      "loss": 0.2889,
      "step": 145500
    },
    {
      "epoch": 3.210131703348651,
      "grad_norm": 7.812214374542236,
      "learning_rate": 3.394945141927399e-05,
      "loss": 0.2938,
      "step": 146000
    },
    {
      "epoch": 3.221125305072448,
      "grad_norm": 10.08163070678711,
      "learning_rate": 3.3894483410654996e-05,
      "loss": 0.3027,
      "step": 146500
    },
    {
      "epoch": 3.2321189067962446,
      "grad_norm": 14.644403457641602,
      "learning_rate": 3.383951540203602e-05,
      "loss": 0.2979,
      "step": 147000
    },
    {
      "epoch": 3.243112508520041,
      "grad_norm": 13.806844711303711,
      "learning_rate": 3.378454739341703e-05,
      "loss": 0.2866,
      "step": 147500
    },
    {
      "epoch": 3.2541061102438382,
      "grad_norm": 12.161569595336914,
      "learning_rate": 3.372957938479805e-05,
      "loss": 0.3023,
      "step": 148000
    },
    {
      "epoch": 3.265099711967635,
      "grad_norm": 8.846882820129395,
      "learning_rate": 3.367461137617906e-05,
      "loss": 0.3044,
      "step": 148500
    },
    {
      "epoch": 3.2760933136914314,
      "grad_norm": 6.716907978057861,
      "learning_rate": 3.3619643367560085e-05,
      "loss": 0.3128,
      "step": 149000
    },
    {
      "epoch": 3.2870869154152285,
      "grad_norm": 2.965118408203125,
      "learning_rate": 3.35646753589411e-05,
      "loss": 0.2878,
      "step": 149500
    },
    {
      "epoch": 3.298080517139025,
      "grad_norm": 14.390677452087402,
      "learning_rate": 3.350970735032211e-05,
      "loss": 0.2854,
      "step": 150000
    },
    {
      "epoch": 3.3090741188628217,
      "grad_norm": 5.812280654907227,
      "learning_rate": 3.345473934170313e-05,
      "loss": 0.3038,
      "step": 150500
    },
    {
      "epoch": 3.3200677205866187,
      "grad_norm": 0.5650742053985596,
      "learning_rate": 3.3399771333084145e-05,
      "loss": 0.298,
      "step": 151000
    },
    {
      "epoch": 3.3310613223104153,
      "grad_norm": 10.516111373901367,
      "learning_rate": 3.334480332446516e-05,
      "loss": 0.2868,
      "step": 151500
    },
    {
      "epoch": 3.342054924034212,
      "grad_norm": 7.457797527313232,
      "learning_rate": 3.328983531584618e-05,
      "loss": 0.2898,
      "step": 152000
    },
    {
      "epoch": 3.353048525758009,
      "grad_norm": 13.127605438232422,
      "learning_rate": 3.32348673072272e-05,
      "loss": 0.2825,
      "step": 152500
    },
    {
      "epoch": 3.3640421274818055,
      "grad_norm": 2.1615912914276123,
      "learning_rate": 3.317989929860821e-05,
      "loss": 0.2889,
      "step": 153000
    },
    {
      "epoch": 3.375035729205602,
      "grad_norm": 5.523284912109375,
      "learning_rate": 3.312493128998923e-05,
      "loss": 0.2832,
      "step": 153500
    },
    {
      "epoch": 3.386029330929399,
      "grad_norm": 1.150922179222107,
      "learning_rate": 3.306996328137024e-05,
      "loss": 0.2949,
      "step": 154000
    },
    {
      "epoch": 3.3970229326531958,
      "grad_norm": 12.670907020568848,
      "learning_rate": 3.301499527275126e-05,
      "loss": 0.2959,
      "step": 154500
    },
    {
      "epoch": 3.4080165343769924,
      "grad_norm": 1.8166604042053223,
      "learning_rate": 3.296002726413227e-05,
      "loss": 0.298,
      "step": 155000
    },
    {
      "epoch": 3.4190101361007894,
      "grad_norm": 10.225469589233398,
      "learning_rate": 3.2905059255513295e-05,
      "loss": 0.3013,
      "step": 155500
    },
    {
      "epoch": 3.430003737824586,
      "grad_norm": 10.566052436828613,
      "learning_rate": 3.285009124689431e-05,
      "loss": 0.2904,
      "step": 156000
    },
    {
      "epoch": 3.4409973395483826,
      "grad_norm": 13.52134895324707,
      "learning_rate": 3.2795123238275325e-05,
      "loss": 0.2901,
      "step": 156500
    },
    {
      "epoch": 3.4519909412721796,
      "grad_norm": 7.4479804039001465,
      "learning_rate": 3.274015522965635e-05,
      "loss": 0.2828,
      "step": 157000
    },
    {
      "epoch": 3.4629845429959762,
      "grad_norm": 10.58980655670166,
      "learning_rate": 3.2685187221037355e-05,
      "loss": 0.3057,
      "step": 157500
    },
    {
      "epoch": 3.473978144719773,
      "grad_norm": 0.5098289847373962,
      "learning_rate": 3.263021921241837e-05,
      "loss": 0.2844,
      "step": 158000
    },
    {
      "epoch": 3.48497174644357,
      "grad_norm": 14.433059692382812,
      "learning_rate": 3.257525120379939e-05,
      "loss": 0.2757,
      "step": 158500
    },
    {
      "epoch": 3.4959653481673665,
      "grad_norm": 17.052282333374023,
      "learning_rate": 3.252028319518041e-05,
      "loss": 0.2863,
      "step": 159000
    },
    {
      "epoch": 3.506958949891163,
      "grad_norm": 8.884498596191406,
      "learning_rate": 3.246531518656142e-05,
      "loss": 0.2942,
      "step": 159500
    },
    {
      "epoch": 3.51795255161496,
      "grad_norm": 11.121171951293945,
      "learning_rate": 3.241034717794244e-05,
      "loss": 0.282,
      "step": 160000
    },
    {
      "epoch": 3.5289461533387567,
      "grad_norm": 7.700333118438721,
      "learning_rate": 3.235537916932346e-05,
      "loss": 0.3027,
      "step": 160500
    },
    {
      "epoch": 3.5399397550625533,
      "grad_norm": 14.526392936706543,
      "learning_rate": 3.230041116070447e-05,
      "loss": 0.3065,
      "step": 161000
    },
    {
      "epoch": 3.5509333567863504,
      "grad_norm": 18.703506469726562,
      "learning_rate": 3.224544315208548e-05,
      "loss": 0.286,
      "step": 161500
    },
    {
      "epoch": 3.561926958510147,
      "grad_norm": 16.31218719482422,
      "learning_rate": 3.2190475143466505e-05,
      "loss": 0.2985,
      "step": 162000
    },
    {
      "epoch": 3.572920560233944,
      "grad_norm": 7.837226390838623,
      "learning_rate": 3.213550713484752e-05,
      "loss": 0.2853,
      "step": 162500
    },
    {
      "epoch": 3.5839141619577406,
      "grad_norm": 0.6673486232757568,
      "learning_rate": 3.2080539126228535e-05,
      "loss": 0.2806,
      "step": 163000
    },
    {
      "epoch": 3.594907763681537,
      "grad_norm": 7.006541728973389,
      "learning_rate": 3.202557111760956e-05,
      "loss": 0.2896,
      "step": 163500
    },
    {
      "epoch": 3.6059013654053342,
      "grad_norm": 6.810146808624268,
      "learning_rate": 3.197060310899057e-05,
      "loss": 0.2884,
      "step": 164000
    },
    {
      "epoch": 3.616894967129131,
      "grad_norm": 7.014373302459717,
      "learning_rate": 3.191563510037158e-05,
      "loss": 0.3044,
      "step": 164500
    },
    {
      "epoch": 3.6278885688529274,
      "grad_norm": 9.071928977966309,
      "learning_rate": 3.18606670917526e-05,
      "loss": 0.284,
      "step": 165000
    },
    {
      "epoch": 3.6388821705767245,
      "grad_norm": 2.148993492126465,
      "learning_rate": 3.180569908313362e-05,
      "loss": 0.2814,
      "step": 165500
    },
    {
      "epoch": 3.649875772300521,
      "grad_norm": 3.737999439239502,
      "learning_rate": 3.175073107451463e-05,
      "loss": 0.2839,
      "step": 166000
    },
    {
      "epoch": 3.660869374024318,
      "grad_norm": 0.6818956136703491,
      "learning_rate": 3.169576306589565e-05,
      "loss": 0.2862,
      "step": 166500
    },
    {
      "epoch": 3.6718629757481147,
      "grad_norm": 6.828344345092773,
      "learning_rate": 3.164079505727667e-05,
      "loss": 0.2981,
      "step": 167000
    },
    {
      "epoch": 3.6828565774719113,
      "grad_norm": 5.8774027824401855,
      "learning_rate": 3.1585827048657685e-05,
      "loss": 0.2826,
      "step": 167500
    },
    {
      "epoch": 3.6938501791957083,
      "grad_norm": 0.3868255019187927,
      "learning_rate": 3.15308590400387e-05,
      "loss": 0.2798,
      "step": 168000
    },
    {
      "epoch": 3.704843780919505,
      "grad_norm": 9.095236778259277,
      "learning_rate": 3.1475891031419715e-05,
      "loss": 0.3018,
      "step": 168500
    },
    {
      "epoch": 3.7158373826433015,
      "grad_norm": 10.832324028015137,
      "learning_rate": 3.142092302280073e-05,
      "loss": 0.2959,
      "step": 169000
    },
    {
      "epoch": 3.7268309843670986,
      "grad_norm": 14.834376335144043,
      "learning_rate": 3.1365955014181745e-05,
      "loss": 0.2858,
      "step": 169500
    },
    {
      "epoch": 3.737824586090895,
      "grad_norm": 0.5412911176681519,
      "learning_rate": 3.131098700556277e-05,
      "loss": 0.2877,
      "step": 170000
    },
    {
      "epoch": 3.748818187814692,
      "grad_norm": 0.7653338313102722,
      "learning_rate": 3.125601899694378e-05,
      "loss": 0.2824,
      "step": 170500
    },
    {
      "epoch": 3.759811789538489,
      "grad_norm": 6.3549370765686035,
      "learning_rate": 3.12010509883248e-05,
      "loss": 0.2905,
      "step": 171000
    },
    {
      "epoch": 3.7708053912622854,
      "grad_norm": 11.727996826171875,
      "learning_rate": 3.114608297970581e-05,
      "loss": 0.2897,
      "step": 171500
    },
    {
      "epoch": 3.781798992986082,
      "grad_norm": 10.040257453918457,
      "learning_rate": 3.109111497108683e-05,
      "loss": 0.2909,
      "step": 172000
    },
    {
      "epoch": 3.792792594709879,
      "grad_norm": 2.4892396926879883,
      "learning_rate": 3.103614696246784e-05,
      "loss": 0.3073,
      "step": 172500
    },
    {
      "epoch": 3.8037861964336757,
      "grad_norm": 7.492894649505615,
      "learning_rate": 3.098117895384886e-05,
      "loss": 0.2769,
      "step": 173000
    },
    {
      "epoch": 3.8147797981574723,
      "grad_norm": 7.704427719116211,
      "learning_rate": 3.092621094522988e-05,
      "loss": 0.2837,
      "step": 173500
    },
    {
      "epoch": 3.8257733998812693,
      "grad_norm": 2.491338014602661,
      "learning_rate": 3.0871242936610895e-05,
      "loss": 0.2932,
      "step": 174000
    },
    {
      "epoch": 3.836767001605066,
      "grad_norm": 5.972451686859131,
      "learning_rate": 3.081627492799191e-05,
      "loss": 0.309,
      "step": 174500
    },
    {
      "epoch": 3.8477606033288625,
      "grad_norm": 1.4980779886245728,
      "learning_rate": 3.076130691937293e-05,
      "loss": 0.2775,
      "step": 175000
    },
    {
      "epoch": 3.8587542050526595,
      "grad_norm": 4.519969463348389,
      "learning_rate": 3.070633891075394e-05,
      "loss": 0.273,
      "step": 175500
    },
    {
      "epoch": 3.869747806776456,
      "grad_norm": 12.655150413513184,
      "learning_rate": 3.0651370902134955e-05,
      "loss": 0.279,
      "step": 176000
    },
    {
      "epoch": 3.8807414085002527,
      "grad_norm": 8.98100471496582,
      "learning_rate": 3.059640289351598e-05,
      "loss": 0.2928,
      "step": 176500
    },
    {
      "epoch": 3.8917350102240498,
      "grad_norm": 10.903685569763184,
      "learning_rate": 3.054143488489699e-05,
      "loss": 0.2934,
      "step": 177000
    },
    {
      "epoch": 3.9027286119478464,
      "grad_norm": 9.265530586242676,
      "learning_rate": 3.0486466876278007e-05,
      "loss": 0.2882,
      "step": 177500
    },
    {
      "epoch": 3.913722213671643,
      "grad_norm": 0.7618274092674255,
      "learning_rate": 3.0431498867659026e-05,
      "loss": 0.3053,
      "step": 178000
    },
    {
      "epoch": 3.92471581539544,
      "grad_norm": 16.53829002380371,
      "learning_rate": 3.037653085904004e-05,
      "loss": 0.2972,
      "step": 178500
    },
    {
      "epoch": 3.9357094171192366,
      "grad_norm": 11.409941673278809,
      "learning_rate": 3.032156285042106e-05,
      "loss": 0.2925,
      "step": 179000
    },
    {
      "epoch": 3.946703018843033,
      "grad_norm": 9.664896011352539,
      "learning_rate": 3.026659484180207e-05,
      "loss": 0.2951,
      "step": 179500
    },
    {
      "epoch": 3.9576966205668302,
      "grad_norm": 12.644094467163086,
      "learning_rate": 3.0211626833183086e-05,
      "loss": 0.274,
      "step": 180000
    },
    {
      "epoch": 3.968690222290627,
      "grad_norm": 12.70506763458252,
      "learning_rate": 3.0156658824564105e-05,
      "loss": 0.294,
      "step": 180500
    },
    {
      "epoch": 3.9796838240144234,
      "grad_norm": 3.6369152069091797,
      "learning_rate": 3.0101690815945123e-05,
      "loss": 0.2806,
      "step": 181000
    },
    {
      "epoch": 3.9906774257382205,
      "grad_norm": 14.573278427124023,
      "learning_rate": 3.0046722807326138e-05,
      "loss": 0.2932,
      "step": 181500
    },
    {
      "epoch": 4.0016710274620175,
      "grad_norm": 2.844874858856201,
      "learning_rate": 2.9991754798707157e-05,
      "loss": 0.2927,
      "step": 182000
    },
    {
      "epoch": 4.012664629185814,
      "grad_norm": 16.63310432434082,
      "learning_rate": 2.9936786790088172e-05,
      "loss": 0.2808,
      "step": 182500
    },
    {
      "epoch": 4.023658230909611,
      "grad_norm": 0.4773117005825043,
      "learning_rate": 2.9881818781469184e-05,
      "loss": 0.2974,
      "step": 183000
    },
    {
      "epoch": 4.034651832633408,
      "grad_norm": 6.818424224853516,
      "learning_rate": 2.9826850772850202e-05,
      "loss": 0.2842,
      "step": 183500
    },
    {
      "epoch": 4.045645434357204,
      "grad_norm": 1.1497576236724854,
      "learning_rate": 2.9771882764231217e-05,
      "loss": 0.2788,
      "step": 184000
    },
    {
      "epoch": 4.056639036081001,
      "grad_norm": 1.0795512199401855,
      "learning_rate": 2.9716914755612236e-05,
      "loss": 0.2712,
      "step": 184500
    },
    {
      "epoch": 4.067632637804798,
      "grad_norm": 8.198368072509766,
      "learning_rate": 2.966194674699325e-05,
      "loss": 0.2855,
      "step": 185000
    },
    {
      "epoch": 4.078626239528594,
      "grad_norm": 1.790134310722351,
      "learning_rate": 2.960697873837427e-05,
      "loss": 0.2731,
      "step": 185500
    },
    {
      "epoch": 4.089619841252391,
      "grad_norm": 1.1248100996017456,
      "learning_rate": 2.9552010729755288e-05,
      "loss": 0.2914,
      "step": 186000
    },
    {
      "epoch": 4.100613442976188,
      "grad_norm": 2.7450244426727295,
      "learning_rate": 2.9497042721136296e-05,
      "loss": 0.2926,
      "step": 186500
    },
    {
      "epoch": 4.111607044699984,
      "grad_norm": 9.180065155029297,
      "learning_rate": 2.9442074712517315e-05,
      "loss": 0.2775,
      "step": 187000
    },
    {
      "epoch": 4.122600646423781,
      "grad_norm": 11.459710121154785,
      "learning_rate": 2.9387106703898333e-05,
      "loss": 0.2791,
      "step": 187500
    },
    {
      "epoch": 4.1335942481475785,
      "grad_norm": 3.298827648162842,
      "learning_rate": 2.9332138695279348e-05,
      "loss": 0.2681,
      "step": 188000
    },
    {
      "epoch": 4.144587849871375,
      "grad_norm": 0.4720885455608368,
      "learning_rate": 2.9277170686660367e-05,
      "loss": 0.2817,
      "step": 188500
    },
    {
      "epoch": 4.155581451595172,
      "grad_norm": 1.1827914714813232,
      "learning_rate": 2.9222202678041382e-05,
      "loss": 0.2756,
      "step": 189000
    },
    {
      "epoch": 4.166575053318969,
      "grad_norm": 0.862234890460968,
      "learning_rate": 2.91672346694224e-05,
      "loss": 0.2897,
      "step": 189500
    },
    {
      "epoch": 4.177568655042765,
      "grad_norm": 9.329187393188477,
      "learning_rate": 2.9112266660803412e-05,
      "loss": 0.279,
      "step": 190000
    },
    {
      "epoch": 4.188562256766562,
      "grad_norm": 2.0248208045959473,
      "learning_rate": 2.9057298652184427e-05,
      "loss": 0.2694,
      "step": 190500
    },
    {
      "epoch": 4.199555858490359,
      "grad_norm": 7.963052749633789,
      "learning_rate": 2.9002330643565446e-05,
      "loss": 0.2685,
      "step": 191000
    },
    {
      "epoch": 4.210549460214155,
      "grad_norm": 3.6001532077789307,
      "learning_rate": 2.894736263494646e-05,
      "loss": 0.2787,
      "step": 191500
    },
    {
      "epoch": 4.221543061937952,
      "grad_norm": 17.655574798583984,
      "learning_rate": 2.889239462632748e-05,
      "loss": 0.2895,
      "step": 192000
    },
    {
      "epoch": 4.232536663661749,
      "grad_norm": 7.449512958526611,
      "learning_rate": 2.8837426617708498e-05,
      "loss": 0.2815,
      "step": 192500
    },
    {
      "epoch": 4.243530265385545,
      "grad_norm": 14.792319297790527,
      "learning_rate": 2.8782458609089513e-05,
      "loss": 0.3083,
      "step": 193000
    },
    {
      "epoch": 4.254523867109342,
      "grad_norm": 1.495776891708374,
      "learning_rate": 2.872749060047053e-05,
      "loss": 0.2844,
      "step": 193500
    },
    {
      "epoch": 4.265517468833139,
      "grad_norm": 7.207265853881836,
      "learning_rate": 2.8672522591851543e-05,
      "loss": 0.2754,
      "step": 194000
    },
    {
      "epoch": 4.276511070556936,
      "grad_norm": 7.008718967437744,
      "learning_rate": 2.861755458323256e-05,
      "loss": 0.2964,
      "step": 194500
    },
    {
      "epoch": 4.287504672280733,
      "grad_norm": 16.336179733276367,
      "learning_rate": 2.8562586574613577e-05,
      "loss": 0.2665,
      "step": 195000
    },
    {
      "epoch": 4.29849827400453,
      "grad_norm": 16.858001708984375,
      "learning_rate": 2.8507618565994592e-05,
      "loss": 0.2812,
      "step": 195500
    },
    {
      "epoch": 4.309491875728326,
      "grad_norm": 1.9535256624221802,
      "learning_rate": 2.845265055737561e-05,
      "loss": 0.2955,
      "step": 196000
    },
    {
      "epoch": 4.320485477452123,
      "grad_norm": 16.030567169189453,
      "learning_rate": 2.8397682548756626e-05,
      "loss": 0.2735,
      "step": 196500
    },
    {
      "epoch": 4.33147907917592,
      "grad_norm": 1.8680860996246338,
      "learning_rate": 2.8342714540137644e-05,
      "loss": 0.2675,
      "step": 197000
    },
    {
      "epoch": 4.342472680899716,
      "grad_norm": 6.480403423309326,
      "learning_rate": 2.8287746531518656e-05,
      "loss": 0.295,
      "step": 197500
    },
    {
      "epoch": 4.353466282623513,
      "grad_norm": 0.2664981186389923,
      "learning_rate": 2.823277852289967e-05,
      "loss": 0.3024,
      "step": 198000
    },
    {
      "epoch": 4.36445988434731,
      "grad_norm": 1.231545329093933,
      "learning_rate": 2.817781051428069e-05,
      "loss": 0.2678,
      "step": 198500
    },
    {
      "epoch": 4.375453486071106,
      "grad_norm": 5.9845290184021,
      "learning_rate": 2.8122842505661705e-05,
      "loss": 0.2761,
      "step": 199000
    },
    {
      "epoch": 4.386447087794903,
      "grad_norm": 1.0023155212402344,
      "learning_rate": 2.8067874497042723e-05,
      "loss": 0.2733,
      "step": 199500
    },
    {
      "epoch": 4.3974406895187,
      "grad_norm": 16.75244903564453,
      "learning_rate": 2.801290648842374e-05,
      "loss": 0.2736,
      "step": 200000
    },
    {
      "epoch": 4.4084342912424965,
      "grad_norm": 6.695202350616455,
      "learning_rate": 2.7957938479804757e-05,
      "loss": 0.273,
      "step": 200500
    },
    {
      "epoch": 4.419427892966294,
      "grad_norm": 13.116875648498535,
      "learning_rate": 2.790297047118577e-05,
      "loss": 0.2881,
      "step": 201000
    },
    {
      "epoch": 4.430421494690091,
      "grad_norm": 17.509389877319336,
      "learning_rate": 2.7848002462566787e-05,
      "loss": 0.2718,
      "step": 201500
    },
    {
      "epoch": 4.441415096413887,
      "grad_norm": 8.14511775970459,
      "learning_rate": 2.7793034453947802e-05,
      "loss": 0.2702,
      "step": 202000
    },
    {
      "epoch": 4.452408698137684,
      "grad_norm": 16.810184478759766,
      "learning_rate": 2.773806644532882e-05,
      "loss": 0.2817,
      "step": 202500
    },
    {
      "epoch": 4.463402299861481,
      "grad_norm": 5.89200496673584,
      "learning_rate": 2.7683098436709836e-05,
      "loss": 0.286,
      "step": 203000
    },
    {
      "epoch": 4.474395901585277,
      "grad_norm": 1.2484943866729736,
      "learning_rate": 2.7628130428090854e-05,
      "loss": 0.2829,
      "step": 203500
    },
    {
      "epoch": 4.485389503309074,
      "grad_norm": 13.952207565307617,
      "learning_rate": 2.757316241947187e-05,
      "loss": 0.2883,
      "step": 204000
    },
    {
      "epoch": 4.496383105032871,
      "grad_norm": 17.755922317504883,
      "learning_rate": 2.751819441085288e-05,
      "loss": 0.292,
      "step": 204500
    },
    {
      "epoch": 4.507376706756667,
      "grad_norm": 5.866535663604736,
      "learning_rate": 2.74632264022339e-05,
      "loss": 0.284,
      "step": 205000
    },
    {
      "epoch": 4.518370308480464,
      "grad_norm": 7.257752418518066,
      "learning_rate": 2.7408258393614915e-05,
      "loss": 0.2765,
      "step": 205500
    },
    {
      "epoch": 4.529363910204261,
      "grad_norm": 10.193904876708984,
      "learning_rate": 2.7353290384995933e-05,
      "loss": 0.2745,
      "step": 206000
    },
    {
      "epoch": 4.5403575119280575,
      "grad_norm": 0.38974231481552124,
      "learning_rate": 2.729832237637695e-05,
      "loss": 0.2896,
      "step": 206500
    },
    {
      "epoch": 4.5513511136518545,
      "grad_norm": 17.746007919311523,
      "learning_rate": 2.7243354367757967e-05,
      "loss": 0.2892,
      "step": 207000
    },
    {
      "epoch": 4.562344715375652,
      "grad_norm": 0.9025241136550903,
      "learning_rate": 2.7188386359138985e-05,
      "loss": 0.275,
      "step": 207500
    },
    {
      "epoch": 4.573338317099449,
      "grad_norm": 2.2165682315826416,
      "learning_rate": 2.713341835052e-05,
      "loss": 0.2752,
      "step": 208000
    },
    {
      "epoch": 4.584331918823245,
      "grad_norm": 1.3191064596176147,
      "learning_rate": 2.7078450341901012e-05,
      "loss": 0.2959,
      "step": 208500
    },
    {
      "epoch": 4.595325520547042,
      "grad_norm": 1.6735217571258545,
      "learning_rate": 2.702348233328203e-05,
      "loss": 0.2769,
      "step": 209000
    },
    {
      "epoch": 4.606319122270838,
      "grad_norm": 18.78565788269043,
      "learning_rate": 2.6968514324663046e-05,
      "loss": 0.2864,
      "step": 209500
    },
    {
      "epoch": 4.617312723994635,
      "grad_norm": 2.4785966873168945,
      "learning_rate": 2.6913546316044064e-05,
      "loss": 0.306,
      "step": 210000
    },
    {
      "epoch": 4.628306325718432,
      "grad_norm": 8.956345558166504,
      "learning_rate": 2.685857830742508e-05,
      "loss": 0.2859,
      "step": 210500
    },
    {
      "epoch": 4.639299927442229,
      "grad_norm": 15.839003562927246,
      "learning_rate": 2.6803610298806098e-05,
      "loss": 0.2689,
      "step": 211000
    },
    {
      "epoch": 4.650293529166025,
      "grad_norm": 0.5148014426231384,
      "learning_rate": 2.6748642290187116e-05,
      "loss": 0.2888,
      "step": 211500
    },
    {
      "epoch": 4.661287130889822,
      "grad_norm": 8.29941463470459,
      "learning_rate": 2.6693674281568125e-05,
      "loss": 0.2867,
      "step": 212000
    },
    {
      "epoch": 4.672280732613618,
      "grad_norm": 0.41244611144065857,
      "learning_rate": 2.6638706272949143e-05,
      "loss": 0.2894,
      "step": 212500
    },
    {
      "epoch": 4.6832743343374155,
      "grad_norm": 9.750991821289062,
      "learning_rate": 2.658373826433016e-05,
      "loss": 0.2654,
      "step": 213000
    },
    {
      "epoch": 4.6942679360612125,
      "grad_norm": 15.462173461914062,
      "learning_rate": 2.6528770255711177e-05,
      "loss": 0.2778,
      "step": 213500
    },
    {
      "epoch": 4.7052615377850096,
      "grad_norm": 10.779348373413086,
      "learning_rate": 2.6473802247092195e-05,
      "loss": 0.2848,
      "step": 214000
    },
    {
      "epoch": 4.716255139508806,
      "grad_norm": 0.12448650598526001,
      "learning_rate": 2.641883423847321e-05,
      "loss": 0.2954,
      "step": 214500
    },
    {
      "epoch": 4.727248741232603,
      "grad_norm": 15.326800346374512,
      "learning_rate": 2.636386622985423e-05,
      "loss": 0.2819,
      "step": 215000
    },
    {
      "epoch": 4.738242342956399,
      "grad_norm": 9.866533279418945,
      "learning_rate": 2.630889822123524e-05,
      "loss": 0.2885,
      "step": 215500
    },
    {
      "epoch": 4.749235944680196,
      "grad_norm": 7.071689605712891,
      "learning_rate": 2.6253930212616256e-05,
      "loss": 0.2875,
      "step": 216000
    },
    {
      "epoch": 4.760229546403993,
      "grad_norm": 7.5760579109191895,
      "learning_rate": 2.6198962203997274e-05,
      "loss": 0.2806,
      "step": 216500
    },
    {
      "epoch": 4.77122314812779,
      "grad_norm": 12.175812721252441,
      "learning_rate": 2.614399419537829e-05,
      "loss": 0.2898,
      "step": 217000
    },
    {
      "epoch": 4.782216749851586,
      "grad_norm": 12.818252563476562,
      "learning_rate": 2.6089026186759308e-05,
      "loss": 0.2925,
      "step": 217500
    },
    {
      "epoch": 4.793210351575383,
      "grad_norm": 6.3232269287109375,
      "learning_rate": 2.6034058178140326e-05,
      "loss": 0.2782,
      "step": 218000
    },
    {
      "epoch": 4.80420395329918,
      "grad_norm": 0.7374396920204163,
      "learning_rate": 2.597909016952134e-05,
      "loss": 0.2847,
      "step": 218500
    },
    {
      "epoch": 4.815197555022976,
      "grad_norm": 15.031373977661133,
      "learning_rate": 2.592412216090236e-05,
      "loss": 0.2794,
      "step": 219000
    },
    {
      "epoch": 4.8261911567467735,
      "grad_norm": 7.169040679931641,
      "learning_rate": 2.586915415228337e-05,
      "loss": 0.2861,
      "step": 219500
    },
    {
      "epoch": 4.8371847584705705,
      "grad_norm": 5.074714660644531,
      "learning_rate": 2.5814186143664387e-05,
      "loss": 0.2718,
      "step": 220000
    },
    {
      "epoch": 4.848178360194367,
      "grad_norm": 7.753561973571777,
      "learning_rate": 2.5759218135045405e-05,
      "loss": 0.2687,
      "step": 220500
    },
    {
      "epoch": 4.859171961918164,
      "grad_norm": 9.461535453796387,
      "learning_rate": 2.570425012642642e-05,
      "loss": 0.2773,
      "step": 221000
    },
    {
      "epoch": 4.870165563641961,
      "grad_norm": 18.876140594482422,
      "learning_rate": 2.564928211780744e-05,
      "loss": 0.2949,
      "step": 221500
    },
    {
      "epoch": 4.881159165365757,
      "grad_norm": 7.816246509552002,
      "learning_rate": 2.5594314109188454e-05,
      "loss": 0.2657,
      "step": 222000
    },
    {
      "epoch": 4.892152767089554,
      "grad_norm": 9.077609062194824,
      "learning_rate": 2.5539346100569472e-05,
      "loss": 0.2927,
      "step": 222500
    },
    {
      "epoch": 4.903146368813351,
      "grad_norm": 1.4059295654296875,
      "learning_rate": 2.5484378091950484e-05,
      "loss": 0.2882,
      "step": 223000
    },
    {
      "epoch": 4.914139970537147,
      "grad_norm": 9.421820640563965,
      "learning_rate": 2.54294100833315e-05,
      "loss": 0.2835,
      "step": 223500
    },
    {
      "epoch": 4.925133572260944,
      "grad_norm": 4.161566734313965,
      "learning_rate": 2.5374442074712518e-05,
      "loss": 0.2734,
      "step": 224000
    },
    {
      "epoch": 4.936127173984741,
      "grad_norm": 18.405466079711914,
      "learning_rate": 2.5319474066093536e-05,
      "loss": 0.2825,
      "step": 224500
    },
    {
      "epoch": 4.947120775708537,
      "grad_norm": 0.4321858286857605,
      "learning_rate": 2.526450605747455e-05,
      "loss": 0.2734,
      "step": 225000
    },
    {
      "epoch": 4.958114377432334,
      "grad_norm": 11.497639656066895,
      "learning_rate": 2.520953804885557e-05,
      "loss": 0.3062,
      "step": 225500
    },
    {
      "epoch": 4.9691079791561314,
      "grad_norm": 1.4827295541763306,
      "learning_rate": 2.5154570040236585e-05,
      "loss": 0.2883,
      "step": 226000
    },
    {
      "epoch": 4.980101580879928,
      "grad_norm": 4.031532287597656,
      "learning_rate": 2.5099602031617597e-05,
      "loss": 0.2892,
      "step": 226500
    },
    {
      "epoch": 4.991095182603725,
      "grad_norm": 10.2671480178833,
      "learning_rate": 2.5044634022998615e-05,
      "loss": 0.2801,
      "step": 227000
    },
    {
      "epoch": 5.002088784327522,
      "grad_norm": 6.386577129364014,
      "learning_rate": 2.498966601437963e-05,
      "loss": 0.2737,
      "step": 227500
    },
    {
      "epoch": 5.013082386051318,
      "grad_norm": 4.403781890869141,
      "learning_rate": 2.493469800576065e-05,
      "loss": 0.2777,
      "step": 228000
    },
    {
      "epoch": 5.024075987775115,
      "grad_norm": 11.37314510345459,
      "learning_rate": 2.4879729997141664e-05,
      "loss": 0.2829,
      "step": 228500
    },
    {
      "epoch": 5.035069589498912,
      "grad_norm": 2.347327947616577,
      "learning_rate": 2.4824761988522683e-05,
      "loss": 0.2766,
      "step": 229000
    },
    {
      "epoch": 5.046063191222708,
      "grad_norm": 16.481290817260742,
      "learning_rate": 2.4769793979903698e-05,
      "loss": 0.2634,
      "step": 229500
    },
    {
      "epoch": 5.057056792946505,
      "grad_norm": 19.77266502380371,
      "learning_rate": 2.4714825971284713e-05,
      "loss": 0.2876,
      "step": 230000
    },
    {
      "epoch": 5.068050394670302,
      "grad_norm": 12.039043426513672,
      "learning_rate": 2.465985796266573e-05,
      "loss": 0.2916,
      "step": 230500
    },
    {
      "epoch": 5.079043996394098,
      "grad_norm": 1.194934606552124,
      "learning_rate": 2.4604889954046746e-05,
      "loss": 0.2706,
      "step": 231000
    },
    {
      "epoch": 5.090037598117895,
      "grad_norm": 0.7327463626861572,
      "learning_rate": 2.454992194542776e-05,
      "loss": 0.2646,
      "step": 231500
    },
    {
      "epoch": 5.101031199841692,
      "grad_norm": 1.0546201467514038,
      "learning_rate": 2.449495393680878e-05,
      "loss": 0.2643,
      "step": 232000
    },
    {
      "epoch": 5.1120248015654886,
      "grad_norm": 12.288138389587402,
      "learning_rate": 2.4439985928189795e-05,
      "loss": 0.2795,
      "step": 232500
    },
    {
      "epoch": 5.123018403289286,
      "grad_norm": 11.529952049255371,
      "learning_rate": 2.438501791957081e-05,
      "loss": 0.2746,
      "step": 233000
    },
    {
      "epoch": 5.134012005013083,
      "grad_norm": 0.21018722653388977,
      "learning_rate": 2.433004991095183e-05,
      "loss": 0.2766,
      "step": 233500
    },
    {
      "epoch": 5.145005606736879,
      "grad_norm": 0.5320094227790833,
      "learning_rate": 2.4275081902332844e-05,
      "loss": 0.2747,
      "step": 234000
    },
    {
      "epoch": 5.155999208460676,
      "grad_norm": 6.288522243499756,
      "learning_rate": 2.4220113893713862e-05,
      "loss": 0.2695,
      "step": 234500
    },
    {
      "epoch": 5.166992810184473,
      "grad_norm": 0.5201558470726013,
      "learning_rate": 2.4165145885094874e-05,
      "loss": 0.2812,
      "step": 235000
    },
    {
      "epoch": 5.177986411908269,
      "grad_norm": 5.76743745803833,
      "learning_rate": 2.4110177876475893e-05,
      "loss": 0.2711,
      "step": 235500
    },
    {
      "epoch": 5.188980013632066,
      "grad_norm": 0.995596706867218,
      "learning_rate": 2.4055209867856908e-05,
      "loss": 0.2884,
      "step": 236000
    },
    {
      "epoch": 5.199973615355863,
      "grad_norm": 10.275565147399902,
      "learning_rate": 2.4000241859237923e-05,
      "loss": 0.2806,
      "step": 236500
    },
    {
      "epoch": 5.210967217079659,
      "grad_norm": 7.608597755432129,
      "learning_rate": 2.394527385061894e-05,
      "loss": 0.2733,
      "step": 237000
    },
    {
      "epoch": 5.221960818803456,
      "grad_norm": 7.195961952209473,
      "learning_rate": 2.3890305841999956e-05,
      "loss": 0.2656,
      "step": 237500
    },
    {
      "epoch": 5.232954420527253,
      "grad_norm": 4.034346580505371,
      "learning_rate": 2.3835337833380975e-05,
      "loss": 0.2713,
      "step": 238000
    },
    {
      "epoch": 5.2439480222510495,
      "grad_norm": 12.522611618041992,
      "learning_rate": 2.378036982476199e-05,
      "loss": 0.2761,
      "step": 238500
    },
    {
      "epoch": 5.2549416239748465,
      "grad_norm": 5.470338344573975,
      "learning_rate": 2.3725401816143005e-05,
      "loss": 0.2712,
      "step": 239000
    },
    {
      "epoch": 5.265935225698644,
      "grad_norm": 14.638073921203613,
      "learning_rate": 2.3670433807524024e-05,
      "loss": 0.2704,
      "step": 239500
    },
    {
      "epoch": 5.27692882742244,
      "grad_norm": 18.795438766479492,
      "learning_rate": 2.361546579890504e-05,
      "loss": 0.2571,
      "step": 240000
    },
    {
      "epoch": 5.287922429146237,
      "grad_norm": 5.827826976776123,
      "learning_rate": 2.3560497790286054e-05,
      "loss": 0.291,
      "step": 240500
    },
    {
      "epoch": 5.298916030870034,
      "grad_norm": 1.4563448429107666,
      "learning_rate": 2.3505529781667072e-05,
      "loss": 0.2705,
      "step": 241000
    },
    {
      "epoch": 5.30990963259383,
      "grad_norm": 15.683819770812988,
      "learning_rate": 2.3450561773048087e-05,
      "loss": 0.2888,
      "step": 241500
    },
    {
      "epoch": 5.320903234317627,
      "grad_norm": 16.58009910583496,
      "learning_rate": 2.3395593764429103e-05,
      "loss": 0.2901,
      "step": 242000
    },
    {
      "epoch": 5.331896836041424,
      "grad_norm": 0.5832206010818481,
      "learning_rate": 2.3340625755810118e-05,
      "loss": 0.2721,
      "step": 242500
    },
    {
      "epoch": 5.34289043776522,
      "grad_norm": 2.6090269088745117,
      "learning_rate": 2.3285657747191136e-05,
      "loss": 0.2655,
      "step": 243000
    },
    {
      "epoch": 5.353884039489017,
      "grad_norm": 11.195696830749512,
      "learning_rate": 2.3230689738572155e-05,
      "loss": 0.2847,
      "step": 243500
    },
    {
      "epoch": 5.364877641212814,
      "grad_norm": 5.514035224914551,
      "learning_rate": 2.3175721729953166e-05,
      "loss": 0.2707,
      "step": 244000
    },
    {
      "epoch": 5.375871242936611,
      "grad_norm": 5.166293621063232,
      "learning_rate": 2.3120753721334185e-05,
      "loss": 0.2854,
      "step": 244500
    },
    {
      "epoch": 5.3868648446604075,
      "grad_norm": 12.50465202331543,
      "learning_rate": 2.30657857127152e-05,
      "loss": 0.2801,
      "step": 245000
    },
    {
      "epoch": 5.3978584463842045,
      "grad_norm": 1.2575602531433105,
      "learning_rate": 2.3010817704096215e-05,
      "loss": 0.2781,
      "step": 245500
    },
    {
      "epoch": 5.408852048108001,
      "grad_norm": 0.8332090973854065,
      "learning_rate": 2.2955849695477234e-05,
      "loss": 0.2715,
      "step": 246000
    },
    {
      "epoch": 5.419845649831798,
      "grad_norm": 10.977988243103027,
      "learning_rate": 2.290088168685825e-05,
      "loss": 0.271,
      "step": 246500
    },
    {
      "epoch": 5.430839251555595,
      "grad_norm": 7.93900203704834,
      "learning_rate": 2.2845913678239267e-05,
      "loss": 0.29,
      "step": 247000
    },
    {
      "epoch": 5.441832853279392,
      "grad_norm": 9.405533790588379,
      "learning_rate": 2.2790945669620282e-05,
      "loss": 0.2661,
      "step": 247500
    },
    {
      "epoch": 5.452826455003188,
      "grad_norm": 15.136964797973633,
      "learning_rate": 2.2735977661001297e-05,
      "loss": 0.3002,
      "step": 248000
    },
    {
      "epoch": 5.463820056726985,
      "grad_norm": 3.885523557662964,
      "learning_rate": 2.2681009652382316e-05,
      "loss": 0.2689,
      "step": 248500
    },
    {
      "epoch": 5.474813658450782,
      "grad_norm": 12.38040542602539,
      "learning_rate": 2.262604164376333e-05,
      "loss": 0.267,
      "step": 249000
    },
    {
      "epoch": 5.485807260174578,
      "grad_norm": 8.701943397521973,
      "learning_rate": 2.2571073635144346e-05,
      "loss": 0.2741,
      "step": 249500
    },
    {
      "epoch": 5.496800861898375,
      "grad_norm": 1.2600756883621216,
      "learning_rate": 2.2516105626525365e-05,
      "loss": 0.2699,
      "step": 250000
    },
    {
      "epoch": 5.507794463622172,
      "grad_norm": 17.268054962158203,
      "learning_rate": 2.246113761790638e-05,
      "loss": 0.2652,
      "step": 250500
    },
    {
      "epoch": 5.518788065345968,
      "grad_norm": 2.1233537197113037,
      "learning_rate": 2.2406169609287395e-05,
      "loss": 0.2714,
      "step": 251000
    },
    {
      "epoch": 5.5297816670697655,
      "grad_norm": 7.700139045715332,
      "learning_rate": 2.235120160066841e-05,
      "loss": 0.2702,
      "step": 251500
    },
    {
      "epoch": 5.5407752687935625,
      "grad_norm": 7.130921840667725,
      "learning_rate": 2.229623359204943e-05,
      "loss": 0.285,
      "step": 252000
    },
    {
      "epoch": 5.551768870517359,
      "grad_norm": 3.369375228881836,
      "learning_rate": 2.2241265583430447e-05,
      "loss": 0.2657,
      "step": 252500
    },
    {
      "epoch": 5.562762472241156,
      "grad_norm": 13.161419868469238,
      "learning_rate": 2.218629757481146e-05,
      "loss": 0.2637,
      "step": 253000
    },
    {
      "epoch": 5.573756073964953,
      "grad_norm": 14.916949272155762,
      "learning_rate": 2.2131329566192477e-05,
      "loss": 0.2641,
      "step": 253500
    },
    {
      "epoch": 5.584749675688749,
      "grad_norm": 13.34550666809082,
      "learning_rate": 2.2076361557573492e-05,
      "loss": 0.278,
      "step": 254000
    },
    {
      "epoch": 5.595743277412546,
      "grad_norm": 12.643482208251953,
      "learning_rate": 2.202139354895451e-05,
      "loss": 0.285,
      "step": 254500
    },
    {
      "epoch": 5.606736879136343,
      "grad_norm": 12.250041007995605,
      "learning_rate": 2.1966425540335526e-05,
      "loss": 0.2795,
      "step": 255000
    },
    {
      "epoch": 5.617730480860139,
      "grad_norm": 16.9895076751709,
      "learning_rate": 2.191145753171654e-05,
      "loss": 0.2807,
      "step": 255500
    },
    {
      "epoch": 5.628724082583936,
      "grad_norm": 17.08063316345215,
      "learning_rate": 2.185648952309756e-05,
      "loss": 0.2728,
      "step": 256000
    },
    {
      "epoch": 5.639717684307733,
      "grad_norm": 14.572457313537598,
      "learning_rate": 2.1801521514478575e-05,
      "loss": 0.2821,
      "step": 256500
    },
    {
      "epoch": 5.650711286031529,
      "grad_norm": 1.4316556453704834,
      "learning_rate": 2.174655350585959e-05,
      "loss": 0.2754,
      "step": 257000
    },
    {
      "epoch": 5.661704887755326,
      "grad_norm": 1.8499094247817993,
      "learning_rate": 2.169158549724061e-05,
      "loss": 0.279,
      "step": 257500
    },
    {
      "epoch": 5.6726984894791235,
      "grad_norm": 0.188524529337883,
      "learning_rate": 2.1636617488621623e-05,
      "loss": 0.2903,
      "step": 258000
    },
    {
      "epoch": 5.68369209120292,
      "grad_norm": 15.659905433654785,
      "learning_rate": 2.158164948000264e-05,
      "loss": 0.2845,
      "step": 258500
    },
    {
      "epoch": 5.694685692926717,
      "grad_norm": 5.83554744720459,
      "learning_rate": 2.1526681471383657e-05,
      "loss": 0.2729,
      "step": 259000
    },
    {
      "epoch": 5.705679294650514,
      "grad_norm": 15.373764038085938,
      "learning_rate": 2.1471713462764672e-05,
      "loss": 0.2791,
      "step": 259500
    },
    {
      "epoch": 5.71667289637431,
      "grad_norm": 7.071280002593994,
      "learning_rate": 2.1416745454145687e-05,
      "loss": 0.2632,
      "step": 260000
    },
    {
      "epoch": 5.727666498098107,
      "grad_norm": 11.893906593322754,
      "learning_rate": 2.1361777445526702e-05,
      "loss": 0.258,
      "step": 260500
    },
    {
      "epoch": 5.738660099821904,
      "grad_norm": 14.980462074279785,
      "learning_rate": 2.130680943690772e-05,
      "loss": 0.2709,
      "step": 261000
    },
    {
      "epoch": 5.7496537015457,
      "grad_norm": 8.779207229614258,
      "learning_rate": 2.125184142828874e-05,
      "loss": 0.2951,
      "step": 261500
    },
    {
      "epoch": 5.760647303269497,
      "grad_norm": 13.5264310836792,
      "learning_rate": 2.119687341966975e-05,
      "loss": 0.2481,
      "step": 262000
    },
    {
      "epoch": 5.771640904993294,
      "grad_norm": 0.415309876203537,
      "learning_rate": 2.114190541105077e-05,
      "loss": 0.2852,
      "step": 262500
    },
    {
      "epoch": 5.78263450671709,
      "grad_norm": 0.6840352416038513,
      "learning_rate": 2.1086937402431785e-05,
      "loss": 0.2825,
      "step": 263000
    },
    {
      "epoch": 5.793628108440887,
      "grad_norm": 0.49565112590789795,
      "learning_rate": 2.1031969393812803e-05,
      "loss": 0.2624,
      "step": 263500
    },
    {
      "epoch": 5.804621710164684,
      "grad_norm": 8.343179702758789,
      "learning_rate": 2.097700138519382e-05,
      "loss": 0.2966,
      "step": 264000
    },
    {
      "epoch": 5.815615311888481,
      "grad_norm": 12.784666061401367,
      "learning_rate": 2.0922033376574834e-05,
      "loss": 0.2901,
      "step": 264500
    },
    {
      "epoch": 5.826608913612278,
      "grad_norm": 1.1970640420913696,
      "learning_rate": 2.0867065367955852e-05,
      "loss": 0.2574,
      "step": 265000
    },
    {
      "epoch": 5.837602515336075,
      "grad_norm": 0.3328879773616791,
      "learning_rate": 2.0812097359336867e-05,
      "loss": 0.2658,
      "step": 265500
    },
    {
      "epoch": 5.848596117059871,
      "grad_norm": 17.217979431152344,
      "learning_rate": 2.0757129350717882e-05,
      "loss": 0.2741,
      "step": 266000
    },
    {
      "epoch": 5.859589718783668,
      "grad_norm": 12.983806610107422,
      "learning_rate": 2.07021613420989e-05,
      "loss": 0.2883,
      "step": 266500
    },
    {
      "epoch": 5.870583320507465,
      "grad_norm": 8.902253150939941,
      "learning_rate": 2.0647193333479916e-05,
      "loss": 0.2737,
      "step": 267000
    },
    {
      "epoch": 5.881576922231261,
      "grad_norm": 16.610761642456055,
      "learning_rate": 2.059222532486093e-05,
      "loss": 0.2579,
      "step": 267500
    },
    {
      "epoch": 5.892570523955058,
      "grad_norm": 0.9576968550682068,
      "learning_rate": 2.053725731624195e-05,
      "loss": 0.2887,
      "step": 268000
    },
    {
      "epoch": 5.903564125678855,
      "grad_norm": 5.042171001434326,
      "learning_rate": 2.0482289307622965e-05,
      "loss": 0.2608,
      "step": 268500
    },
    {
      "epoch": 5.914557727402651,
      "grad_norm": 6.798401832580566,
      "learning_rate": 2.0427321299003983e-05,
      "loss": 0.2658,
      "step": 269000
    },
    {
      "epoch": 5.925551329126448,
      "grad_norm": 1.8430145978927612,
      "learning_rate": 2.0372353290384995e-05,
      "loss": 0.2846,
      "step": 269500
    },
    {
      "epoch": 5.936544930850245,
      "grad_norm": 7.747425556182861,
      "learning_rate": 2.0317385281766013e-05,
      "loss": 0.2725,
      "step": 270000
    },
    {
      "epoch": 5.9475385325740415,
      "grad_norm": 1.3697054386138916,
      "learning_rate": 2.0262417273147032e-05,
      "loss": 0.2859,
      "step": 270500
    },
    {
      "epoch": 5.958532134297839,
      "grad_norm": 23.0757999420166,
      "learning_rate": 2.0207449264528044e-05,
      "loss": 0.2758,
      "step": 271000
    },
    {
      "epoch": 5.969525736021636,
      "grad_norm": 12.323798179626465,
      "learning_rate": 2.0152481255909062e-05,
      "loss": 0.2775,
      "step": 271500
    },
    {
      "epoch": 5.980519337745433,
      "grad_norm": 14.46690559387207,
      "learning_rate": 2.0097513247290077e-05,
      "loss": 0.2603,
      "step": 272000
    },
    {
      "epoch": 5.991512939469229,
      "grad_norm": 5.847137928009033,
      "learning_rate": 2.0042545238671096e-05,
      "loss": 0.2894,
      "step": 272500
    },
    {
      "epoch": 6.002506541193026,
      "grad_norm": 20.03915023803711,
      "learning_rate": 1.998757723005211e-05,
      "loss": 0.263,
      "step": 273000
    },
    {
      "epoch": 6.013500142916822,
      "grad_norm": 4.412625789642334,
      "learning_rate": 1.9932609221433126e-05,
      "loss": 0.2645,
      "step": 273500
    },
    {
      "epoch": 6.024493744640619,
      "grad_norm": 2.8763904571533203,
      "learning_rate": 1.9877641212814144e-05,
      "loss": 0.2749,
      "step": 274000
    },
    {
      "epoch": 6.035487346364416,
      "grad_norm": 11.175874710083008,
      "learning_rate": 1.982267320419516e-05,
      "loss": 0.2628,
      "step": 274500
    },
    {
      "epoch": 6.046480948088212,
      "grad_norm": 7.149959087371826,
      "learning_rate": 1.9767705195576175e-05,
      "loss": 0.2636,
      "step": 275000
    },
    {
      "epoch": 6.057474549812009,
      "grad_norm": 9.249302864074707,
      "learning_rate": 1.9712737186957193e-05,
      "loss": 0.2529,
      "step": 275500
    },
    {
      "epoch": 6.068468151535806,
      "grad_norm": 5.462003231048584,
      "learning_rate": 1.9657769178338208e-05,
      "loss": 0.2601,
      "step": 276000
    },
    {
      "epoch": 6.0794617532596025,
      "grad_norm": 0.2802177965641022,
      "learning_rate": 1.9602801169719223e-05,
      "loss": 0.2488,
      "step": 276500
    },
    {
      "epoch": 6.0904553549833995,
      "grad_norm": 1.5475291013717651,
      "learning_rate": 1.9547833161100242e-05,
      "loss": 0.2941,
      "step": 277000
    },
    {
      "epoch": 6.101448956707197,
      "grad_norm": 12.721526145935059,
      "learning_rate": 1.9492865152481257e-05,
      "loss": 0.2783,
      "step": 277500
    },
    {
      "epoch": 6.112442558430994,
      "grad_norm": 0.7614845037460327,
      "learning_rate": 1.9437897143862275e-05,
      "loss": 0.259,
      "step": 278000
    },
    {
      "epoch": 6.12343616015479,
      "grad_norm": 10.284335136413574,
      "learning_rate": 1.9382929135243287e-05,
      "loss": 0.242,
      "step": 278500
    },
    {
      "epoch": 6.134429761878587,
      "grad_norm": 4.625794887542725,
      "learning_rate": 1.9327961126624306e-05,
      "loss": 0.2626,
      "step": 279000
    },
    {
      "epoch": 6.145423363602384,
      "grad_norm": 11.883264541625977,
      "learning_rate": 1.927299311800532e-05,
      "loss": 0.2768,
      "step": 279500
    },
    {
      "epoch": 6.15641696532618,
      "grad_norm": 16.270444869995117,
      "learning_rate": 1.9218025109386336e-05,
      "loss": 0.2585,
      "step": 280000
    },
    {
      "epoch": 6.167410567049977,
      "grad_norm": 17.29378890991211,
      "learning_rate": 1.9163057100767354e-05,
      "loss": 0.2714,
      "step": 280500
    },
    {
      "epoch": 6.178404168773774,
      "grad_norm": 9.361930847167969,
      "learning_rate": 1.910808909214837e-05,
      "loss": 0.2543,
      "step": 281000
    },
    {
      "epoch": 6.18939777049757,
      "grad_norm": 4.807289123535156,
      "learning_rate": 1.9053121083529388e-05,
      "loss": 0.2595,
      "step": 281500
    },
    {
      "epoch": 6.200391372221367,
      "grad_norm": 1.0138145685195923,
      "learning_rate": 1.8998153074910403e-05,
      "loss": 0.2668,
      "step": 282000
    },
    {
      "epoch": 6.211384973945164,
      "grad_norm": 6.5260725021362305,
      "learning_rate": 1.8943185066291418e-05,
      "loss": 0.2847,
      "step": 282500
    },
    {
      "epoch": 6.2223785756689605,
      "grad_norm": 6.645490646362305,
      "learning_rate": 1.8888217057672437e-05,
      "loss": 0.2563,
      "step": 283000
    },
    {
      "epoch": 6.2333721773927575,
      "grad_norm": 11.542099952697754,
      "learning_rate": 1.8833249049053452e-05,
      "loss": 0.2663,
      "step": 283500
    },
    {
      "epoch": 6.2443657791165545,
      "grad_norm": 0.42281296849250793,
      "learning_rate": 1.8778281040434467e-05,
      "loss": 0.2539,
      "step": 284000
    },
    {
      "epoch": 6.255359380840351,
      "grad_norm": 3.1694846153259277,
      "learning_rate": 1.8723313031815486e-05,
      "loss": 0.2793,
      "step": 284500
    },
    {
      "epoch": 6.266352982564148,
      "grad_norm": 0.233106330037117,
      "learning_rate": 1.86683450231965e-05,
      "loss": 0.2438,
      "step": 285000
    },
    {
      "epoch": 6.277346584287945,
      "grad_norm": 13.849515914916992,
      "learning_rate": 1.8613377014577516e-05,
      "loss": 0.2713,
      "step": 285500
    },
    {
      "epoch": 6.288340186011741,
      "grad_norm": 16.040163040161133,
      "learning_rate": 1.855840900595853e-05,
      "loss": 0.2633,
      "step": 286000
    },
    {
      "epoch": 6.299333787735538,
      "grad_norm": 7.7579193115234375,
      "learning_rate": 1.850344099733955e-05,
      "loss": 0.2733,
      "step": 286500
    },
    {
      "epoch": 6.310327389459335,
      "grad_norm": 17.130664825439453,
      "learning_rate": 1.8448472988720568e-05,
      "loss": 0.2704,
      "step": 287000
    },
    {
      "epoch": 6.321320991183131,
      "grad_norm": 11.68111801147461,
      "learning_rate": 1.839350498010158e-05,
      "loss": 0.2705,
      "step": 287500
    },
    {
      "epoch": 6.332314592906928,
      "grad_norm": 1.9362109899520874,
      "learning_rate": 1.8338536971482598e-05,
      "loss": 0.2734,
      "step": 288000
    },
    {
      "epoch": 6.343308194630725,
      "grad_norm": 11.703132629394531,
      "learning_rate": 1.8283568962863613e-05,
      "loss": 0.2737,
      "step": 288500
    },
    {
      "epoch": 6.354301796354521,
      "grad_norm": 0.47743090987205505,
      "learning_rate": 1.8228600954244632e-05,
      "loss": 0.263,
      "step": 289000
    },
    {
      "epoch": 6.3652953980783185,
      "grad_norm": 9.345637321472168,
      "learning_rate": 1.8173632945625647e-05,
      "loss": 0.2752,
      "step": 289500
    },
    {
      "epoch": 6.3762889998021155,
      "grad_norm": 3.765399932861328,
      "learning_rate": 1.8118664937006662e-05,
      "loss": 0.2775,
      "step": 290000
    },
    {
      "epoch": 6.387282601525912,
      "grad_norm": 0.6770306825637817,
      "learning_rate": 1.806369692838768e-05,
      "loss": 0.2603,
      "step": 290500
    },
    {
      "epoch": 6.398276203249709,
      "grad_norm": 14.039237022399902,
      "learning_rate": 1.8008728919768696e-05,
      "loss": 0.2757,
      "step": 291000
    },
    {
      "epoch": 6.409269804973506,
      "grad_norm": 9.274356842041016,
      "learning_rate": 1.795376091114971e-05,
      "loss": 0.2789,
      "step": 291500
    },
    {
      "epoch": 6.420263406697302,
      "grad_norm": 15.968616485595703,
      "learning_rate": 1.789879290253073e-05,
      "loss": 0.276,
      "step": 292000
    },
    {
      "epoch": 6.431257008421099,
      "grad_norm": 4.934060096740723,
      "learning_rate": 1.7843824893911744e-05,
      "loss": 0.2675,
      "step": 292500
    },
    {
      "epoch": 6.442250610144896,
      "grad_norm": 20.050159454345703,
      "learning_rate": 1.778885688529276e-05,
      "loss": 0.2846,
      "step": 293000
    },
    {
      "epoch": 6.453244211868692,
      "grad_norm": 19.38918685913086,
      "learning_rate": 1.7733888876673778e-05,
      "loss": 0.2736,
      "step": 293500
    },
    {
      "epoch": 6.464237813592489,
      "grad_norm": 0.3655034303665161,
      "learning_rate": 1.7678920868054793e-05,
      "loss": 0.2526,
      "step": 294000
    },
    {
      "epoch": 6.475231415316286,
      "grad_norm": 0.3024774491786957,
      "learning_rate": 1.762395285943581e-05,
      "loss": 0.2864,
      "step": 294500
    },
    {
      "epoch": 6.486225017040082,
      "grad_norm": 1.211289882659912,
      "learning_rate": 1.7568984850816823e-05,
      "loss": 0.2646,
      "step": 295000
    },
    {
      "epoch": 6.497218618763879,
      "grad_norm": 7.488253116607666,
      "learning_rate": 1.7514016842197842e-05,
      "loss": 0.26,
      "step": 295500
    },
    {
      "epoch": 6.5082122204876764,
      "grad_norm": 0.42164233326911926,
      "learning_rate": 1.745904883357886e-05,
      "loss": 0.2804,
      "step": 296000
    },
    {
      "epoch": 6.519205822211473,
      "grad_norm": 10.387120246887207,
      "learning_rate": 1.7404080824959872e-05,
      "loss": 0.2769,
      "step": 296500
    },
    {
      "epoch": 6.53019942393527,
      "grad_norm": 7.694187164306641,
      "learning_rate": 1.734911281634089e-05,
      "loss": 0.2583,
      "step": 297000
    },
    {
      "epoch": 6.541193025659067,
      "grad_norm": 8.621971130371094,
      "learning_rate": 1.7294144807721906e-05,
      "loss": 0.2613,
      "step": 297500
    },
    {
      "epoch": 6.552186627382863,
      "grad_norm": 10.021193504333496,
      "learning_rate": 1.7239176799102924e-05,
      "loss": 0.2813,
      "step": 298000
    },
    {
      "epoch": 6.56318022910666,
      "grad_norm": 1.557435393333435,
      "learning_rate": 1.718420879048394e-05,
      "loss": 0.2882,
      "step": 298500
    },
    {
      "epoch": 6.574173830830457,
      "grad_norm": 0.6697182059288025,
      "learning_rate": 1.7129240781864954e-05,
      "loss": 0.2502,
      "step": 299000
    },
    {
      "epoch": 6.585167432554253,
      "grad_norm": 0.2241702675819397,
      "learning_rate": 1.7074272773245973e-05,
      "loss": 0.2794,
      "step": 299500
    },
    {
      "epoch": 6.59616103427805,
      "grad_norm": 3.3601033687591553,
      "learning_rate": 1.7019304764626988e-05,
      "loss": 0.2582,
      "step": 300000
    },
    {
      "epoch": 6.607154636001847,
      "grad_norm": 9.968061447143555,
      "learning_rate": 1.6964336756008003e-05,
      "loss": 0.2748,
      "step": 300500
    },
    {
      "epoch": 6.618148237725643,
      "grad_norm": 6.296371936798096,
      "learning_rate": 1.690936874738902e-05,
      "loss": 0.2694,
      "step": 301000
    },
    {
      "epoch": 6.62914183944944,
      "grad_norm": 22.50356101989746,
      "learning_rate": 1.6854400738770037e-05,
      "loss": 0.2842,
      "step": 301500
    },
    {
      "epoch": 6.640135441173237,
      "grad_norm": 8.103245735168457,
      "learning_rate": 1.6799432730151052e-05,
      "loss": 0.2709,
      "step": 302000
    },
    {
      "epoch": 6.6511290428970335,
      "grad_norm": 11.021881103515625,
      "learning_rate": 1.674446472153207e-05,
      "loss": 0.2846,
      "step": 302500
    },
    {
      "epoch": 6.662122644620831,
      "grad_norm": 1.0715149641036987,
      "learning_rate": 1.6689496712913085e-05,
      "loss": 0.2493,
      "step": 303000
    },
    {
      "epoch": 6.673116246344628,
      "grad_norm": 20.791400909423828,
      "learning_rate": 1.6634528704294104e-05,
      "loss": 0.267,
      "step": 303500
    },
    {
      "epoch": 6.684109848068424,
      "grad_norm": 0.3448217213153839,
      "learning_rate": 1.6579560695675116e-05,
      "loss": 0.2794,
      "step": 304000
    },
    {
      "epoch": 6.695103449792221,
      "grad_norm": 0.8398261666297913,
      "learning_rate": 1.6524592687056134e-05,
      "loss": 0.2903,
      "step": 304500
    },
    {
      "epoch": 6.706097051516018,
      "grad_norm": 12.431689262390137,
      "learning_rate": 1.6469624678437153e-05,
      "loss": 0.298,
      "step": 305000
    },
    {
      "epoch": 6.717090653239815,
      "grad_norm": 0.6172216534614563,
      "learning_rate": 1.6414656669818164e-05,
      "loss": 0.2601,
      "step": 305500
    },
    {
      "epoch": 6.728084254963611,
      "grad_norm": 1.9610120058059692,
      "learning_rate": 1.6359688661199183e-05,
      "loss": 0.2574,
      "step": 306000
    },
    {
      "epoch": 6.739077856687408,
      "grad_norm": 4.058801651000977,
      "learning_rate": 1.6304720652580198e-05,
      "loss": 0.2686,
      "step": 306500
    },
    {
      "epoch": 6.750071458411204,
      "grad_norm": 2.1434834003448486,
      "learning_rate": 1.6249752643961216e-05,
      "loss": 0.2595,
      "step": 307000
    },
    {
      "epoch": 6.761065060135001,
      "grad_norm": 0.3219200670719147,
      "learning_rate": 1.619478463534223e-05,
      "loss": 0.2784,
      "step": 307500
    },
    {
      "epoch": 6.772058661858798,
      "grad_norm": 6.090395927429199,
      "learning_rate": 1.6139816626723247e-05,
      "loss": 0.2746,
      "step": 308000
    },
    {
      "epoch": 6.783052263582595,
      "grad_norm": 7.1926703453063965,
      "learning_rate": 1.6084848618104265e-05,
      "loss": 0.2565,
      "step": 308500
    },
    {
      "epoch": 6.7940458653063915,
      "grad_norm": 12.808427810668945,
      "learning_rate": 1.602988060948528e-05,
      "loss": 0.2639,
      "step": 309000
    },
    {
      "epoch": 6.805039467030189,
      "grad_norm": 0.1455288976430893,
      "learning_rate": 1.5974912600866295e-05,
      "loss": 0.2819,
      "step": 309500
    },
    {
      "epoch": 6.816033068753985,
      "grad_norm": 16.006120681762695,
      "learning_rate": 1.5919944592247314e-05,
      "loss": 0.2716,
      "step": 310000
    },
    {
      "epoch": 6.827026670477782,
      "grad_norm": 4.725459098815918,
      "learning_rate": 1.586497658362833e-05,
      "loss": 0.2604,
      "step": 310500
    },
    {
      "epoch": 6.838020272201579,
      "grad_norm": 6.786791801452637,
      "learning_rate": 1.5810008575009344e-05,
      "loss": 0.2687,
      "step": 311000
    },
    {
      "epoch": 6.849013873925376,
      "grad_norm": 0.6368587017059326,
      "learning_rate": 1.5755040566390363e-05,
      "loss": 0.278,
      "step": 311500
    },
    {
      "epoch": 6.860007475649172,
      "grad_norm": 13.020255088806152,
      "learning_rate": 1.5700072557771378e-05,
      "loss": 0.2679,
      "step": 312000
    },
    {
      "epoch": 6.871001077372969,
      "grad_norm": 10.257661819458008,
      "learning_rate": 1.5645104549152396e-05,
      "loss": 0.2729,
      "step": 312500
    },
    {
      "epoch": 6.881994679096765,
      "grad_norm": 0.34293073415756226,
      "learning_rate": 1.5590136540533408e-05,
      "loss": 0.2679,
      "step": 313000
    },
    {
      "epoch": 6.892988280820562,
      "grad_norm": 0.5395570397377014,
      "learning_rate": 1.5535168531914427e-05,
      "loss": 0.2726,
      "step": 313500
    },
    {
      "epoch": 6.903981882544359,
      "grad_norm": 0.6254929304122925,
      "learning_rate": 1.5480200523295445e-05,
      "loss": 0.2794,
      "step": 314000
    },
    {
      "epoch": 6.914975484268156,
      "grad_norm": 0.4827207624912262,
      "learning_rate": 1.542523251467646e-05,
      "loss": 0.281,
      "step": 314500
    },
    {
      "epoch": 6.9259690859919525,
      "grad_norm": 1.3303064107894897,
      "learning_rate": 1.5370264506057475e-05,
      "loss": 0.259,
      "step": 315000
    },
    {
      "epoch": 6.9369626877157495,
      "grad_norm": 0.9676883220672607,
      "learning_rate": 1.531529649743849e-05,
      "loss": 0.2692,
      "step": 315500
    },
    {
      "epoch": 6.947956289439546,
      "grad_norm": 16.72637367248535,
      "learning_rate": 1.526032848881951e-05,
      "loss": 0.2614,
      "step": 316000
    },
    {
      "epoch": 6.958949891163343,
      "grad_norm": 0.5471804738044739,
      "learning_rate": 1.5205360480200522e-05,
      "loss": 0.2896,
      "step": 316500
    },
    {
      "epoch": 6.96994349288714,
      "grad_norm": 11.51943302154541,
      "learning_rate": 1.5150392471581539e-05,
      "loss": 0.2719,
      "step": 317000
    },
    {
      "epoch": 6.980937094610937,
      "grad_norm": 9.344624519348145,
      "learning_rate": 1.5095424462962558e-05,
      "loss": 0.2591,
      "step": 317500
    },
    {
      "epoch": 6.991930696334733,
      "grad_norm": 21.37694549560547,
      "learning_rate": 1.5040456454343574e-05,
      "loss": 0.2809,
      "step": 318000
    },
    {
      "epoch": 7.00292429805853,
      "grad_norm": 0.591389000415802,
      "learning_rate": 1.4985488445724588e-05,
      "loss": 0.2849,
      "step": 318500
    },
    {
      "epoch": 7.013917899782327,
      "grad_norm": 4.37274694442749,
      "learning_rate": 1.4930520437105605e-05,
      "loss": 0.2616,
      "step": 319000
    },
    {
      "epoch": 7.024911501506123,
      "grad_norm": 14.69211483001709,
      "learning_rate": 1.4875552428486621e-05,
      "loss": 0.2666,
      "step": 319500
    },
    {
      "epoch": 7.03590510322992,
      "grad_norm": 0.48231297731399536,
      "learning_rate": 1.482058441986764e-05,
      "loss": 0.2399,
      "step": 320000
    },
    {
      "epoch": 7.046898704953717,
      "grad_norm": 5.110250473022461,
      "learning_rate": 1.4765616411248653e-05,
      "loss": 0.2815,
      "step": 320500
    },
    {
      "epoch": 7.057892306677513,
      "grad_norm": 14.146740913391113,
      "learning_rate": 1.471064840262967e-05,
      "loss": 0.2592,
      "step": 321000
    },
    {
      "epoch": 7.0688859084013105,
      "grad_norm": 2.4041287899017334,
      "learning_rate": 1.4655680394010687e-05,
      "loss": 0.2733,
      "step": 321500
    },
    {
      "epoch": 7.0798795101251075,
      "grad_norm": 0.6426034569740295,
      "learning_rate": 1.4600712385391702e-05,
      "loss": 0.2514,
      "step": 322000
    },
    {
      "epoch": 7.090873111848904,
      "grad_norm": 20.453636169433594,
      "learning_rate": 1.4545744376772719e-05,
      "loss": 0.2607,
      "step": 322500
    },
    {
      "epoch": 7.101866713572701,
      "grad_norm": 1.1130634546279907,
      "learning_rate": 1.4490776368153736e-05,
      "loss": 0.2663,
      "step": 323000
    },
    {
      "epoch": 7.112860315296498,
      "grad_norm": 4.651683807373047,
      "learning_rate": 1.4435808359534753e-05,
      "loss": 0.2592,
      "step": 323500
    },
    {
      "epoch": 7.123853917020294,
      "grad_norm": 5.64998197555542,
      "learning_rate": 1.4380840350915768e-05,
      "loss": 0.2801,
      "step": 324000
    },
    {
      "epoch": 7.134847518744091,
      "grad_norm": 9.622928619384766,
      "learning_rate": 1.4325872342296784e-05,
      "loss": 0.2638,
      "step": 324500
    },
    {
      "epoch": 7.145841120467888,
      "grad_norm": 16.641155242919922,
      "learning_rate": 1.4270904333677801e-05,
      "loss": 0.27,
      "step": 325000
    },
    {
      "epoch": 7.156834722191684,
      "grad_norm": 10.92292594909668,
      "learning_rate": 1.4215936325058815e-05,
      "loss": 0.2526,
      "step": 325500
    },
    {
      "epoch": 7.167828323915481,
      "grad_norm": 27.666790008544922,
      "learning_rate": 1.4160968316439831e-05,
      "loss": 0.2544,
      "step": 326000
    },
    {
      "epoch": 7.178821925639278,
      "grad_norm": 23.030431747436523,
      "learning_rate": 1.410600030782085e-05,
      "loss": 0.2649,
      "step": 326500
    },
    {
      "epoch": 7.189815527363074,
      "grad_norm": 7.667928218841553,
      "learning_rate": 1.4051032299201867e-05,
      "loss": 0.275,
      "step": 327000
    },
    {
      "epoch": 7.200809129086871,
      "grad_norm": 7.699097156524658,
      "learning_rate": 1.399606429058288e-05,
      "loss": 0.2491,
      "step": 327500
    },
    {
      "epoch": 7.2118027308106685,
      "grad_norm": 37.110572814941406,
      "learning_rate": 1.3941096281963897e-05,
      "loss": 0.2654,
      "step": 328000
    },
    {
      "epoch": 7.222796332534465,
      "grad_norm": 10.71968936920166,
      "learning_rate": 1.3886128273344914e-05,
      "loss": 0.2593,
      "step": 328500
    },
    {
      "epoch": 7.233789934258262,
      "grad_norm": 0.9767385721206665,
      "learning_rate": 1.383116026472593e-05,
      "loss": 0.2692,
      "step": 329000
    },
    {
      "epoch": 7.244783535982059,
      "grad_norm": 13.067646980285645,
      "learning_rate": 1.3776192256106946e-05,
      "loss": 0.2732,
      "step": 329500
    },
    {
      "epoch": 7.255777137705855,
      "grad_norm": 8.024004936218262,
      "learning_rate": 1.3721224247487963e-05,
      "loss": 0.2604,
      "step": 330000
    },
    {
      "epoch": 7.266770739429652,
      "grad_norm": 14.381461143493652,
      "learning_rate": 1.366625623886898e-05,
      "loss": 0.2566,
      "step": 330500
    },
    {
      "epoch": 7.277764341153449,
      "grad_norm": 0.9243181943893433,
      "learning_rate": 1.3611288230249994e-05,
      "loss": 0.246,
      "step": 331000
    },
    {
      "epoch": 7.288757942877245,
      "grad_norm": 0.34048131108283997,
      "learning_rate": 1.3556320221631011e-05,
      "loss": 0.2509,
      "step": 331500
    },
    {
      "epoch": 7.299751544601042,
      "grad_norm": 12.101808547973633,
      "learning_rate": 1.3501352213012028e-05,
      "loss": 0.273,
      "step": 332000
    },
    {
      "epoch": 7.310745146324839,
      "grad_norm": 13.75164794921875,
      "learning_rate": 1.3446384204393045e-05,
      "loss": 0.257,
      "step": 332500
    },
    {
      "epoch": 7.321738748048635,
      "grad_norm": 10.359282493591309,
      "learning_rate": 1.3391416195774058e-05,
      "loss": 0.2577,
      "step": 333000
    },
    {
      "epoch": 7.332732349772432,
      "grad_norm": 12.815690994262695,
      "learning_rate": 1.3336448187155077e-05,
      "loss": 0.2764,
      "step": 333500
    },
    {
      "epoch": 7.343725951496229,
      "grad_norm": 1.0336490869522095,
      "learning_rate": 1.3281480178536094e-05,
      "loss": 0.265,
      "step": 334000
    },
    {
      "epoch": 7.354719553220026,
      "grad_norm": 0.4791889488697052,
      "learning_rate": 1.322651216991711e-05,
      "loss": 0.2843,
      "step": 334500
    },
    {
      "epoch": 7.365713154943823,
      "grad_norm": 0.2148377001285553,
      "learning_rate": 1.3171544161298124e-05,
      "loss": 0.2485,
      "step": 335000
    },
    {
      "epoch": 7.37670675666762,
      "grad_norm": 0.961244523525238,
      "learning_rate": 1.311657615267914e-05,
      "loss": 0.2678,
      "step": 335500
    },
    {
      "epoch": 7.387700358391416,
      "grad_norm": 2.759814739227295,
      "learning_rate": 1.306160814406016e-05,
      "loss": 0.2733,
      "step": 336000
    },
    {
      "epoch": 7.398693960115213,
      "grad_norm": 1.0307798385620117,
      "learning_rate": 1.3006640135441173e-05,
      "loss": 0.2698,
      "step": 336500
    },
    {
      "epoch": 7.40968756183901,
      "grad_norm": 16.76552391052246,
      "learning_rate": 1.295167212682219e-05,
      "loss": 0.2496,
      "step": 337000
    },
    {
      "epoch": 7.420681163562806,
      "grad_norm": 11.19717025756836,
      "learning_rate": 1.2896704118203206e-05,
      "loss": 0.2762,
      "step": 337500
    },
    {
      "epoch": 7.431674765286603,
      "grad_norm": 19.323009490966797,
      "learning_rate": 1.2841736109584223e-05,
      "loss": 0.2474,
      "step": 338000
    },
    {
      "epoch": 7.4426683670104,
      "grad_norm": 0.9544785618782043,
      "learning_rate": 1.2786768100965238e-05,
      "loss": 0.2692,
      "step": 338500
    },
    {
      "epoch": 7.453661968734196,
      "grad_norm": 0.35109391808509827,
      "learning_rate": 1.2731800092346255e-05,
      "loss": 0.2763,
      "step": 339000
    },
    {
      "epoch": 7.464655570457993,
      "grad_norm": 8.524725914001465,
      "learning_rate": 1.2676832083727272e-05,
      "loss": 0.2679,
      "step": 339500
    },
    {
      "epoch": 7.47564917218179,
      "grad_norm": 0.8548899292945862,
      "learning_rate": 1.2621864075108289e-05,
      "loss": 0.2572,
      "step": 340000
    },
    {
      "epoch": 7.4866427739055865,
      "grad_norm": 12.642956733703613,
      "learning_rate": 1.2566896066489304e-05,
      "loss": 0.2716,
      "step": 340500
    },
    {
      "epoch": 7.497636375629384,
      "grad_norm": 13.959696769714355,
      "learning_rate": 1.251192805787032e-05,
      "loss": 0.2845,
      "step": 341000
    },
    {
      "epoch": 7.508629977353181,
      "grad_norm": 1.8245211839675903,
      "learning_rate": 1.2456960049251336e-05,
      "loss": 0.2599,
      "step": 341500
    },
    {
      "epoch": 7.519623579076978,
      "grad_norm": 10.828299522399902,
      "learning_rate": 1.2401992040632352e-05,
      "loss": 0.2615,
      "step": 342000
    },
    {
      "epoch": 7.530617180800774,
      "grad_norm": 2.6722023487091064,
      "learning_rate": 1.234702403201337e-05,
      "loss": 0.2794,
      "step": 342500
    },
    {
      "epoch": 7.541610782524571,
      "grad_norm": 0.14752168953418732,
      "learning_rate": 1.2292056023394386e-05,
      "loss": 0.2717,
      "step": 343000
    },
    {
      "epoch": 7.552604384248367,
      "grad_norm": 22.372373580932617,
      "learning_rate": 1.2237088014775401e-05,
      "loss": 0.2555,
      "step": 343500
    },
    {
      "epoch": 7.563597985972164,
      "grad_norm": 19.060758590698242,
      "learning_rate": 1.2182120006156418e-05,
      "loss": 0.2664,
      "step": 344000
    },
    {
      "epoch": 7.574591587695961,
      "grad_norm": 10.173792839050293,
      "learning_rate": 1.2127151997537433e-05,
      "loss": 0.2882,
      "step": 344500
    },
    {
      "epoch": 7.585585189419758,
      "grad_norm": 10.00798511505127,
      "learning_rate": 1.2072183988918452e-05,
      "loss": 0.2908,
      "step": 345000
    },
    {
      "epoch": 7.596578791143554,
      "grad_norm": 12.419585227966309,
      "learning_rate": 1.2017215980299467e-05,
      "loss": 0.2781,
      "step": 345500
    },
    {
      "epoch": 7.607572392867351,
      "grad_norm": 2.998307466506958,
      "learning_rate": 1.1962247971680482e-05,
      "loss": 0.2643,
      "step": 346000
    },
    {
      "epoch": 7.6185659945911475,
      "grad_norm": 7.3951311111450195,
      "learning_rate": 1.1907279963061499e-05,
      "loss": 0.2572,
      "step": 346500
    },
    {
      "epoch": 7.6295595963149445,
      "grad_norm": 1.3398966789245605,
      "learning_rate": 1.1852311954442515e-05,
      "loss": 0.2594,
      "step": 347000
    },
    {
      "epoch": 7.6405531980387416,
      "grad_norm": 0.38793835043907166,
      "learning_rate": 1.1797343945823532e-05,
      "loss": 0.2517,
      "step": 347500
    },
    {
      "epoch": 7.651546799762539,
      "grad_norm": 8.906951904296875,
      "learning_rate": 1.1742375937204547e-05,
      "loss": 0.2605,
      "step": 348000
    },
    {
      "epoch": 7.662540401486335,
      "grad_norm": 3.6884825229644775,
      "learning_rate": 1.1687407928585564e-05,
      "loss": 0.2834,
      "step": 348500
    },
    {
      "epoch": 7.673534003210132,
      "grad_norm": 5.170904159545898,
      "learning_rate": 1.163243991996658e-05,
      "loss": 0.2586,
      "step": 349000
    },
    {
      "epoch": 7.684527604933929,
      "grad_norm": 10.966479301452637,
      "learning_rate": 1.1577471911347596e-05,
      "loss": 0.2954,
      "step": 349500
    },
    {
      "epoch": 7.695521206657725,
      "grad_norm": 0.35831698775291443,
      "learning_rate": 1.1522503902728613e-05,
      "loss": 0.263,
      "step": 350000
    },
    {
      "epoch": 7.706514808381522,
      "grad_norm": 9.283620834350586,
      "learning_rate": 1.1467535894109628e-05,
      "loss": 0.2714,
      "step": 350500
    },
    {
      "epoch": 7.717508410105319,
      "grad_norm": 1.2214860916137695,
      "learning_rate": 1.1412567885490645e-05,
      "loss": 0.2571,
      "step": 351000
    },
    {
      "epoch": 7.728502011829115,
      "grad_norm": 9.132270812988281,
      "learning_rate": 1.135759987687166e-05,
      "loss": 0.2481,
      "step": 351500
    },
    {
      "epoch": 7.739495613552912,
      "grad_norm": 0.3136681914329529,
      "learning_rate": 1.1302631868252678e-05,
      "loss": 0.2731,
      "step": 352000
    },
    {
      "epoch": 7.750489215276709,
      "grad_norm": 16.377269744873047,
      "learning_rate": 1.1247663859633693e-05,
      "loss": 0.2622,
      "step": 352500
    },
    {
      "epoch": 7.7614828170005055,
      "grad_norm": 5.582587718963623,
      "learning_rate": 1.119269585101471e-05,
      "loss": 0.2487,
      "step": 353000
    },
    {
      "epoch": 7.7724764187243025,
      "grad_norm": 16.93551254272461,
      "learning_rate": 1.1137727842395725e-05,
      "loss": 0.2741,
      "step": 353500
    },
    {
      "epoch": 7.7834700204480995,
      "grad_norm": 15.634310722351074,
      "learning_rate": 1.1082759833776742e-05,
      "loss": 0.2776,
      "step": 354000
    },
    {
      "epoch": 7.794463622171896,
      "grad_norm": 1.8954389095306396,
      "learning_rate": 1.1027791825157759e-05,
      "loss": 0.2747,
      "step": 354500
    },
    {
      "epoch": 7.805457223895693,
      "grad_norm": 7.018278121948242,
      "learning_rate": 1.0972823816538776e-05,
      "loss": 0.2588,
      "step": 355000
    },
    {
      "epoch": 7.81645082561949,
      "grad_norm": 24.890254974365234,
      "learning_rate": 1.0917855807919791e-05,
      "loss": 0.2499,
      "step": 355500
    },
    {
      "epoch": 7.827444427343286,
      "grad_norm": 0.3367525637149811,
      "learning_rate": 1.0862887799300806e-05,
      "loss": 0.2724,
      "step": 356000
    },
    {
      "epoch": 7.838438029067083,
      "grad_norm": 10.0278959274292,
      "learning_rate": 1.0807919790681825e-05,
      "loss": 0.269,
      "step": 356500
    },
    {
      "epoch": 7.84943163079088,
      "grad_norm": 0.6670719385147095,
      "learning_rate": 1.075295178206284e-05,
      "loss": 0.2613,
      "step": 357000
    },
    {
      "epoch": 7.860425232514676,
      "grad_norm": 1.2707165479660034,
      "learning_rate": 1.0697983773443856e-05,
      "loss": 0.2487,
      "step": 357500
    },
    {
      "epoch": 7.871418834238473,
      "grad_norm": 0.6761378645896912,
      "learning_rate": 1.0643015764824872e-05,
      "loss": 0.2652,
      "step": 358000
    },
    {
      "epoch": 7.88241243596227,
      "grad_norm": 8.755736351013184,
      "learning_rate": 1.0588047756205888e-05,
      "loss": 0.2891,
      "step": 358500
    },
    {
      "epoch": 7.893406037686066,
      "grad_norm": 18.66051483154297,
      "learning_rate": 1.0533079747586905e-05,
      "loss": 0.2692,
      "step": 359000
    },
    {
      "epoch": 7.9043996394098635,
      "grad_norm": 17.380126953125,
      "learning_rate": 1.0478111738967922e-05,
      "loss": 0.2696,
      "step": 359500
    },
    {
      "epoch": 7.9153932411336605,
      "grad_norm": 0.21498478949069977,
      "learning_rate": 1.0423143730348937e-05,
      "loss": 0.2674,
      "step": 360000
    },
    {
      "epoch": 7.926386842857457,
      "grad_norm": 1.4047166109085083,
      "learning_rate": 1.0368175721729952e-05,
      "loss": 0.2486,
      "step": 360500
    },
    {
      "epoch": 7.937380444581254,
      "grad_norm": 0.922021746635437,
      "learning_rate": 1.031320771311097e-05,
      "loss": 0.259,
      "step": 361000
    },
    {
      "epoch": 7.948374046305051,
      "grad_norm": 8.361268997192383,
      "learning_rate": 1.0258239704491986e-05,
      "loss": 0.2758,
      "step": 361500
    },
    {
      "epoch": 7.959367648028847,
      "grad_norm": 6.821991443634033,
      "learning_rate": 1.0203271695873003e-05,
      "loss": 0.2688,
      "step": 362000
    },
    {
      "epoch": 7.970361249752644,
      "grad_norm": 12.020538330078125,
      "learning_rate": 1.0148303687254018e-05,
      "loss": 0.2636,
      "step": 362500
    },
    {
      "epoch": 7.981354851476441,
      "grad_norm": 6.6103034019470215,
      "learning_rate": 1.0093335678635035e-05,
      "loss": 0.263,
      "step": 363000
    },
    {
      "epoch": 7.992348453200237,
      "grad_norm": 0.3278445899486542,
      "learning_rate": 1.0038367670016051e-05,
      "loss": 0.2785,
      "step": 363500
    },
    {
      "epoch": 8.003342054924035,
      "grad_norm": 0.32418566942214966,
      "learning_rate": 9.983399661397068e-06,
      "loss": 0.2669,
      "step": 364000
    },
    {
      "epoch": 8.01433565664783,
      "grad_norm": 1.1546884775161743,
      "learning_rate": 9.928431652778083e-06,
      "loss": 0.2464,
      "step": 364500
    },
    {
      "epoch": 8.025329258371627,
      "grad_norm": 17.02813148498535,
      "learning_rate": 9.8734636441591e-06,
      "loss": 0.2533,
      "step": 365000
    },
    {
      "epoch": 8.036322860095424,
      "grad_norm": 0.3272104263305664,
      "learning_rate": 9.818495635540117e-06,
      "loss": 0.2591,
      "step": 365500
    },
    {
      "epoch": 8.047316461819221,
      "grad_norm": 14.700984001159668,
      "learning_rate": 9.763527626921132e-06,
      "loss": 0.2625,
      "step": 366000
    },
    {
      "epoch": 8.058310063543018,
      "grad_norm": 6.891590595245361,
      "learning_rate": 9.708559618302149e-06,
      "loss": 0.2727,
      "step": 366500
    },
    {
      "epoch": 8.069303665266816,
      "grad_norm": 6.878860950469971,
      "learning_rate": 9.653591609683164e-06,
      "loss": 0.2521,
      "step": 367000
    },
    {
      "epoch": 8.08029726699061,
      "grad_norm": 19.778362274169922,
      "learning_rate": 9.59862360106418e-06,
      "loss": 0.245,
      "step": 367500
    },
    {
      "epoch": 8.091290868714408,
      "grad_norm": 22.404842376708984,
      "learning_rate": 9.543655592445198e-06,
      "loss": 0.2767,
      "step": 368000
    },
    {
      "epoch": 8.102284470438205,
      "grad_norm": 18.661029815673828,
      "learning_rate": 9.488687583826214e-06,
      "loss": 0.2425,
      "step": 368500
    },
    {
      "epoch": 8.113278072162002,
      "grad_norm": 9.656877517700195,
      "learning_rate": 9.43371957520723e-06,
      "loss": 0.2449,
      "step": 369000
    },
    {
      "epoch": 8.124271673885799,
      "grad_norm": 0.40236416459083557,
      "learning_rate": 9.378751566588246e-06,
      "loss": 0.2575,
      "step": 369500
    },
    {
      "epoch": 8.135265275609596,
      "grad_norm": 27.573410034179688,
      "learning_rate": 9.323783557969261e-06,
      "loss": 0.2475,
      "step": 370000
    },
    {
      "epoch": 8.146258877333391,
      "grad_norm": 6.51427698135376,
      "learning_rate": 9.268815549350278e-06,
      "loss": 0.2793,
      "step": 370500
    },
    {
      "epoch": 8.157252479057188,
      "grad_norm": 1.7049187421798706,
      "learning_rate": 9.213847540731295e-06,
      "loss": 0.2613,
      "step": 371000
    },
    {
      "epoch": 8.168246080780985,
      "grad_norm": 17.31573486328125,
      "learning_rate": 9.15887953211231e-06,
      "loss": 0.293,
      "step": 371500
    },
    {
      "epoch": 8.179239682504782,
      "grad_norm": 6.598927974700928,
      "learning_rate": 9.103911523493327e-06,
      "loss": 0.2741,
      "step": 372000
    },
    {
      "epoch": 8.19023328422858,
      "grad_norm": 3.2017316818237305,
      "learning_rate": 9.048943514874344e-06,
      "loss": 0.2658,
      "step": 372500
    },
    {
      "epoch": 8.201226885952376,
      "grad_norm": 9.928788185119629,
      "learning_rate": 8.99397550625536e-06,
      "loss": 0.2526,
      "step": 373000
    },
    {
      "epoch": 8.212220487676172,
      "grad_norm": 14.615266799926758,
      "learning_rate": 8.939007497636376e-06,
      "loss": 0.2504,
      "step": 373500
    },
    {
      "epoch": 8.223214089399969,
      "grad_norm": 14.227230072021484,
      "learning_rate": 8.884039489017393e-06,
      "loss": 0.2495,
      "step": 374000
    },
    {
      "epoch": 8.234207691123766,
      "grad_norm": 12.855712890625,
      "learning_rate": 8.829071480398408e-06,
      "loss": 0.2726,
      "step": 374500
    },
    {
      "epoch": 8.245201292847563,
      "grad_norm": 10.695145606994629,
      "learning_rate": 8.774103471779426e-06,
      "loss": 0.2878,
      "step": 375000
    },
    {
      "epoch": 8.25619489457136,
      "grad_norm": 3.807751417160034,
      "learning_rate": 8.719135463160441e-06,
      "loss": 0.2706,
      "step": 375500
    },
    {
      "epoch": 8.267188496295157,
      "grad_norm": 2.121506929397583,
      "learning_rate": 8.664167454541456e-06,
      "loss": 0.2474,
      "step": 376000
    },
    {
      "epoch": 8.278182098018952,
      "grad_norm": 23.198537826538086,
      "learning_rate": 8.609199445922473e-06,
      "loss": 0.2452,
      "step": 376500
    },
    {
      "epoch": 8.28917569974275,
      "grad_norm": 0.7727935910224915,
      "learning_rate": 8.55423143730349e-06,
      "loss": 0.2668,
      "step": 377000
    },
    {
      "epoch": 8.300169301466546,
      "grad_norm": 8.03891372680664,
      "learning_rate": 8.499263428684507e-06,
      "loss": 0.2683,
      "step": 377500
    },
    {
      "epoch": 8.311162903190343,
      "grad_norm": 9.486642837524414,
      "learning_rate": 8.444295420065522e-06,
      "loss": 0.2447,
      "step": 378000
    },
    {
      "epoch": 8.32215650491414,
      "grad_norm": 17.229413986206055,
      "learning_rate": 8.389327411446539e-06,
      "loss": 0.2675,
      "step": 378500
    },
    {
      "epoch": 8.333150106637937,
      "grad_norm": 1.062376856803894,
      "learning_rate": 8.334359402827554e-06,
      "loss": 0.2512,
      "step": 379000
    },
    {
      "epoch": 8.344143708361733,
      "grad_norm": 1.0481703281402588,
      "learning_rate": 8.279391394208572e-06,
      "loss": 0.2509,
      "step": 379500
    },
    {
      "epoch": 8.35513731008553,
      "grad_norm": 18.868375778198242,
      "learning_rate": 8.224423385589587e-06,
      "loss": 0.2747,
      "step": 380000
    },
    {
      "epoch": 8.366130911809327,
      "grad_norm": 13.230135917663574,
      "learning_rate": 8.169455376970603e-06,
      "loss": 0.247,
      "step": 380500
    },
    {
      "epoch": 8.377124513533124,
      "grad_norm": 12.545027732849121,
      "learning_rate": 8.11448736835162e-06,
      "loss": 0.268,
      "step": 381000
    },
    {
      "epoch": 8.38811811525692,
      "grad_norm": 0.5405644178390503,
      "learning_rate": 8.059519359732636e-06,
      "loss": 0.2676,
      "step": 381500
    },
    {
      "epoch": 8.399111716980718,
      "grad_norm": 0.519802451133728,
      "learning_rate": 8.004551351113653e-06,
      "loss": 0.2499,
      "step": 382000
    },
    {
      "epoch": 8.410105318704513,
      "grad_norm": 35.95849609375,
      "learning_rate": 7.949583342494668e-06,
      "loss": 0.2355,
      "step": 382500
    },
    {
      "epoch": 8.42109892042831,
      "grad_norm": 11.796894073486328,
      "learning_rate": 7.894615333875685e-06,
      "loss": 0.2573,
      "step": 383000
    },
    {
      "epoch": 8.432092522152107,
      "grad_norm": 20.47601318359375,
      "learning_rate": 7.8396473252567e-06,
      "loss": 0.2467,
      "step": 383500
    },
    {
      "epoch": 8.443086123875904,
      "grad_norm": 6.45081901550293,
      "learning_rate": 7.784679316637719e-06,
      "loss": 0.248,
      "step": 384000
    },
    {
      "epoch": 8.454079725599701,
      "grad_norm": 2.248415231704712,
      "learning_rate": 7.729711308018734e-06,
      "loss": 0.2772,
      "step": 384500
    },
    {
      "epoch": 8.465073327323498,
      "grad_norm": 13.735818862915039,
      "learning_rate": 7.67474329939975e-06,
      "loss": 0.2468,
      "step": 385000
    },
    {
      "epoch": 8.476066929047294,
      "grad_norm": 0.38378778100013733,
      "learning_rate": 7.6197752907807656e-06,
      "loss": 0.268,
      "step": 385500
    },
    {
      "epoch": 8.48706053077109,
      "grad_norm": 11.279451370239258,
      "learning_rate": 7.5648072821617815e-06,
      "loss": 0.2713,
      "step": 386000
    },
    {
      "epoch": 8.498054132494888,
      "grad_norm": 11.145171165466309,
      "learning_rate": 7.509839273542798e-06,
      "loss": 0.2561,
      "step": 386500
    },
    {
      "epoch": 8.509047734218685,
      "grad_norm": 0.615268349647522,
      "learning_rate": 7.454871264923814e-06,
      "loss": 0.2665,
      "step": 387000
    },
    {
      "epoch": 8.520041335942482,
      "grad_norm": 16.0561580657959,
      "learning_rate": 7.399903256304831e-06,
      "loss": 0.2555,
      "step": 387500
    },
    {
      "epoch": 8.531034937666279,
      "grad_norm": 12.508055686950684,
      "learning_rate": 7.344935247685847e-06,
      "loss": 0.258,
      "step": 388000
    },
    {
      "epoch": 8.542028539390074,
      "grad_norm": 14.659184455871582,
      "learning_rate": 7.289967239066864e-06,
      "loss": 0.2747,
      "step": 388500
    },
    {
      "epoch": 8.553022141113871,
      "grad_norm": 0.3804604411125183,
      "learning_rate": 7.23499923044788e-06,
      "loss": 0.2831,
      "step": 389000
    },
    {
      "epoch": 8.564015742837668,
      "grad_norm": 1.6809513568878174,
      "learning_rate": 7.180031221828897e-06,
      "loss": 0.2661,
      "step": 389500
    },
    {
      "epoch": 8.575009344561465,
      "grad_norm": 20.17511558532715,
      "learning_rate": 7.125063213209912e-06,
      "loss": 0.2365,
      "step": 390000
    },
    {
      "epoch": 8.586002946285262,
      "grad_norm": 7.230310916900635,
      "learning_rate": 7.070095204590929e-06,
      "loss": 0.2316,
      "step": 390500
    },
    {
      "epoch": 8.59699654800906,
      "grad_norm": 13.823753356933594,
      "learning_rate": 7.0151271959719445e-06,
      "loss": 0.269,
      "step": 391000
    },
    {
      "epoch": 8.607990149732856,
      "grad_norm": 0.20413239300251007,
      "learning_rate": 6.9601591873529605e-06,
      "loss": 0.2681,
      "step": 391500
    },
    {
      "epoch": 8.618983751456652,
      "grad_norm": 7.448430061340332,
      "learning_rate": 6.905191178733977e-06,
      "loss": 0.2785,
      "step": 392000
    },
    {
      "epoch": 8.629977353180449,
      "grad_norm": 0.6403422355651855,
      "learning_rate": 6.850223170114993e-06,
      "loss": 0.2646,
      "step": 392500
    },
    {
      "epoch": 8.640970954904246,
      "grad_norm": 4.24240255355835,
      "learning_rate": 6.79525516149601e-06,
      "loss": 0.2721,
      "step": 393000
    },
    {
      "epoch": 8.651964556628043,
      "grad_norm": 7.553249835968018,
      "learning_rate": 6.740287152877026e-06,
      "loss": 0.2486,
      "step": 393500
    },
    {
      "epoch": 8.66295815835184,
      "grad_norm": 19.911434173583984,
      "learning_rate": 6.685319144258043e-06,
      "loss": 0.2534,
      "step": 394000
    },
    {
      "epoch": 8.673951760075635,
      "grad_norm": 14.648839950561523,
      "learning_rate": 6.630351135639058e-06,
      "loss": 0.2587,
      "step": 394500
    },
    {
      "epoch": 8.684945361799432,
      "grad_norm": 25.27048683166504,
      "learning_rate": 6.575383127020076e-06,
      "loss": 0.2718,
      "step": 395000
    },
    {
      "epoch": 8.69593896352323,
      "grad_norm": 11.185770034790039,
      "learning_rate": 6.520415118401091e-06,
      "loss": 0.2637,
      "step": 395500
    },
    {
      "epoch": 8.706932565247026,
      "grad_norm": 15.981945037841797,
      "learning_rate": 6.465447109782107e-06,
      "loss": 0.2715,
      "step": 396000
    },
    {
      "epoch": 8.717926166970823,
      "grad_norm": 10.880187034606934,
      "learning_rate": 6.4104791011631235e-06,
      "loss": 0.2526,
      "step": 396500
    },
    {
      "epoch": 8.72891976869462,
      "grad_norm": 0.20245014131069183,
      "learning_rate": 6.3555110925441394e-06,
      "loss": 0.2747,
      "step": 397000
    },
    {
      "epoch": 8.739913370418417,
      "grad_norm": 7.967769145965576,
      "learning_rate": 6.300543083925156e-06,
      "loss": 0.2787,
      "step": 397500
    },
    {
      "epoch": 8.750906972142213,
      "grad_norm": 9.728178977966309,
      "learning_rate": 6.245575075306172e-06,
      "loss": 0.2438,
      "step": 398000
    },
    {
      "epoch": 8.76190057386601,
      "grad_norm": 14.298083305358887,
      "learning_rate": 6.190607066687188e-06,
      "loss": 0.2768,
      "step": 398500
    },
    {
      "epoch": 8.772894175589807,
      "grad_norm": 1.6082786321640015,
      "learning_rate": 6.135639058068204e-06,
      "loss": 0.2591,
      "step": 399000
    },
    {
      "epoch": 8.783887777313604,
      "grad_norm": 1.0214921236038208,
      "learning_rate": 6.080671049449221e-06,
      "loss": 0.2753,
      "step": 399500
    },
    {
      "epoch": 8.7948813790374,
      "grad_norm": 0.5241511464118958,
      "learning_rate": 6.025703040830237e-06,
      "loss": 0.2518,
      "step": 400000
    },
    {
      "epoch": 8.805874980761198,
      "grad_norm": 10.94429874420166,
      "learning_rate": 5.970735032211254e-06,
      "loss": 0.2571,
      "step": 400500
    },
    {
      "epoch": 8.816868582484993,
      "grad_norm": 19.452632904052734,
      "learning_rate": 5.91576702359227e-06,
      "loss": 0.2531,
      "step": 401000
    },
    {
      "epoch": 8.82786218420879,
      "grad_norm": 8.710125923156738,
      "learning_rate": 5.860799014973286e-06,
      "loss": 0.2729,
      "step": 401500
    },
    {
      "epoch": 8.838855785932587,
      "grad_norm": 14.326274871826172,
      "learning_rate": 5.8058310063543024e-06,
      "loss": 0.2514,
      "step": 402000
    },
    {
      "epoch": 8.849849387656384,
      "grad_norm": 15.43450927734375,
      "learning_rate": 5.750862997735318e-06,
      "loss": 0.2638,
      "step": 402500
    },
    {
      "epoch": 8.860842989380181,
      "grad_norm": 0.3155185282230377,
      "learning_rate": 5.695894989116335e-06,
      "loss": 0.2789,
      "step": 403000
    },
    {
      "epoch": 8.871836591103978,
      "grad_norm": 0.1888679563999176,
      "learning_rate": 5.64092698049735e-06,
      "loss": 0.2589,
      "step": 403500
    },
    {
      "epoch": 8.882830192827774,
      "grad_norm": 12.6144380569458,
      "learning_rate": 5.585958971878367e-06,
      "loss": 0.2556,
      "step": 404000
    },
    {
      "epoch": 8.89382379455157,
      "grad_norm": 15.142207145690918,
      "learning_rate": 5.530990963259383e-06,
      "loss": 0.2843,
      "step": 404500
    },
    {
      "epoch": 8.904817396275368,
      "grad_norm": 12.055487632751465,
      "learning_rate": 5.476022954640399e-06,
      "loss": 0.2491,
      "step": 405000
    },
    {
      "epoch": 8.915810997999165,
      "grad_norm": 12.130145072937012,
      "learning_rate": 5.421054946021416e-06,
      "loss": 0.2663,
      "step": 405500
    },
    {
      "epoch": 8.926804599722962,
      "grad_norm": 2.9430062770843506,
      "learning_rate": 5.366086937402432e-06,
      "loss": 0.243,
      "step": 406000
    },
    {
      "epoch": 8.937798201446759,
      "grad_norm": 23.953420639038086,
      "learning_rate": 5.311118928783449e-06,
      "loss": 0.255,
      "step": 406500
    },
    {
      "epoch": 8.948791803170554,
      "grad_norm": 0.1688920259475708,
      "learning_rate": 5.256150920164465e-06,
      "loss": 0.2478,
      "step": 407000
    },
    {
      "epoch": 8.959785404894351,
      "grad_norm": 12.807400703430176,
      "learning_rate": 5.201182911545481e-06,
      "loss": 0.2655,
      "step": 407500
    },
    {
      "epoch": 8.970779006618148,
      "grad_norm": 14.333030700683594,
      "learning_rate": 5.146214902926497e-06,
      "loss": 0.2852,
      "step": 408000
    },
    {
      "epoch": 8.981772608341945,
      "grad_norm": 0.6623417139053345,
      "learning_rate": 5.091246894307513e-06,
      "loss": 0.2499,
      "step": 408500
    },
    {
      "epoch": 8.992766210065742,
      "grad_norm": 15.866168975830078,
      "learning_rate": 5.036278885688529e-06,
      "loss": 0.2652,
      "step": 409000
    },
    {
      "epoch": 9.00375981178954,
      "grad_norm": 7.398312091827393,
      "learning_rate": 4.981310877069545e-06,
      "loss": 0.2893,
      "step": 409500
    },
    {
      "epoch": 9.014753413513334,
      "grad_norm": 7.28401517868042,
      "learning_rate": 4.926342868450562e-06,
      "loss": 0.2516,
      "step": 410000
    },
    {
      "epoch": 9.025747015237132,
      "grad_norm": 0.18914669752120972,
      "learning_rate": 4.871374859831578e-06,
      "loss": 0.245,
      "step": 410500
    },
    {
      "epoch": 9.036740616960929,
      "grad_norm": 21.70296859741211,
      "learning_rate": 4.816406851212595e-06,
      "loss": 0.2787,
      "step": 411000
    },
    {
      "epoch": 9.047734218684726,
      "grad_norm": 4.975039482116699,
      "learning_rate": 4.761438842593611e-06,
      "loss": 0.245,
      "step": 411500
    },
    {
      "epoch": 9.058727820408523,
      "grad_norm": 13.624786376953125,
      "learning_rate": 4.706470833974628e-06,
      "loss": 0.2413,
      "step": 412000
    },
    {
      "epoch": 9.06972142213232,
      "grad_norm": 10.832953453063965,
      "learning_rate": 4.6515028253556436e-06,
      "loss": 0.242,
      "step": 412500
    },
    {
      "epoch": 9.080715023856115,
      "grad_norm": 0.6794453263282776,
      "learning_rate": 4.5965348167366595e-06,
      "loss": 0.238,
      "step": 413000
    },
    {
      "epoch": 9.091708625579912,
      "grad_norm": 11.204073905944824,
      "learning_rate": 4.5415668081176755e-06,
      "loss": 0.2757,
      "step": 413500
    },
    {
      "epoch": 9.102702227303709,
      "grad_norm": 0.7741495370864868,
      "learning_rate": 4.4865987994986914e-06,
      "loss": 0.2594,
      "step": 414000
    },
    {
      "epoch": 9.113695829027506,
      "grad_norm": 0.5860830545425415,
      "learning_rate": 4.431630790879708e-06,
      "loss": 0.234,
      "step": 414500
    },
    {
      "epoch": 9.124689430751303,
      "grad_norm": 2.4277052879333496,
      "learning_rate": 4.376662782260724e-06,
      "loss": 0.27,
      "step": 415000
    },
    {
      "epoch": 9.1356830324751,
      "grad_norm": 13.73658275604248,
      "learning_rate": 4.321694773641741e-06,
      "loss": 0.2738,
      "step": 415500
    },
    {
      "epoch": 9.146676634198895,
      "grad_norm": 0.6949929594993591,
      "learning_rate": 4.266726765022757e-06,
      "loss": 0.2553,
      "step": 416000
    },
    {
      "epoch": 9.157670235922692,
      "grad_norm": 1.9352763891220093,
      "learning_rate": 4.211758756403773e-06,
      "loss": 0.256,
      "step": 416500
    },
    {
      "epoch": 9.16866383764649,
      "grad_norm": 26.07308006286621,
      "learning_rate": 4.15679074778479e-06,
      "loss": 0.2523,
      "step": 417000
    },
    {
      "epoch": 9.179657439370287,
      "grad_norm": 9.709150314331055,
      "learning_rate": 4.101822739165806e-06,
      "loss": 0.25,
      "step": 417500
    },
    {
      "epoch": 9.190651041094084,
      "grad_norm": 6.524566173553467,
      "learning_rate": 4.0468547305468225e-06,
      "loss": 0.2748,
      "step": 418000
    },
    {
      "epoch": 9.20164464281788,
      "grad_norm": 8.793963432312012,
      "learning_rate": 3.991886721927838e-06,
      "loss": 0.2601,
      "step": 418500
    },
    {
      "epoch": 9.212638244541676,
      "grad_norm": 1.8572702407836914,
      "learning_rate": 3.9369187133088544e-06,
      "loss": 0.2529,
      "step": 419000
    },
    {
      "epoch": 9.223631846265473,
      "grad_norm": 7.987642765045166,
      "learning_rate": 3.88195070468987e-06,
      "loss": 0.2595,
      "step": 419500
    },
    {
      "epoch": 9.23462544798927,
      "grad_norm": 5.203607082366943,
      "learning_rate": 3.826982696070887e-06,
      "loss": 0.2694,
      "step": 420000
    },
    {
      "epoch": 9.245619049713067,
      "grad_norm": 7.948923110961914,
      "learning_rate": 3.772014687451903e-06,
      "loss": 0.2643,
      "step": 420500
    },
    {
      "epoch": 9.256612651436864,
      "grad_norm": 0.587200403213501,
      "learning_rate": 3.7170466788329195e-06,
      "loss": 0.2786,
      "step": 421000
    },
    {
      "epoch": 9.267606253160661,
      "grad_norm": 0.30654650926589966,
      "learning_rate": 3.662078670213936e-06,
      "loss": 0.2554,
      "step": 421500
    },
    {
      "epoch": 9.278599854884458,
      "grad_norm": 7.468478202819824,
      "learning_rate": 3.607110661594952e-06,
      "loss": 0.2542,
      "step": 422000
    },
    {
      "epoch": 9.289593456608253,
      "grad_norm": 0.7695357203483582,
      "learning_rate": 3.5521426529759683e-06,
      "loss": 0.2443,
      "step": 422500
    },
    {
      "epoch": 9.30058705833205,
      "grad_norm": 3.390495777130127,
      "learning_rate": 3.4971746443569847e-06,
      "loss": 0.2612,
      "step": 423000
    },
    {
      "epoch": 9.311580660055848,
      "grad_norm": 0.561860978603363,
      "learning_rate": 3.442206635738e-06,
      "loss": 0.2448,
      "step": 423500
    },
    {
      "epoch": 9.322574261779645,
      "grad_norm": 7.57467794418335,
      "learning_rate": 3.3872386271190166e-06,
      "loss": 0.2608,
      "step": 424000
    },
    {
      "epoch": 9.333567863503442,
      "grad_norm": 19.190258026123047,
      "learning_rate": 3.332270618500033e-06,
      "loss": 0.265,
      "step": 424500
    },
    {
      "epoch": 9.344561465227237,
      "grad_norm": 12.931038856506348,
      "learning_rate": 3.2773026098810494e-06,
      "loss": 0.2448,
      "step": 425000
    },
    {
      "epoch": 9.355555066951034,
      "grad_norm": 7.062999725341797,
      "learning_rate": 3.2223346012620657e-06,
      "loss": 0.2494,
      "step": 425500
    },
    {
      "epoch": 9.366548668674831,
      "grad_norm": 20.1601619720459,
      "learning_rate": 3.167366592643082e-06,
      "loss": 0.242,
      "step": 426000
    },
    {
      "epoch": 9.377542270398628,
      "grad_norm": 18.630218505859375,
      "learning_rate": 3.112398584024098e-06,
      "loss": 0.2753,
      "step": 426500
    },
    {
      "epoch": 9.388535872122425,
      "grad_norm": 0.4602026641368866,
      "learning_rate": 3.057430575405114e-06,
      "loss": 0.2523,
      "step": 427000
    },
    {
      "epoch": 9.399529473846222,
      "grad_norm": 17.378894805908203,
      "learning_rate": 3.0024625667861304e-06,
      "loss": 0.2638,
      "step": 427500
    },
    {
      "epoch": 9.410523075570019,
      "grad_norm": 14.14229679107666,
      "learning_rate": 2.947494558167147e-06,
      "loss": 0.2455,
      "step": 428000
    },
    {
      "epoch": 9.421516677293814,
      "grad_norm": 0.22461102902889252,
      "learning_rate": 2.892526549548163e-06,
      "loss": 0.2524,
      "step": 428500
    },
    {
      "epoch": 9.432510279017611,
      "grad_norm": 22.343585968017578,
      "learning_rate": 2.8375585409291796e-06,
      "loss": 0.2813,
      "step": 429000
    },
    {
      "epoch": 9.443503880741408,
      "grad_norm": 1.0507798194885254,
      "learning_rate": 2.782590532310196e-06,
      "loss": 0.2535,
      "step": 429500
    },
    {
      "epoch": 9.454497482465205,
      "grad_norm": 0.336744487285614,
      "learning_rate": 2.727622523691212e-06,
      "loss": 0.2673,
      "step": 430000
    },
    {
      "epoch": 9.465491084189003,
      "grad_norm": 21.647912979125977,
      "learning_rate": 2.672654515072228e-06,
      "loss": 0.2421,
      "step": 430500
    },
    {
      "epoch": 9.4764846859128,
      "grad_norm": 0.5604549646377563,
      "learning_rate": 2.6176865064532443e-06,
      "loss": 0.2447,
      "step": 431000
    },
    {
      "epoch": 9.487478287636595,
      "grad_norm": 0.17843785881996155,
      "learning_rate": 2.5627184978342607e-06,
      "loss": 0.2624,
      "step": 431500
    },
    {
      "epoch": 9.498471889360392,
      "grad_norm": 12.595467567443848,
      "learning_rate": 2.507750489215277e-06,
      "loss": 0.2715,
      "step": 432000
    },
    {
      "epoch": 9.509465491084189,
      "grad_norm": 0.7979206442832947,
      "learning_rate": 2.452782480596293e-06,
      "loss": 0.252,
      "step": 432500
    },
    {
      "epoch": 9.520459092807986,
      "grad_norm": 6.473247528076172,
      "learning_rate": 2.3978144719773094e-06,
      "loss": 0.272,
      "step": 433000
    },
    {
      "epoch": 9.531452694531783,
      "grad_norm": 0.601150393486023,
      "learning_rate": 2.3428464633583258e-06,
      "loss": 0.2688,
      "step": 433500
    },
    {
      "epoch": 9.54244629625558,
      "grad_norm": 0.6473596692085266,
      "learning_rate": 2.2878784547393417e-06,
      "loss": 0.254,
      "step": 434000
    },
    {
      "epoch": 9.553439897979375,
      "grad_norm": 9.328082084655762,
      "learning_rate": 2.232910446120358e-06,
      "loss": 0.2572,
      "step": 434500
    },
    {
      "epoch": 9.564433499703172,
      "grad_norm": 19.867725372314453,
      "learning_rate": 2.177942437501374e-06,
      "loss": 0.2723,
      "step": 435000
    },
    {
      "epoch": 9.57542710142697,
      "grad_norm": 0.5344159603118896,
      "learning_rate": 2.1229744288823905e-06,
      "loss": 0.2597,
      "step": 435500
    },
    {
      "epoch": 9.586420703150766,
      "grad_norm": 0.22184619307518005,
      "learning_rate": 2.068006420263407e-06,
      "loss": 0.2605,
      "step": 436000
    },
    {
      "epoch": 9.597414304874563,
      "grad_norm": 7.330702781677246,
      "learning_rate": 2.0130384116444232e-06,
      "loss": 0.2335,
      "step": 436500
    },
    {
      "epoch": 9.60840790659836,
      "grad_norm": 0.23557710647583008,
      "learning_rate": 1.9580704030254396e-06,
      "loss": 0.2504,
      "step": 437000
    },
    {
      "epoch": 9.619401508322156,
      "grad_norm": 0.9570900201797485,
      "learning_rate": 1.9031023944064554e-06,
      "loss": 0.2513,
      "step": 437500
    },
    {
      "epoch": 9.630395110045953,
      "grad_norm": 14.656542778015137,
      "learning_rate": 1.8481343857874718e-06,
      "loss": 0.2487,
      "step": 438000
    },
    {
      "epoch": 9.64138871176975,
      "grad_norm": 0.20675477385520935,
      "learning_rate": 1.7931663771684881e-06,
      "loss": 0.2432,
      "step": 438500
    },
    {
      "epoch": 9.652382313493547,
      "grad_norm": 17.5664119720459,
      "learning_rate": 1.7381983685495043e-06,
      "loss": 0.2518,
      "step": 439000
    },
    {
      "epoch": 9.663375915217344,
      "grad_norm": 10.994365692138672,
      "learning_rate": 1.6832303599305207e-06,
      "loss": 0.2635,
      "step": 439500
    },
    {
      "epoch": 9.674369516941141,
      "grad_norm": 0.8012085556983948,
      "learning_rate": 1.6282623513115367e-06,
      "loss": 0.2365,
      "step": 440000
    },
    {
      "epoch": 9.685363118664936,
      "grad_norm": 19.041040420532227,
      "learning_rate": 1.573294342692553e-06,
      "loss": 0.2379,
      "step": 440500
    },
    {
      "epoch": 9.696356720388733,
      "grad_norm": 10.620783805847168,
      "learning_rate": 1.5183263340735692e-06,
      "loss": 0.2559,
      "step": 441000
    },
    {
      "epoch": 9.70735032211253,
      "grad_norm": 28.3383731842041,
      "learning_rate": 1.4633583254545854e-06,
      "loss": 0.2516,
      "step": 441500
    },
    {
      "epoch": 9.718343923836327,
      "grad_norm": 13.005993843078613,
      "learning_rate": 1.4083903168356018e-06,
      "loss": 0.2677,
      "step": 442000
    },
    {
      "epoch": 9.729337525560124,
      "grad_norm": 0.4635216295719147,
      "learning_rate": 1.3534223082166182e-06,
      "loss": 0.2766,
      "step": 442500
    },
    {
      "epoch": 9.740331127283921,
      "grad_norm": 0.9398747682571411,
      "learning_rate": 1.2984542995976341e-06,
      "loss": 0.2607,
      "step": 443000
    },
    {
      "epoch": 9.751324729007717,
      "grad_norm": 13.78712272644043,
      "learning_rate": 1.2434862909786505e-06,
      "loss": 0.2636,
      "step": 443500
    },
    {
      "epoch": 9.762318330731514,
      "grad_norm": 14.91170597076416,
      "learning_rate": 1.1885182823596667e-06,
      "loss": 0.2635,
      "step": 444000
    },
    {
      "epoch": 9.77331193245531,
      "grad_norm": 17.369327545166016,
      "learning_rate": 1.133550273740683e-06,
      "loss": 0.2643,
      "step": 444500
    },
    {
      "epoch": 9.784305534179108,
      "grad_norm": 13.338132858276367,
      "learning_rate": 1.0785822651216992e-06,
      "loss": 0.2492,
      "step": 445000
    },
    {
      "epoch": 9.795299135902905,
      "grad_norm": 16.397891998291016,
      "learning_rate": 1.0236142565027154e-06,
      "loss": 0.2587,
      "step": 445500
    },
    {
      "epoch": 9.806292737626702,
      "grad_norm": 9.462088584899902,
      "learning_rate": 9.686462478837318e-07,
      "loss": 0.2552,
      "step": 446000
    },
    {
      "epoch": 9.817286339350497,
      "grad_norm": 0.5656477212905884,
      "learning_rate": 9.136782392647479e-07,
      "loss": 0.2816,
      "step": 446500
    },
    {
      "epoch": 9.828279941074294,
      "grad_norm": 9.203143119812012,
      "learning_rate": 8.587102306457642e-07,
      "loss": 0.2564,
      "step": 447000
    },
    {
      "epoch": 9.839273542798091,
      "grad_norm": 16.470027923583984,
      "learning_rate": 8.037422220267805e-07,
      "loss": 0.2606,
      "step": 447500
    },
    {
      "epoch": 9.850267144521888,
      "grad_norm": 1.0964395999908447,
      "learning_rate": 7.487742134077967e-07,
      "loss": 0.2681,
      "step": 448000
    },
    {
      "epoch": 9.861260746245685,
      "grad_norm": 0.45120954513549805,
      "learning_rate": 6.93806204788813e-07,
      "loss": 0.2665,
      "step": 448500
    },
    {
      "epoch": 9.872254347969482,
      "grad_norm": 13.001272201538086,
      "learning_rate": 6.388381961698291e-07,
      "loss": 0.2523,
      "step": 449000
    },
    {
      "epoch": 9.883247949693278,
      "grad_norm": 16.07122230529785,
      "learning_rate": 5.838701875508454e-07,
      "loss": 0.2508,
      "step": 449500
    },
    {
      "epoch": 9.894241551417075,
      "grad_norm": 8.612627983093262,
      "learning_rate": 5.289021789318616e-07,
      "loss": 0.2348,
      "step": 450000
    },
    {
      "epoch": 9.905235153140872,
      "grad_norm": 15.276042938232422,
      "learning_rate": 4.73934170312878e-07,
      "loss": 0.2359,
      "step": 450500
    },
    {
      "epoch": 9.916228754864669,
      "grad_norm": 28.280094146728516,
      "learning_rate": 4.189661616938942e-07,
      "loss": 0.2479,
      "step": 451000
    },
    {
      "epoch": 9.927222356588466,
      "grad_norm": 0.5250316262245178,
      "learning_rate": 3.6399815307491043e-07,
      "loss": 0.2689,
      "step": 451500
    },
    {
      "epoch": 9.938215958312263,
      "grad_norm": 0.820663571357727,
      "learning_rate": 3.0903014445592666e-07,
      "loss": 0.2472,
      "step": 452000
    },
    {
      "epoch": 9.94920956003606,
      "grad_norm": 15.228693008422852,
      "learning_rate": 2.5406213583694293e-07,
      "loss": 0.2635,
      "step": 452500
    },
    {
      "epoch": 9.960203161759855,
      "grad_norm": 0.7486788034439087,
      "learning_rate": 1.9909412721795913e-07,
      "loss": 0.2444,
      "step": 453000
    },
    {
      "epoch": 9.971196763483652,
      "grad_norm": 33.4185676574707,
      "learning_rate": 1.4412611859897539e-07,
      "loss": 0.2443,
      "step": 453500
    },
    {
      "epoch": 9.98219036520745,
      "grad_norm": 20.74648666381836,
      "learning_rate": 8.915810997999165e-08,
      "loss": 0.2391,
      "step": 454000
    },
    {
      "epoch": 9.993183966931246,
      "grad_norm": 7.60670280456543,
      "learning_rate": 3.419010136100789e-08,
      "loss": 0.263,
      "step": 454500
    },
    {
      "epoch": 10.0,
      "step": 454810,
      "total_flos": 8.493526426368614e+17,
      "train_loss": 0.289228129020475,
      "train_runtime": 70094.8602,
      "train_samples_per_second": 51.908,
      "train_steps_per_second": 6.488
    }
  ],
  "logging_steps": 500,
  "max_steps": 454810,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8.493526426368614e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
