{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 490880,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.010185788787483703,
      "grad_norm": 10.475431442260742,
      "learning_rate": 4.994917291395046e-05,
      "loss": 1.1144,
      "step": 500
    },
    {
      "epoch": 0.020371577574967405,
      "grad_norm": 5.540585517883301,
      "learning_rate": 4.9898243970013045e-05,
      "loss": 1.0663,
      "step": 1000
    },
    {
      "epoch": 0.03055736636245111,
      "grad_norm": 12.996308326721191,
      "learning_rate": 4.984731502607562e-05,
      "loss": 0.91,
      "step": 1500
    },
    {
      "epoch": 0.04074315514993481,
      "grad_norm": 10.069573402404785,
      "learning_rate": 4.97963860821382e-05,
      "loss": 0.7902,
      "step": 2000
    },
    {
      "epoch": 0.050928943937418515,
      "grad_norm": 9.712420463562012,
      "learning_rate": 4.974545713820079e-05,
      "loss": 0.773,
      "step": 2500
    },
    {
      "epoch": 0.06111473272490222,
      "grad_norm": 11.350540161132812,
      "learning_rate": 4.969452819426337e-05,
      "loss": 0.7303,
      "step": 3000
    },
    {
      "epoch": 0.07130052151238592,
      "grad_norm": 11.822952270507812,
      "learning_rate": 4.964359925032595e-05,
      "loss": 0.7182,
      "step": 3500
    },
    {
      "epoch": 0.08148631029986962,
      "grad_norm": 16.433393478393555,
      "learning_rate": 4.959267030638853e-05,
      "loss": 0.6845,
      "step": 4000
    },
    {
      "epoch": 0.09167209908735333,
      "grad_norm": 7.735564708709717,
      "learning_rate": 4.954174136245111e-05,
      "loss": 0.6716,
      "step": 4500
    },
    {
      "epoch": 0.10185788787483703,
      "grad_norm": 4.1606950759887695,
      "learning_rate": 4.949081241851369e-05,
      "loss": 0.6751,
      "step": 5000
    },
    {
      "epoch": 0.11204367666232073,
      "grad_norm": 3.6047027111053467,
      "learning_rate": 4.943988347457628e-05,
      "loss": 0.6448,
      "step": 5500
    },
    {
      "epoch": 0.12222946544980444,
      "grad_norm": 13.506863594055176,
      "learning_rate": 4.938895453063885e-05,
      "loss": 0.6143,
      "step": 6000
    },
    {
      "epoch": 0.13241525423728814,
      "grad_norm": 5.929473400115967,
      "learning_rate": 4.933802558670144e-05,
      "loss": 0.6256,
      "step": 6500
    },
    {
      "epoch": 0.14260104302477183,
      "grad_norm": 5.684669017791748,
      "learning_rate": 4.928709664276402e-05,
      "loss": 0.6553,
      "step": 7000
    },
    {
      "epoch": 0.15278683181225555,
      "grad_norm": 6.73915958404541,
      "learning_rate": 4.92361676988266e-05,
      "loss": 0.6243,
      "step": 7500
    },
    {
      "epoch": 0.16297262059973924,
      "grad_norm": 4.979188919067383,
      "learning_rate": 4.918523875488918e-05,
      "loss": 0.6227,
      "step": 8000
    },
    {
      "epoch": 0.17315840938722293,
      "grad_norm": 4.232501983642578,
      "learning_rate": 4.913430981095176e-05,
      "loss": 0.6016,
      "step": 8500
    },
    {
      "epoch": 0.18334419817470665,
      "grad_norm": 5.746284008026123,
      "learning_rate": 4.908338086701434e-05,
      "loss": 0.6126,
      "step": 9000
    },
    {
      "epoch": 0.19352998696219034,
      "grad_norm": 5.164944648742676,
      "learning_rate": 4.903245192307693e-05,
      "loss": 0.5956,
      "step": 9500
    },
    {
      "epoch": 0.20371577574967406,
      "grad_norm": 7.725281238555908,
      "learning_rate": 4.8981522979139504e-05,
      "loss": 0.6201,
      "step": 10000
    },
    {
      "epoch": 0.21390156453715775,
      "grad_norm": 13.571431159973145,
      "learning_rate": 4.8930594035202085e-05,
      "loss": 0.5878,
      "step": 10500
    },
    {
      "epoch": 0.22408735332464147,
      "grad_norm": 8.05587387084961,
      "learning_rate": 4.887966509126467e-05,
      "loss": 0.5882,
      "step": 11000
    },
    {
      "epoch": 0.23427314211212516,
      "grad_norm": 15.168427467346191,
      "learning_rate": 4.8828736147327253e-05,
      "loss": 0.5887,
      "step": 11500
    },
    {
      "epoch": 0.24445893089960888,
      "grad_norm": 11.225293159484863,
      "learning_rate": 4.877780720338983e-05,
      "loss": 0.5734,
      "step": 12000
    },
    {
      "epoch": 0.25464471968709257,
      "grad_norm": 10.41153335571289,
      "learning_rate": 4.8726878259452415e-05,
      "loss": 0.5794,
      "step": 12500
    },
    {
      "epoch": 0.2648305084745763,
      "grad_norm": 11.126709938049316,
      "learning_rate": 4.8675949315514996e-05,
      "loss": 0.5903,
      "step": 13000
    },
    {
      "epoch": 0.27501629726205995,
      "grad_norm": 16.26884651184082,
      "learning_rate": 4.8625020371577576e-05,
      "loss": 0.5784,
      "step": 13500
    },
    {
      "epoch": 0.28520208604954367,
      "grad_norm": 14.449189186096191,
      "learning_rate": 4.8574091427640164e-05,
      "loss": 0.553,
      "step": 14000
    },
    {
      "epoch": 0.2953878748370274,
      "grad_norm": 14.406533241271973,
      "learning_rate": 4.852316248370274e-05,
      "loss": 0.5516,
      "step": 14500
    },
    {
      "epoch": 0.3055736636245111,
      "grad_norm": 8.185270309448242,
      "learning_rate": 4.847223353976532e-05,
      "loss": 0.5616,
      "step": 15000
    },
    {
      "epoch": 0.31575945241199477,
      "grad_norm": 7.142495155334473,
      "learning_rate": 4.8421304595827906e-05,
      "loss": 0.5541,
      "step": 15500
    },
    {
      "epoch": 0.3259452411994785,
      "grad_norm": 4.571239471435547,
      "learning_rate": 4.837037565189049e-05,
      "loss": 0.5363,
      "step": 16000
    },
    {
      "epoch": 0.3361310299869622,
      "grad_norm": 19.395648956298828,
      "learning_rate": 4.831944670795307e-05,
      "loss": 0.5396,
      "step": 16500
    },
    {
      "epoch": 0.34631681877444587,
      "grad_norm": 7.450521469116211,
      "learning_rate": 4.826851776401565e-05,
      "loss": 0.5476,
      "step": 17000
    },
    {
      "epoch": 0.3565026075619296,
      "grad_norm": 10.77426528930664,
      "learning_rate": 4.821758882007823e-05,
      "loss": 0.5761,
      "step": 17500
    },
    {
      "epoch": 0.3666883963494133,
      "grad_norm": 10.157403945922852,
      "learning_rate": 4.816665987614081e-05,
      "loss": 0.5475,
      "step": 18000
    },
    {
      "epoch": 0.376874185136897,
      "grad_norm": 10.723608016967773,
      "learning_rate": 4.811573093220339e-05,
      "loss": 0.5563,
      "step": 18500
    },
    {
      "epoch": 0.3870599739243807,
      "grad_norm": 9.04412841796875,
      "learning_rate": 4.806480198826597e-05,
      "loss": 0.5257,
      "step": 19000
    },
    {
      "epoch": 0.3972457627118644,
      "grad_norm": 6.544617652893066,
      "learning_rate": 4.801387304432856e-05,
      "loss": 0.5331,
      "step": 19500
    },
    {
      "epoch": 0.4074315514993481,
      "grad_norm": 13.187667846679688,
      "learning_rate": 4.796294410039114e-05,
      "loss": 0.5498,
      "step": 20000
    },
    {
      "epoch": 0.41761734028683184,
      "grad_norm": 5.243826866149902,
      "learning_rate": 4.791201515645371e-05,
      "loss": 0.5403,
      "step": 20500
    },
    {
      "epoch": 0.4278031290743155,
      "grad_norm": 21.13518524169922,
      "learning_rate": 4.78610862125163e-05,
      "loss": 0.5306,
      "step": 21000
    },
    {
      "epoch": 0.4379889178617992,
      "grad_norm": 2.608628511428833,
      "learning_rate": 4.781015726857888e-05,
      "loss": 0.5574,
      "step": 21500
    },
    {
      "epoch": 0.44817470664928294,
      "grad_norm": 14.351607322692871,
      "learning_rate": 4.775922832464146e-05,
      "loss": 0.5361,
      "step": 22000
    },
    {
      "epoch": 0.4583604954367666,
      "grad_norm": 5.289997100830078,
      "learning_rate": 4.770829938070404e-05,
      "loss": 0.5409,
      "step": 22500
    },
    {
      "epoch": 0.4685462842242503,
      "grad_norm": 21.039562225341797,
      "learning_rate": 4.7657370436766624e-05,
      "loss": 0.5329,
      "step": 23000
    },
    {
      "epoch": 0.47873207301173404,
      "grad_norm": 9.185378074645996,
      "learning_rate": 4.7606441492829204e-05,
      "loss": 0.5227,
      "step": 23500
    },
    {
      "epoch": 0.48891786179921776,
      "grad_norm": 8.049551010131836,
      "learning_rate": 4.755551254889179e-05,
      "loss": 0.5407,
      "step": 24000
    },
    {
      "epoch": 0.4991036505867014,
      "grad_norm": 15.214658737182617,
      "learning_rate": 4.750458360495437e-05,
      "loss": 0.531,
      "step": 24500
    },
    {
      "epoch": 0.5092894393741851,
      "grad_norm": 10.583504676818848,
      "learning_rate": 4.7453654661016947e-05,
      "loss": 0.5306,
      "step": 25000
    },
    {
      "epoch": 0.5194752281616688,
      "grad_norm": 9.114989280700684,
      "learning_rate": 4.7402725717079534e-05,
      "loss": 0.5486,
      "step": 25500
    },
    {
      "epoch": 0.5296610169491526,
      "grad_norm": 11.988611221313477,
      "learning_rate": 4.7351796773142115e-05,
      "loss": 0.5209,
      "step": 26000
    },
    {
      "epoch": 0.5398468057366362,
      "grad_norm": 9.966570854187012,
      "learning_rate": 4.7300867829204695e-05,
      "loss": 0.5095,
      "step": 26500
    },
    {
      "epoch": 0.5500325945241199,
      "grad_norm": 10.063673973083496,
      "learning_rate": 4.7249938885267276e-05,
      "loss": 0.5026,
      "step": 27000
    },
    {
      "epoch": 0.5602183833116037,
      "grad_norm": 5.852478504180908,
      "learning_rate": 4.719900994132986e-05,
      "loss": 0.5214,
      "step": 27500
    },
    {
      "epoch": 0.5704041720990873,
      "grad_norm": 5.5751824378967285,
      "learning_rate": 4.714808099739244e-05,
      "loss": 0.5058,
      "step": 28000
    },
    {
      "epoch": 0.5805899608865711,
      "grad_norm": 5.71063232421875,
      "learning_rate": 4.7097152053455025e-05,
      "loss": 0.5037,
      "step": 28500
    },
    {
      "epoch": 0.5907757496740548,
      "grad_norm": 8.122380256652832,
      "learning_rate": 4.70462231095176e-05,
      "loss": 0.5055,
      "step": 29000
    },
    {
      "epoch": 0.6009615384615384,
      "grad_norm": 10.066831588745117,
      "learning_rate": 4.699529416558019e-05,
      "loss": 0.5265,
      "step": 29500
    },
    {
      "epoch": 0.6111473272490222,
      "grad_norm": 10.954999923706055,
      "learning_rate": 4.694436522164277e-05,
      "loss": 0.5233,
      "step": 30000
    },
    {
      "epoch": 0.6213331160365059,
      "grad_norm": 3.436002731323242,
      "learning_rate": 4.689343627770535e-05,
      "loss": 0.5083,
      "step": 30500
    },
    {
      "epoch": 0.6315189048239895,
      "grad_norm": 3.422664165496826,
      "learning_rate": 4.684250733376793e-05,
      "loss": 0.5163,
      "step": 31000
    },
    {
      "epoch": 0.6417046936114733,
      "grad_norm": 7.0055928230285645,
      "learning_rate": 4.679157838983051e-05,
      "loss": 0.5279,
      "step": 31500
    },
    {
      "epoch": 0.651890482398957,
      "grad_norm": 7.282504081726074,
      "learning_rate": 4.674064944589309e-05,
      "loss": 0.5302,
      "step": 32000
    },
    {
      "epoch": 0.6620762711864406,
      "grad_norm": 6.015334129333496,
      "learning_rate": 4.668972050195568e-05,
      "loss": 0.5072,
      "step": 32500
    },
    {
      "epoch": 0.6722620599739244,
      "grad_norm": 12.315130233764648,
      "learning_rate": 4.663879155801826e-05,
      "loss": 0.5301,
      "step": 33000
    },
    {
      "epoch": 0.6824478487614081,
      "grad_norm": 13.684551239013672,
      "learning_rate": 4.658786261408083e-05,
      "loss": 0.5121,
      "step": 33500
    },
    {
      "epoch": 0.6926336375488917,
      "grad_norm": 10.495404243469238,
      "learning_rate": 4.653693367014342e-05,
      "loss": 0.5101,
      "step": 34000
    },
    {
      "epoch": 0.7028194263363755,
      "grad_norm": 1.9285764694213867,
      "learning_rate": 4.6486004726206e-05,
      "loss": 0.5165,
      "step": 34500
    },
    {
      "epoch": 0.7130052151238592,
      "grad_norm": 1.9499346017837524,
      "learning_rate": 4.643507578226858e-05,
      "loss": 0.51,
      "step": 35000
    },
    {
      "epoch": 0.7231910039113429,
      "grad_norm": 5.759884357452393,
      "learning_rate": 4.638414683833116e-05,
      "loss": 0.5177,
      "step": 35500
    },
    {
      "epoch": 0.7333767926988266,
      "grad_norm": 4.556836128234863,
      "learning_rate": 4.633321789439374e-05,
      "loss": 0.5302,
      "step": 36000
    },
    {
      "epoch": 0.7435625814863103,
      "grad_norm": 8.503661155700684,
      "learning_rate": 4.6282288950456324e-05,
      "loss": 0.5126,
      "step": 36500
    },
    {
      "epoch": 0.753748370273794,
      "grad_norm": 11.056185722351074,
      "learning_rate": 4.623136000651891e-05,
      "loss": 0.5216,
      "step": 37000
    },
    {
      "epoch": 0.7639341590612777,
      "grad_norm": 6.060840606689453,
      "learning_rate": 4.6180431062581485e-05,
      "loss": 0.4928,
      "step": 37500
    },
    {
      "epoch": 0.7741199478487614,
      "grad_norm": 13.45090103149414,
      "learning_rate": 4.6129502118644066e-05,
      "loss": 0.4948,
      "step": 38000
    },
    {
      "epoch": 0.7843057366362451,
      "grad_norm": 7.622160911560059,
      "learning_rate": 4.607857317470665e-05,
      "loss": 0.4846,
      "step": 38500
    },
    {
      "epoch": 0.7944915254237288,
      "grad_norm": 11.767451286315918,
      "learning_rate": 4.6027644230769234e-05,
      "loss": 0.5225,
      "step": 39000
    },
    {
      "epoch": 0.8046773142112125,
      "grad_norm": 10.007506370544434,
      "learning_rate": 4.5976715286831815e-05,
      "loss": 0.5064,
      "step": 39500
    },
    {
      "epoch": 0.8148631029986962,
      "grad_norm": 6.652132511138916,
      "learning_rate": 4.5925786342894395e-05,
      "loss": 0.4944,
      "step": 40000
    },
    {
      "epoch": 0.8250488917861799,
      "grad_norm": 10.135337829589844,
      "learning_rate": 4.5874857398956976e-05,
      "loss": 0.5101,
      "step": 40500
    },
    {
      "epoch": 0.8352346805736637,
      "grad_norm": 4.693966865539551,
      "learning_rate": 4.582392845501956e-05,
      "loss": 0.5129,
      "step": 41000
    },
    {
      "epoch": 0.8454204693611473,
      "grad_norm": 9.847211837768555,
      "learning_rate": 4.5772999511082144e-05,
      "loss": 0.4989,
      "step": 41500
    },
    {
      "epoch": 0.855606258148631,
      "grad_norm": 10.47637939453125,
      "learning_rate": 4.572207056714472e-05,
      "loss": 0.4892,
      "step": 42000
    },
    {
      "epoch": 0.8657920469361148,
      "grad_norm": 8.256000518798828,
      "learning_rate": 4.5671141623207306e-05,
      "loss": 0.5013,
      "step": 42500
    },
    {
      "epoch": 0.8759778357235984,
      "grad_norm": 11.574836730957031,
      "learning_rate": 4.5620212679269887e-05,
      "loss": 0.527,
      "step": 43000
    },
    {
      "epoch": 0.8861636245110821,
      "grad_norm": 5.63088846206665,
      "learning_rate": 4.556928373533247e-05,
      "loss": 0.507,
      "step": 43500
    },
    {
      "epoch": 0.8963494132985659,
      "grad_norm": 2.625156879425049,
      "learning_rate": 4.551835479139505e-05,
      "loss": 0.5035,
      "step": 44000
    },
    {
      "epoch": 0.9065352020860495,
      "grad_norm": 4.514748573303223,
      "learning_rate": 4.546742584745763e-05,
      "loss": 0.5266,
      "step": 44500
    },
    {
      "epoch": 0.9167209908735332,
      "grad_norm": 6.986818790435791,
      "learning_rate": 4.541649690352021e-05,
      "loss": 0.4867,
      "step": 45000
    },
    {
      "epoch": 0.926906779661017,
      "grad_norm": 5.505283355712891,
      "learning_rate": 4.53655679595828e-05,
      "loss": 0.503,
      "step": 45500
    },
    {
      "epoch": 0.9370925684485006,
      "grad_norm": 10.526517868041992,
      "learning_rate": 4.531463901564537e-05,
      "loss": 0.4918,
      "step": 46000
    },
    {
      "epoch": 0.9472783572359843,
      "grad_norm": 6.576094627380371,
      "learning_rate": 4.526371007170795e-05,
      "loss": 0.5043,
      "step": 46500
    },
    {
      "epoch": 0.9574641460234681,
      "grad_norm": 3.806121587753296,
      "learning_rate": 4.521278112777054e-05,
      "loss": 0.4955,
      "step": 47000
    },
    {
      "epoch": 0.9676499348109517,
      "grad_norm": 8.922296524047852,
      "learning_rate": 4.516185218383312e-05,
      "loss": 0.4864,
      "step": 47500
    },
    {
      "epoch": 0.9778357235984355,
      "grad_norm": 8.35882568359375,
      "learning_rate": 4.5110923239895694e-05,
      "loss": 0.4818,
      "step": 48000
    },
    {
      "epoch": 0.9880215123859192,
      "grad_norm": 13.39363956451416,
      "learning_rate": 4.505999429595828e-05,
      "loss": 0.4937,
      "step": 48500
    },
    {
      "epoch": 0.9982073011734028,
      "grad_norm": 11.353377342224121,
      "learning_rate": 4.500906535202086e-05,
      "loss": 0.4981,
      "step": 49000
    },
    {
      "epoch": 1.0083930899608866,
      "grad_norm": 11.878575325012207,
      "learning_rate": 4.495813640808344e-05,
      "loss": 0.484,
      "step": 49500
    },
    {
      "epoch": 1.0185788787483703,
      "grad_norm": 6.603085517883301,
      "learning_rate": 4.490720746414603e-05,
      "loss": 0.4756,
      "step": 50000
    },
    {
      "epoch": 1.028764667535854,
      "grad_norm": 7.105685234069824,
      "learning_rate": 4.4856278520208604e-05,
      "loss": 0.4682,
      "step": 50500
    },
    {
      "epoch": 1.0389504563233376,
      "grad_norm": 1.5806610584259033,
      "learning_rate": 4.480534957627119e-05,
      "loss": 0.489,
      "step": 51000
    },
    {
      "epoch": 1.0491362451108215,
      "grad_norm": 5.9800262451171875,
      "learning_rate": 4.475442063233377e-05,
      "loss": 0.4735,
      "step": 51500
    },
    {
      "epoch": 1.0593220338983051,
      "grad_norm": 4.114030838012695,
      "learning_rate": 4.470349168839635e-05,
      "loss": 0.4656,
      "step": 52000
    },
    {
      "epoch": 1.0695078226857888,
      "grad_norm": 9.99636173248291,
      "learning_rate": 4.4652562744458934e-05,
      "loss": 0.4737,
      "step": 52500
    },
    {
      "epoch": 1.0796936114732725,
      "grad_norm": 3.791764736175537,
      "learning_rate": 4.4601633800521515e-05,
      "loss": 0.4895,
      "step": 53000
    },
    {
      "epoch": 1.0898794002607561,
      "grad_norm": 5.436404228210449,
      "learning_rate": 4.4550704856584095e-05,
      "loss": 0.4832,
      "step": 53500
    },
    {
      "epoch": 1.1000651890482398,
      "grad_norm": 16.461843490600586,
      "learning_rate": 4.449977591264668e-05,
      "loss": 0.484,
      "step": 54000
    },
    {
      "epoch": 1.1102509778357237,
      "grad_norm": 10.184579849243164,
      "learning_rate": 4.444884696870926e-05,
      "loss": 0.4719,
      "step": 54500
    },
    {
      "epoch": 1.1204367666232073,
      "grad_norm": 4.281985282897949,
      "learning_rate": 4.439791802477184e-05,
      "loss": 0.4818,
      "step": 55000
    },
    {
      "epoch": 1.130622555410691,
      "grad_norm": 12.305813789367676,
      "learning_rate": 4.4346989080834425e-05,
      "loss": 0.473,
      "step": 55500
    },
    {
      "epoch": 1.1408083441981747,
      "grad_norm": 8.105306625366211,
      "learning_rate": 4.4296060136897006e-05,
      "loss": 0.492,
      "step": 56000
    },
    {
      "epoch": 1.1509941329856583,
      "grad_norm": 16.24725914001465,
      "learning_rate": 4.424513119295958e-05,
      "loss": 0.4714,
      "step": 56500
    },
    {
      "epoch": 1.161179921773142,
      "grad_norm": 11.262846946716309,
      "learning_rate": 4.419420224902217e-05,
      "loss": 0.4915,
      "step": 57000
    },
    {
      "epoch": 1.1713657105606259,
      "grad_norm": 2.7067174911499023,
      "learning_rate": 4.414327330508475e-05,
      "loss": 0.4844,
      "step": 57500
    },
    {
      "epoch": 1.1815514993481095,
      "grad_norm": 8.248831748962402,
      "learning_rate": 4.409234436114733e-05,
      "loss": 0.4826,
      "step": 58000
    },
    {
      "epoch": 1.1917372881355932,
      "grad_norm": 7.511157989501953,
      "learning_rate": 4.4041415417209916e-05,
      "loss": 0.4632,
      "step": 58500
    },
    {
      "epoch": 1.2019230769230769,
      "grad_norm": 9.53864860534668,
      "learning_rate": 4.399048647327249e-05,
      "loss": 0.497,
      "step": 59000
    },
    {
      "epoch": 1.2121088657105605,
      "grad_norm": 7.717372894287109,
      "learning_rate": 4.393955752933507e-05,
      "loss": 0.466,
      "step": 59500
    },
    {
      "epoch": 1.2222946544980444,
      "grad_norm": 9.432421684265137,
      "learning_rate": 4.388862858539766e-05,
      "loss": 0.4755,
      "step": 60000
    },
    {
      "epoch": 1.232480443285528,
      "grad_norm": 8.733996391296387,
      "learning_rate": 4.383769964146024e-05,
      "loss": 0.4783,
      "step": 60500
    },
    {
      "epoch": 1.2426662320730117,
      "grad_norm": 9.8062744140625,
      "learning_rate": 4.378677069752282e-05,
      "loss": 0.4894,
      "step": 61000
    },
    {
      "epoch": 1.2528520208604954,
      "grad_norm": 9.282029151916504,
      "learning_rate": 4.37358417535854e-05,
      "loss": 0.4923,
      "step": 61500
    },
    {
      "epoch": 1.263037809647979,
      "grad_norm": 9.590058326721191,
      "learning_rate": 4.368491280964798e-05,
      "loss": 0.474,
      "step": 62000
    },
    {
      "epoch": 1.2732235984354627,
      "grad_norm": 8.0099458694458,
      "learning_rate": 4.363398386571056e-05,
      "loss": 0.4894,
      "step": 62500
    },
    {
      "epoch": 1.2834093872229466,
      "grad_norm": 7.039783000946045,
      "learning_rate": 4.358305492177314e-05,
      "loss": 0.4753,
      "step": 63000
    },
    {
      "epoch": 1.2935951760104303,
      "grad_norm": 2.6050102710723877,
      "learning_rate": 4.353212597783572e-05,
      "loss": 0.4739,
      "step": 63500
    },
    {
      "epoch": 1.303780964797914,
      "grad_norm": 14.411230087280273,
      "learning_rate": 4.348119703389831e-05,
      "loss": 0.4749,
      "step": 64000
    },
    {
      "epoch": 1.3139667535853976,
      "grad_norm": 8.400032997131348,
      "learning_rate": 4.343026808996089e-05,
      "loss": 0.4687,
      "step": 64500
    },
    {
      "epoch": 1.3241525423728815,
      "grad_norm": 6.9867143630981445,
      "learning_rate": 4.3379339146023466e-05,
      "loss": 0.4647,
      "step": 65000
    },
    {
      "epoch": 1.3343383311603652,
      "grad_norm": 19.63037872314453,
      "learning_rate": 4.332841020208605e-05,
      "loss": 0.4713,
      "step": 65500
    },
    {
      "epoch": 1.3445241199478488,
      "grad_norm": 0.9476502537727356,
      "learning_rate": 4.3277481258148634e-05,
      "loss": 0.4585,
      "step": 66000
    },
    {
      "epoch": 1.3547099087353325,
      "grad_norm": 11.542868614196777,
      "learning_rate": 4.3226552314211215e-05,
      "loss": 0.4878,
      "step": 66500
    },
    {
      "epoch": 1.3648956975228161,
      "grad_norm": 8.694198608398438,
      "learning_rate": 4.3175623370273795e-05,
      "loss": 0.4785,
      "step": 67000
    },
    {
      "epoch": 1.3750814863102998,
      "grad_norm": 7.480368137359619,
      "learning_rate": 4.3124694426336376e-05,
      "loss": 0.4853,
      "step": 67500
    },
    {
      "epoch": 1.3852672750977835,
      "grad_norm": 9.143139839172363,
      "learning_rate": 4.307376548239896e-05,
      "loss": 0.4725,
      "step": 68000
    },
    {
      "epoch": 1.3954530638852674,
      "grad_norm": 6.6910200119018555,
      "learning_rate": 4.3022836538461544e-05,
      "loss": 0.4771,
      "step": 68500
    },
    {
      "epoch": 1.405638852672751,
      "grad_norm": 4.412794589996338,
      "learning_rate": 4.2971907594524125e-05,
      "loss": 0.4732,
      "step": 69000
    },
    {
      "epoch": 1.4158246414602347,
      "grad_norm": 2.0763189792633057,
      "learning_rate": 4.29209786505867e-05,
      "loss": 0.4901,
      "step": 69500
    },
    {
      "epoch": 1.4260104302477183,
      "grad_norm": 6.140751838684082,
      "learning_rate": 4.2870049706649286e-05,
      "loss": 0.4653,
      "step": 70000
    },
    {
      "epoch": 1.436196219035202,
      "grad_norm": 13.497966766357422,
      "learning_rate": 4.281912076271187e-05,
      "loss": 0.4756,
      "step": 70500
    },
    {
      "epoch": 1.4463820078226859,
      "grad_norm": 13.1840181350708,
      "learning_rate": 4.276819181877445e-05,
      "loss": 0.4637,
      "step": 71000
    },
    {
      "epoch": 1.4565677966101696,
      "grad_norm": 7.267779350280762,
      "learning_rate": 4.271726287483703e-05,
      "loss": 0.465,
      "step": 71500
    },
    {
      "epoch": 1.4667535853976532,
      "grad_norm": 8.448637008666992,
      "learning_rate": 4.266633393089961e-05,
      "loss": 0.458,
      "step": 72000
    },
    {
      "epoch": 1.4769393741851369,
      "grad_norm": 15.093948364257812,
      "learning_rate": 4.261540498696219e-05,
      "loss": 0.4834,
      "step": 72500
    },
    {
      "epoch": 1.4871251629726205,
      "grad_norm": 1.5353198051452637,
      "learning_rate": 4.256447604302478e-05,
      "loss": 0.4664,
      "step": 73000
    },
    {
      "epoch": 1.4973109517601042,
      "grad_norm": 1.1907920837402344,
      "learning_rate": 4.251354709908735e-05,
      "loss": 0.4582,
      "step": 73500
    },
    {
      "epoch": 1.5074967405475879,
      "grad_norm": 14.067605018615723,
      "learning_rate": 4.246261815514994e-05,
      "loss": 0.4653,
      "step": 74000
    },
    {
      "epoch": 1.5176825293350718,
      "grad_norm": 7.1303791999816895,
      "learning_rate": 4.241168921121252e-05,
      "loss": 0.4596,
      "step": 74500
    },
    {
      "epoch": 1.5278683181225554,
      "grad_norm": 11.561150550842285,
      "learning_rate": 4.23607602672751e-05,
      "loss": 0.4603,
      "step": 75000
    },
    {
      "epoch": 1.538054106910039,
      "grad_norm": 1.6666969060897827,
      "learning_rate": 4.230983132333768e-05,
      "loss": 0.4608,
      "step": 75500
    },
    {
      "epoch": 1.548239895697523,
      "grad_norm": 7.225660800933838,
      "learning_rate": 4.225890237940026e-05,
      "loss": 0.4891,
      "step": 76000
    },
    {
      "epoch": 1.5584256844850066,
      "grad_norm": 12.556843757629395,
      "learning_rate": 4.220797343546284e-05,
      "loss": 0.4732,
      "step": 76500
    },
    {
      "epoch": 1.5686114732724903,
      "grad_norm": 1.9742499589920044,
      "learning_rate": 4.215704449152543e-05,
      "loss": 0.4563,
      "step": 77000
    },
    {
      "epoch": 1.578797262059974,
      "grad_norm": 4.456070899963379,
      "learning_rate": 4.210611554758801e-05,
      "loss": 0.484,
      "step": 77500
    },
    {
      "epoch": 1.5889830508474576,
      "grad_norm": 3.496981382369995,
      "learning_rate": 4.2055186603650585e-05,
      "loss": 0.4622,
      "step": 78000
    },
    {
      "epoch": 1.5991688396349413,
      "grad_norm": 9.350762367248535,
      "learning_rate": 4.200425765971317e-05,
      "loss": 0.4574,
      "step": 78500
    },
    {
      "epoch": 1.609354628422425,
      "grad_norm": 7.188249588012695,
      "learning_rate": 4.195332871577575e-05,
      "loss": 0.4729,
      "step": 79000
    },
    {
      "epoch": 1.6195404172099086,
      "grad_norm": 7.842177391052246,
      "learning_rate": 4.1902399771838334e-05,
      "loss": 0.4584,
      "step": 79500
    },
    {
      "epoch": 1.6297262059973925,
      "grad_norm": 8.867236137390137,
      "learning_rate": 4.1851470827900914e-05,
      "loss": 0.4621,
      "step": 80000
    },
    {
      "epoch": 1.6399119947848761,
      "grad_norm": 13.031272888183594,
      "learning_rate": 4.1800541883963495e-05,
      "loss": 0.4783,
      "step": 80500
    },
    {
      "epoch": 1.6500977835723598,
      "grad_norm": 8.964146614074707,
      "learning_rate": 4.1749612940026076e-05,
      "loss": 0.4462,
      "step": 81000
    },
    {
      "epoch": 1.6602835723598437,
      "grad_norm": 10.317181587219238,
      "learning_rate": 4.169868399608866e-05,
      "loss": 0.4519,
      "step": 81500
    },
    {
      "epoch": 1.6704693611473274,
      "grad_norm": 7.976959705352783,
      "learning_rate": 4.164775505215124e-05,
      "loss": 0.4688,
      "step": 82000
    },
    {
      "epoch": 1.680655149934811,
      "grad_norm": 10.943944931030273,
      "learning_rate": 4.159682610821382e-05,
      "loss": 0.4599,
      "step": 82500
    },
    {
      "epoch": 1.6908409387222947,
      "grad_norm": 6.4088592529296875,
      "learning_rate": 4.1545897164276406e-05,
      "loss": 0.4615,
      "step": 83000
    },
    {
      "epoch": 1.7010267275097783,
      "grad_norm": 14.149932861328125,
      "learning_rate": 4.1494968220338986e-05,
      "loss": 0.4641,
      "step": 83500
    },
    {
      "epoch": 1.711212516297262,
      "grad_norm": 11.732865333557129,
      "learning_rate": 4.144403927640157e-05,
      "loss": 0.4492,
      "step": 84000
    },
    {
      "epoch": 1.7213983050847457,
      "grad_norm": 6.667607307434082,
      "learning_rate": 4.139311033246415e-05,
      "loss": 0.4688,
      "step": 84500
    },
    {
      "epoch": 1.7315840938722293,
      "grad_norm": 4.316483497619629,
      "learning_rate": 4.134218138852673e-05,
      "loss": 0.4663,
      "step": 85000
    },
    {
      "epoch": 1.7417698826597132,
      "grad_norm": 2.486891031265259,
      "learning_rate": 4.129125244458931e-05,
      "loss": 0.4644,
      "step": 85500
    },
    {
      "epoch": 1.7519556714471969,
      "grad_norm": 9.355844497680664,
      "learning_rate": 4.12403235006519e-05,
      "loss": 0.4404,
      "step": 86000
    },
    {
      "epoch": 1.7621414602346805,
      "grad_norm": 6.535762786865234,
      "learning_rate": 4.118939455671447e-05,
      "loss": 0.4723,
      "step": 86500
    },
    {
      "epoch": 1.7723272490221644,
      "grad_norm": 13.144072532653809,
      "learning_rate": 4.113846561277706e-05,
      "loss": 0.46,
      "step": 87000
    },
    {
      "epoch": 1.782513037809648,
      "grad_norm": 9.606925010681152,
      "learning_rate": 4.108753666883964e-05,
      "loss": 0.4505,
      "step": 87500
    },
    {
      "epoch": 1.7926988265971318,
      "grad_norm": 9.283092498779297,
      "learning_rate": 4.103660772490222e-05,
      "loss": 0.4367,
      "step": 88000
    },
    {
      "epoch": 1.8028846153846154,
      "grad_norm": 7.863165378570557,
      "learning_rate": 4.09856787809648e-05,
      "loss": 0.4828,
      "step": 88500
    },
    {
      "epoch": 1.813070404172099,
      "grad_norm": 5.150698661804199,
      "learning_rate": 4.093474983702738e-05,
      "loss": 0.4621,
      "step": 89000
    },
    {
      "epoch": 1.8232561929595827,
      "grad_norm": 6.663280010223389,
      "learning_rate": 4.088382089308996e-05,
      "loss": 0.4444,
      "step": 89500
    },
    {
      "epoch": 1.8334419817470664,
      "grad_norm": 10.727579116821289,
      "learning_rate": 4.083289194915255e-05,
      "loss": 0.4813,
      "step": 90000
    },
    {
      "epoch": 1.84362777053455,
      "grad_norm": 9.25354290008545,
      "learning_rate": 4.078196300521512e-05,
      "loss": 0.4475,
      "step": 90500
    },
    {
      "epoch": 1.8538135593220337,
      "grad_norm": 4.825973033905029,
      "learning_rate": 4.0731034061277704e-05,
      "loss": 0.4594,
      "step": 91000
    },
    {
      "epoch": 1.8639993481095176,
      "grad_norm": 12.907291412353516,
      "learning_rate": 4.068010511734029e-05,
      "loss": 0.4379,
      "step": 91500
    },
    {
      "epoch": 1.8741851368970013,
      "grad_norm": 6.000626087188721,
      "learning_rate": 4.062917617340287e-05,
      "loss": 0.4698,
      "step": 92000
    },
    {
      "epoch": 1.8843709256844852,
      "grad_norm": 10.602869987487793,
      "learning_rate": 4.0578247229465446e-05,
      "loss": 0.4831,
      "step": 92500
    },
    {
      "epoch": 1.8945567144719688,
      "grad_norm": 10.815075874328613,
      "learning_rate": 4.0527318285528034e-05,
      "loss": 0.4416,
      "step": 93000
    },
    {
      "epoch": 1.9047425032594525,
      "grad_norm": 8.391493797302246,
      "learning_rate": 4.0476389341590614e-05,
      "loss": 0.4619,
      "step": 93500
    },
    {
      "epoch": 1.9149282920469362,
      "grad_norm": 3.1200194358825684,
      "learning_rate": 4.0425460397653195e-05,
      "loss": 0.462,
      "step": 94000
    },
    {
      "epoch": 1.9251140808344198,
      "grad_norm": 4.883077621459961,
      "learning_rate": 4.037453145371578e-05,
      "loss": 0.4565,
      "step": 94500
    },
    {
      "epoch": 1.9352998696219035,
      "grad_norm": 7.846095085144043,
      "learning_rate": 4.0323602509778357e-05,
      "loss": 0.4805,
      "step": 95000
    },
    {
      "epoch": 1.9454856584093871,
      "grad_norm": 10.598714828491211,
      "learning_rate": 4.027267356584094e-05,
      "loss": 0.4511,
      "step": 95500
    },
    {
      "epoch": 1.9556714471968708,
      "grad_norm": 2.1500165462493896,
      "learning_rate": 4.0221744621903525e-05,
      "loss": 0.4652,
      "step": 96000
    },
    {
      "epoch": 1.9658572359843545,
      "grad_norm": 13.387860298156738,
      "learning_rate": 4.0170815677966105e-05,
      "loss": 0.4293,
      "step": 96500
    },
    {
      "epoch": 1.9760430247718384,
      "grad_norm": 10.594857215881348,
      "learning_rate": 4.0119886734028686e-05,
      "loss": 0.4501,
      "step": 97000
    },
    {
      "epoch": 1.986228813559322,
      "grad_norm": 9.59248161315918,
      "learning_rate": 4.006895779009127e-05,
      "loss": 0.46,
      "step": 97500
    },
    {
      "epoch": 1.996414602346806,
      "grad_norm": 1.1380144357681274,
      "learning_rate": 4.001802884615385e-05,
      "loss": 0.4478,
      "step": 98000
    },
    {
      "epoch": 2.0066003911342896,
      "grad_norm": 3.4055325984954834,
      "learning_rate": 3.996709990221643e-05,
      "loss": 0.4491,
      "step": 98500
    },
    {
      "epoch": 2.0167861799217732,
      "grad_norm": 9.17263412475586,
      "learning_rate": 3.991617095827901e-05,
      "loss": 0.4409,
      "step": 99000
    },
    {
      "epoch": 2.026971968709257,
      "grad_norm": 10.357195854187012,
      "learning_rate": 3.986524201434159e-05,
      "loss": 0.454,
      "step": 99500
    },
    {
      "epoch": 2.0371577574967406,
      "grad_norm": 7.706973552703857,
      "learning_rate": 3.981431307040418e-05,
      "loss": 0.4501,
      "step": 100000
    },
    {
      "epoch": 2.047343546284224,
      "grad_norm": 15.173712730407715,
      "learning_rate": 3.976338412646676e-05,
      "loss": 0.4333,
      "step": 100500
    },
    {
      "epoch": 2.057529335071708,
      "grad_norm": 9.448700904846191,
      "learning_rate": 3.971245518252933e-05,
      "loss": 0.4105,
      "step": 101000
    },
    {
      "epoch": 2.0677151238591915,
      "grad_norm": 2.6614813804626465,
      "learning_rate": 3.966152623859192e-05,
      "loss": 0.4448,
      "step": 101500
    },
    {
      "epoch": 2.077900912646675,
      "grad_norm": 10.785887718200684,
      "learning_rate": 3.96105972946545e-05,
      "loss": 0.4473,
      "step": 102000
    },
    {
      "epoch": 2.088086701434159,
      "grad_norm": 6.077980041503906,
      "learning_rate": 3.955966835071708e-05,
      "loss": 0.4371,
      "step": 102500
    },
    {
      "epoch": 2.098272490221643,
      "grad_norm": 3.2956368923187256,
      "learning_rate": 3.950873940677967e-05,
      "loss": 0.4429,
      "step": 103000
    },
    {
      "epoch": 2.1084582790091266,
      "grad_norm": 3.3227765560150146,
      "learning_rate": 3.945781046284224e-05,
      "loss": 0.4414,
      "step": 103500
    },
    {
      "epoch": 2.1186440677966103,
      "grad_norm": 9.704853057861328,
      "learning_rate": 3.940688151890482e-05,
      "loss": 0.4427,
      "step": 104000
    },
    {
      "epoch": 2.128829856584094,
      "grad_norm": 9.74812126159668,
      "learning_rate": 3.935595257496741e-05,
      "loss": 0.4539,
      "step": 104500
    },
    {
      "epoch": 2.1390156453715776,
      "grad_norm": 4.709912300109863,
      "learning_rate": 3.930502363102999e-05,
      "loss": 0.4437,
      "step": 105000
    },
    {
      "epoch": 2.1492014341590613,
      "grad_norm": 6.584346294403076,
      "learning_rate": 3.9254094687092565e-05,
      "loss": 0.4542,
      "step": 105500
    },
    {
      "epoch": 2.159387222946545,
      "grad_norm": 8.108885765075684,
      "learning_rate": 3.920316574315515e-05,
      "loss": 0.4591,
      "step": 106000
    },
    {
      "epoch": 2.1695730117340286,
      "grad_norm": 7.468087673187256,
      "learning_rate": 3.9152236799217734e-05,
      "loss": 0.4376,
      "step": 106500
    },
    {
      "epoch": 2.1797588005215123,
      "grad_norm": 5.365513801574707,
      "learning_rate": 3.9101307855280314e-05,
      "loss": 0.4183,
      "step": 107000
    },
    {
      "epoch": 2.189944589308996,
      "grad_norm": 10.418703079223633,
      "learning_rate": 3.9050378911342895e-05,
      "loss": 0.4409,
      "step": 107500
    },
    {
      "epoch": 2.2001303780964796,
      "grad_norm": 2.8520963191986084,
      "learning_rate": 3.8999449967405476e-05,
      "loss": 0.4339,
      "step": 108000
    },
    {
      "epoch": 2.2103161668839633,
      "grad_norm": 5.2922821044921875,
      "learning_rate": 3.8948521023468056e-05,
      "loss": 0.4552,
      "step": 108500
    },
    {
      "epoch": 2.2205019556714474,
      "grad_norm": 8.105360984802246,
      "learning_rate": 3.8897592079530644e-05,
      "loss": 0.4448,
      "step": 109000
    },
    {
      "epoch": 2.230687744458931,
      "grad_norm": 10.889514923095703,
      "learning_rate": 3.884666313559322e-05,
      "loss": 0.429,
      "step": 109500
    },
    {
      "epoch": 2.2408735332464147,
      "grad_norm": 17.17924690246582,
      "learning_rate": 3.8795734191655805e-05,
      "loss": 0.4366,
      "step": 110000
    },
    {
      "epoch": 2.2510593220338984,
      "grad_norm": 7.272899627685547,
      "learning_rate": 3.8744805247718386e-05,
      "loss": 0.4466,
      "step": 110500
    },
    {
      "epoch": 2.261245110821382,
      "grad_norm": 6.457223892211914,
      "learning_rate": 3.869387630378097e-05,
      "loss": 0.4616,
      "step": 111000
    },
    {
      "epoch": 2.2714308996088657,
      "grad_norm": 9.001237869262695,
      "learning_rate": 3.864294735984355e-05,
      "loss": 0.4434,
      "step": 111500
    },
    {
      "epoch": 2.2816166883963493,
      "grad_norm": 5.662372589111328,
      "learning_rate": 3.859201841590613e-05,
      "loss": 0.4374,
      "step": 112000
    },
    {
      "epoch": 2.291802477183833,
      "grad_norm": 7.146838188171387,
      "learning_rate": 3.854108947196871e-05,
      "loss": 0.4335,
      "step": 112500
    },
    {
      "epoch": 2.3019882659713167,
      "grad_norm": 5.535552978515625,
      "learning_rate": 3.8490160528031297e-05,
      "loss": 0.4301,
      "step": 113000
    },
    {
      "epoch": 2.3121740547588003,
      "grad_norm": 18.2402400970459,
      "learning_rate": 3.843923158409388e-05,
      "loss": 0.4152,
      "step": 113500
    },
    {
      "epoch": 2.322359843546284,
      "grad_norm": 9.920639991760254,
      "learning_rate": 3.838830264015645e-05,
      "loss": 0.4105,
      "step": 114000
    },
    {
      "epoch": 2.332545632333768,
      "grad_norm": 5.4987053871154785,
      "learning_rate": 3.833737369621904e-05,
      "loss": 0.4356,
      "step": 114500
    },
    {
      "epoch": 2.3427314211212518,
      "grad_norm": 7.673066139221191,
      "learning_rate": 3.828644475228162e-05,
      "loss": 0.4411,
      "step": 115000
    },
    {
      "epoch": 2.3529172099087354,
      "grad_norm": 10.389033317565918,
      "learning_rate": 3.82355158083442e-05,
      "loss": 0.4419,
      "step": 115500
    },
    {
      "epoch": 2.363102998696219,
      "grad_norm": 8.846965789794922,
      "learning_rate": 3.818458686440678e-05,
      "loss": 0.4461,
      "step": 116000
    },
    {
      "epoch": 2.3732887874837028,
      "grad_norm": 9.048911094665527,
      "learning_rate": 3.813365792046936e-05,
      "loss": 0.4316,
      "step": 116500
    },
    {
      "epoch": 2.3834745762711864,
      "grad_norm": 8.568016052246094,
      "learning_rate": 3.808272897653194e-05,
      "loss": 0.4525,
      "step": 117000
    },
    {
      "epoch": 2.39366036505867,
      "grad_norm": 9.883415222167969,
      "learning_rate": 3.803180003259453e-05,
      "loss": 0.4288,
      "step": 117500
    },
    {
      "epoch": 2.4038461538461537,
      "grad_norm": 7.900099277496338,
      "learning_rate": 3.7980871088657104e-05,
      "loss": 0.4283,
      "step": 118000
    },
    {
      "epoch": 2.4140319426336374,
      "grad_norm": 11.5402250289917,
      "learning_rate": 3.7929942144719684e-05,
      "loss": 0.4349,
      "step": 118500
    },
    {
      "epoch": 2.424217731421121,
      "grad_norm": 3.3502047061920166,
      "learning_rate": 3.787901320078227e-05,
      "loss": 0.4356,
      "step": 119000
    },
    {
      "epoch": 2.4344035202086047,
      "grad_norm": 2.5056586265563965,
      "learning_rate": 3.782808425684485e-05,
      "loss": 0.4598,
      "step": 119500
    },
    {
      "epoch": 2.444589308996089,
      "grad_norm": 15.1227388381958,
      "learning_rate": 3.7777155312907433e-05,
      "loss": 0.4398,
      "step": 120000
    },
    {
      "epoch": 2.4547750977835725,
      "grad_norm": 3.846834659576416,
      "learning_rate": 3.7726226368970014e-05,
      "loss": 0.432,
      "step": 120500
    },
    {
      "epoch": 2.464960886571056,
      "grad_norm": 4.544222354888916,
      "learning_rate": 3.7675297425032595e-05,
      "loss": 0.4578,
      "step": 121000
    },
    {
      "epoch": 2.47514667535854,
      "grad_norm": 10.658864974975586,
      "learning_rate": 3.762436848109518e-05,
      "loss": 0.4454,
      "step": 121500
    },
    {
      "epoch": 2.4853324641460235,
      "grad_norm": 6.27044677734375,
      "learning_rate": 3.757343953715776e-05,
      "loss": 0.4248,
      "step": 122000
    },
    {
      "epoch": 2.495518252933507,
      "grad_norm": 8.235798835754395,
      "learning_rate": 3.752251059322034e-05,
      "loss": 0.4455,
      "step": 122500
    },
    {
      "epoch": 2.505704041720991,
      "grad_norm": 16.24420166015625,
      "learning_rate": 3.7471581649282925e-05,
      "loss": 0.4576,
      "step": 123000
    },
    {
      "epoch": 2.5158898305084745,
      "grad_norm": 9.173416137695312,
      "learning_rate": 3.7420652705345505e-05,
      "loss": 0.4222,
      "step": 123500
    },
    {
      "epoch": 2.526075619295958,
      "grad_norm": 14.17966365814209,
      "learning_rate": 3.7369723761408086e-05,
      "loss": 0.4406,
      "step": 124000
    },
    {
      "epoch": 2.5362614080834422,
      "grad_norm": 10.692254066467285,
      "learning_rate": 3.731879481747067e-05,
      "loss": 0.4372,
      "step": 124500
    },
    {
      "epoch": 2.5464471968709255,
      "grad_norm": 5.098020553588867,
      "learning_rate": 3.726786587353325e-05,
      "loss": 0.4543,
      "step": 125000
    },
    {
      "epoch": 2.5566329856584096,
      "grad_norm": 6.57349967956543,
      "learning_rate": 3.721693692959583e-05,
      "loss": 0.4345,
      "step": 125500
    },
    {
      "epoch": 2.5668187744458932,
      "grad_norm": 12.342206954956055,
      "learning_rate": 3.7166007985658416e-05,
      "loss": 0.4486,
      "step": 126000
    },
    {
      "epoch": 2.577004563233377,
      "grad_norm": 15.863073348999023,
      "learning_rate": 3.711507904172099e-05,
      "loss": 0.4352,
      "step": 126500
    },
    {
      "epoch": 2.5871903520208606,
      "grad_norm": 6.8415446281433105,
      "learning_rate": 3.706415009778357e-05,
      "loss": 0.4408,
      "step": 127000
    },
    {
      "epoch": 2.5973761408083442,
      "grad_norm": 3.093221664428711,
      "learning_rate": 3.701322115384616e-05,
      "loss": 0.4315,
      "step": 127500
    },
    {
      "epoch": 2.607561929595828,
      "grad_norm": 10.021106719970703,
      "learning_rate": 3.696229220990874e-05,
      "loss": 0.4344,
      "step": 128000
    },
    {
      "epoch": 2.6177477183833116,
      "grad_norm": 15.986038208007812,
      "learning_rate": 3.691136326597132e-05,
      "loss": 0.4248,
      "step": 128500
    },
    {
      "epoch": 2.627933507170795,
      "grad_norm": 8.431838035583496,
      "learning_rate": 3.68604343220339e-05,
      "loss": 0.4608,
      "step": 129000
    },
    {
      "epoch": 2.638119295958279,
      "grad_norm": 6.408621788024902,
      "learning_rate": 3.680950537809648e-05,
      "loss": 0.4308,
      "step": 129500
    },
    {
      "epoch": 2.648305084745763,
      "grad_norm": 6.741468906402588,
      "learning_rate": 3.675857643415906e-05,
      "loss": 0.4272,
      "step": 130000
    },
    {
      "epoch": 2.658490873533246,
      "grad_norm": 14.159317970275879,
      "learning_rate": 3.670764749022165e-05,
      "loss": 0.4427,
      "step": 130500
    },
    {
      "epoch": 2.6686766623207303,
      "grad_norm": 5.34979248046875,
      "learning_rate": 3.665671854628422e-05,
      "loss": 0.4432,
      "step": 131000
    },
    {
      "epoch": 2.678862451108214,
      "grad_norm": 3.42084002494812,
      "learning_rate": 3.660578960234681e-05,
      "loss": 0.4358,
      "step": 131500
    },
    {
      "epoch": 2.6890482398956976,
      "grad_norm": 6.157237529754639,
      "learning_rate": 3.655486065840939e-05,
      "loss": 0.4414,
      "step": 132000
    },
    {
      "epoch": 2.6992340286831813,
      "grad_norm": 11.403375625610352,
      "learning_rate": 3.650393171447197e-05,
      "loss": 0.4553,
      "step": 132500
    },
    {
      "epoch": 2.709419817470665,
      "grad_norm": 9.441143989562988,
      "learning_rate": 3.645300277053455e-05,
      "loss": 0.4322,
      "step": 133000
    },
    {
      "epoch": 2.7196056062581486,
      "grad_norm": 6.068149566650391,
      "learning_rate": 3.640207382659713e-05,
      "loss": 0.4377,
      "step": 133500
    },
    {
      "epoch": 2.7297913950456323,
      "grad_norm": 11.315947532653809,
      "learning_rate": 3.6351144882659714e-05,
      "loss": 0.4521,
      "step": 134000
    },
    {
      "epoch": 2.739977183833116,
      "grad_norm": 9.823126792907715,
      "learning_rate": 3.63002159387223e-05,
      "loss": 0.4312,
      "step": 134500
    },
    {
      "epoch": 2.7501629726205996,
      "grad_norm": 7.5110931396484375,
      "learning_rate": 3.6249286994784876e-05,
      "loss": 0.4468,
      "step": 135000
    },
    {
      "epoch": 2.7603487614080837,
      "grad_norm": 5.858348369598389,
      "learning_rate": 3.6198358050847456e-05,
      "loss": 0.4417,
      "step": 135500
    },
    {
      "epoch": 2.770534550195567,
      "grad_norm": 9.770501136779785,
      "learning_rate": 3.6147429106910044e-05,
      "loss": 0.4397,
      "step": 136000
    },
    {
      "epoch": 2.780720338983051,
      "grad_norm": 7.528244972229004,
      "learning_rate": 3.6096500162972624e-05,
      "loss": 0.4427,
      "step": 136500
    },
    {
      "epoch": 2.7909061277705347,
      "grad_norm": 6.774832725524902,
      "learning_rate": 3.60455712190352e-05,
      "loss": 0.4342,
      "step": 137000
    },
    {
      "epoch": 2.8010919165580184,
      "grad_norm": 9.918696403503418,
      "learning_rate": 3.5994642275097786e-05,
      "loss": 0.4388,
      "step": 137500
    },
    {
      "epoch": 2.811277705345502,
      "grad_norm": 12.642434120178223,
      "learning_rate": 3.594371333116037e-05,
      "loss": 0.4643,
      "step": 138000
    },
    {
      "epoch": 2.8214634941329857,
      "grad_norm": 4.793460369110107,
      "learning_rate": 3.589278438722295e-05,
      "loss": 0.415,
      "step": 138500
    },
    {
      "epoch": 2.8316492829204694,
      "grad_norm": 10.482735633850098,
      "learning_rate": 3.5841855443285535e-05,
      "loss": 0.4216,
      "step": 139000
    },
    {
      "epoch": 2.841835071707953,
      "grad_norm": 8.955060005187988,
      "learning_rate": 3.579092649934811e-05,
      "loss": 0.4356,
      "step": 139500
    },
    {
      "epoch": 2.8520208604954367,
      "grad_norm": 12.664506912231445,
      "learning_rate": 3.573999755541069e-05,
      "loss": 0.4183,
      "step": 140000
    },
    {
      "epoch": 2.8622066492829203,
      "grad_norm": 1.6878869533538818,
      "learning_rate": 3.568906861147328e-05,
      "loss": 0.4216,
      "step": 140500
    },
    {
      "epoch": 2.872392438070404,
      "grad_norm": 10.881550788879395,
      "learning_rate": 3.563813966753586e-05,
      "loss": 0.4238,
      "step": 141000
    },
    {
      "epoch": 2.8825782268578877,
      "grad_norm": 10.17381763458252,
      "learning_rate": 3.558721072359844e-05,
      "loss": 0.4303,
      "step": 141500
    },
    {
      "epoch": 2.8927640156453718,
      "grad_norm": 12.224323272705078,
      "learning_rate": 3.553628177966102e-05,
      "loss": 0.4311,
      "step": 142000
    },
    {
      "epoch": 2.9029498044328554,
      "grad_norm": 9.934318542480469,
      "learning_rate": 3.54853528357236e-05,
      "loss": 0.4472,
      "step": 142500
    },
    {
      "epoch": 2.913135593220339,
      "grad_norm": 5.659976959228516,
      "learning_rate": 3.543442389178618e-05,
      "loss": 0.4272,
      "step": 143000
    },
    {
      "epoch": 2.9233213820078228,
      "grad_norm": 3.3996052742004395,
      "learning_rate": 3.538349494784876e-05,
      "loss": 0.423,
      "step": 143500
    },
    {
      "epoch": 2.9335071707953064,
      "grad_norm": 7.553640842437744,
      "learning_rate": 3.533256600391134e-05,
      "loss": 0.4431,
      "step": 144000
    },
    {
      "epoch": 2.94369295958279,
      "grad_norm": 6.113999843597412,
      "learning_rate": 3.528163705997393e-05,
      "loss": 0.4253,
      "step": 144500
    },
    {
      "epoch": 2.9538787483702738,
      "grad_norm": 4.024819850921631,
      "learning_rate": 3.523070811603651e-05,
      "loss": 0.4191,
      "step": 145000
    },
    {
      "epoch": 2.9640645371577574,
      "grad_norm": 15.218417167663574,
      "learning_rate": 3.5179779172099084e-05,
      "loss": 0.4293,
      "step": 145500
    },
    {
      "epoch": 2.974250325945241,
      "grad_norm": 8.603485107421875,
      "learning_rate": 3.512885022816167e-05,
      "loss": 0.4378,
      "step": 146000
    },
    {
      "epoch": 2.9844361147327247,
      "grad_norm": 12.291519165039062,
      "learning_rate": 3.507792128422425e-05,
      "loss": 0.4388,
      "step": 146500
    },
    {
      "epoch": 2.9946219035202084,
      "grad_norm": 12.331767082214355,
      "learning_rate": 3.502699234028683e-05,
      "loss": 0.4372,
      "step": 147000
    },
    {
      "epoch": 3.0048076923076925,
      "grad_norm": 7.051860332489014,
      "learning_rate": 3.4976063396349414e-05,
      "loss": 0.4455,
      "step": 147500
    },
    {
      "epoch": 3.014993481095176,
      "grad_norm": 12.004691123962402,
      "learning_rate": 3.4925134452411995e-05,
      "loss": 0.4308,
      "step": 148000
    },
    {
      "epoch": 3.02517926988266,
      "grad_norm": 9.29611873626709,
      "learning_rate": 3.4874205508474575e-05,
      "loss": 0.4245,
      "step": 148500
    },
    {
      "epoch": 3.0353650586701435,
      "grad_norm": 7.048533916473389,
      "learning_rate": 3.482327656453716e-05,
      "loss": 0.4256,
      "step": 149000
    },
    {
      "epoch": 3.045550847457627,
      "grad_norm": 13.386937141418457,
      "learning_rate": 3.4772347620599744e-05,
      "loss": 0.395,
      "step": 149500
    },
    {
      "epoch": 3.055736636245111,
      "grad_norm": 2.125547409057617,
      "learning_rate": 3.472141867666232e-05,
      "loss": 0.4388,
      "step": 150000
    },
    {
      "epoch": 3.0659224250325945,
      "grad_norm": 10.369894981384277,
      "learning_rate": 3.4670489732724905e-05,
      "loss": 0.3992,
      "step": 150500
    },
    {
      "epoch": 3.076108213820078,
      "grad_norm": 12.90390682220459,
      "learning_rate": 3.4619560788787486e-05,
      "loss": 0.4023,
      "step": 151000
    },
    {
      "epoch": 3.086294002607562,
      "grad_norm": 10.387295722961426,
      "learning_rate": 3.4568631844850067e-05,
      "loss": 0.4361,
      "step": 151500
    },
    {
      "epoch": 3.0964797913950455,
      "grad_norm": 10.090109825134277,
      "learning_rate": 3.451770290091265e-05,
      "loss": 0.4111,
      "step": 152000
    },
    {
      "epoch": 3.106665580182529,
      "grad_norm": 7.020503997802734,
      "learning_rate": 3.446677395697523e-05,
      "loss": 0.426,
      "step": 152500
    },
    {
      "epoch": 3.1168513689700132,
      "grad_norm": 7.3259477615356445,
      "learning_rate": 3.441584501303781e-05,
      "loss": 0.4139,
      "step": 153000
    },
    {
      "epoch": 3.127037157757497,
      "grad_norm": 9.185271263122559,
      "learning_rate": 3.4364916069100396e-05,
      "loss": 0.4317,
      "step": 153500
    },
    {
      "epoch": 3.1372229465449806,
      "grad_norm": 4.209028720855713,
      "learning_rate": 3.431398712516297e-05,
      "loss": 0.4406,
      "step": 154000
    },
    {
      "epoch": 3.1474087353324642,
      "grad_norm": 10.067952156066895,
      "learning_rate": 3.426305818122556e-05,
      "loss": 0.4201,
      "step": 154500
    },
    {
      "epoch": 3.157594524119948,
      "grad_norm": 11.773770332336426,
      "learning_rate": 3.421212923728814e-05,
      "loss": 0.4059,
      "step": 155000
    },
    {
      "epoch": 3.1677803129074316,
      "grad_norm": 17.439485549926758,
      "learning_rate": 3.416120029335072e-05,
      "loss": 0.4125,
      "step": 155500
    },
    {
      "epoch": 3.1779661016949152,
      "grad_norm": 2.5573890209198,
      "learning_rate": 3.41102713494133e-05,
      "loss": 0.4127,
      "step": 156000
    },
    {
      "epoch": 3.188151890482399,
      "grad_norm": 14.987594604492188,
      "learning_rate": 3.405934240547588e-05,
      "loss": 0.4187,
      "step": 156500
    },
    {
      "epoch": 3.1983376792698825,
      "grad_norm": 9.163748741149902,
      "learning_rate": 3.400841346153846e-05,
      "loss": 0.3937,
      "step": 157000
    },
    {
      "epoch": 3.208523468057366,
      "grad_norm": 6.859022617340088,
      "learning_rate": 3.395748451760105e-05,
      "loss": 0.4288,
      "step": 157500
    },
    {
      "epoch": 3.21870925684485,
      "grad_norm": 12.577760696411133,
      "learning_rate": 3.390655557366363e-05,
      "loss": 0.4122,
      "step": 158000
    },
    {
      "epoch": 3.228895045632334,
      "grad_norm": 5.988066673278809,
      "learning_rate": 3.3855626629726204e-05,
      "loss": 0.4351,
      "step": 158500
    },
    {
      "epoch": 3.2390808344198176,
      "grad_norm": 13.07882308959961,
      "learning_rate": 3.380469768578879e-05,
      "loss": 0.422,
      "step": 159000
    },
    {
      "epoch": 3.2492666232073013,
      "grad_norm": 11.924285888671875,
      "learning_rate": 3.375376874185137e-05,
      "loss": 0.4,
      "step": 159500
    },
    {
      "epoch": 3.259452411994785,
      "grad_norm": 22.715744018554688,
      "learning_rate": 3.370283979791395e-05,
      "loss": 0.4347,
      "step": 160000
    },
    {
      "epoch": 3.2696382007822686,
      "grad_norm": 14.734285354614258,
      "learning_rate": 3.365191085397653e-05,
      "loss": 0.4241,
      "step": 160500
    },
    {
      "epoch": 3.2798239895697523,
      "grad_norm": 10.598616600036621,
      "learning_rate": 3.3600981910039114e-05,
      "loss": 0.4275,
      "step": 161000
    },
    {
      "epoch": 3.290009778357236,
      "grad_norm": 1.5539884567260742,
      "learning_rate": 3.3550052966101695e-05,
      "loss": 0.4346,
      "step": 161500
    },
    {
      "epoch": 3.3001955671447196,
      "grad_norm": 5.562021732330322,
      "learning_rate": 3.349912402216428e-05,
      "loss": 0.4226,
      "step": 162000
    },
    {
      "epoch": 3.3103813559322033,
      "grad_norm": 7.002508640289307,
      "learning_rate": 3.3448195078226856e-05,
      "loss": 0.4198,
      "step": 162500
    },
    {
      "epoch": 3.320567144719687,
      "grad_norm": 14.064929962158203,
      "learning_rate": 3.339726613428944e-05,
      "loss": 0.4215,
      "step": 163000
    },
    {
      "epoch": 3.3307529335071706,
      "grad_norm": 11.475176811218262,
      "learning_rate": 3.3346337190352024e-05,
      "loss": 0.4052,
      "step": 163500
    },
    {
      "epoch": 3.3409387222946547,
      "grad_norm": 4.39823579788208,
      "learning_rate": 3.3295408246414605e-05,
      "loss": 0.403,
      "step": 164000
    },
    {
      "epoch": 3.3511245110821384,
      "grad_norm": 7.914492130279541,
      "learning_rate": 3.3244479302477186e-05,
      "loss": 0.4244,
      "step": 164500
    },
    {
      "epoch": 3.361310299869622,
      "grad_norm": 8.693334579467773,
      "learning_rate": 3.3193550358539767e-05,
      "loss": 0.4227,
      "step": 165000
    },
    {
      "epoch": 3.3714960886571057,
      "grad_norm": 9.782271385192871,
      "learning_rate": 3.314262141460235e-05,
      "loss": 0.4133,
      "step": 165500
    },
    {
      "epoch": 3.3816818774445894,
      "grad_norm": 7.097237586975098,
      "learning_rate": 3.309169247066493e-05,
      "loss": 0.4508,
      "step": 166000
    },
    {
      "epoch": 3.391867666232073,
      "grad_norm": 27.30677604675293,
      "learning_rate": 3.3040763526727515e-05,
      "loss": 0.3774,
      "step": 166500
    },
    {
      "epoch": 3.4020534550195567,
      "grad_norm": 5.590078353881836,
      "learning_rate": 3.298983458279009e-05,
      "loss": 0.4238,
      "step": 167000
    },
    {
      "epoch": 3.4122392438070404,
      "grad_norm": 13.644962310791016,
      "learning_rate": 3.293890563885268e-05,
      "loss": 0.4047,
      "step": 167500
    },
    {
      "epoch": 3.422425032594524,
      "grad_norm": 6.1023664474487305,
      "learning_rate": 3.288797669491526e-05,
      "loss": 0.4318,
      "step": 168000
    },
    {
      "epoch": 3.4326108213820077,
      "grad_norm": 8.590790748596191,
      "learning_rate": 3.283704775097784e-05,
      "loss": 0.4383,
      "step": 168500
    },
    {
      "epoch": 3.4427966101694913,
      "grad_norm": 7.0074968338012695,
      "learning_rate": 3.278611880704042e-05,
      "loss": 0.4163,
      "step": 169000
    },
    {
      "epoch": 3.4529823989569755,
      "grad_norm": 15.823731422424316,
      "learning_rate": 3.2735189863103e-05,
      "loss": 0.4311,
      "step": 169500
    },
    {
      "epoch": 3.463168187744459,
      "grad_norm": 10.946093559265137,
      "learning_rate": 3.268426091916558e-05,
      "loss": 0.4098,
      "step": 170000
    },
    {
      "epoch": 3.4733539765319428,
      "grad_norm": 10.516510009765625,
      "learning_rate": 3.263333197522817e-05,
      "loss": 0.4232,
      "step": 170500
    },
    {
      "epoch": 3.4835397653194264,
      "grad_norm": 15.36925220489502,
      "learning_rate": 3.258240303129074e-05,
      "loss": 0.4122,
      "step": 171000
    },
    {
      "epoch": 3.49372555410691,
      "grad_norm": 10.13420581817627,
      "learning_rate": 3.253147408735332e-05,
      "loss": 0.4345,
      "step": 171500
    },
    {
      "epoch": 3.5039113428943938,
      "grad_norm": 12.130987167358398,
      "learning_rate": 3.248054514341591e-05,
      "loss": 0.4182,
      "step": 172000
    },
    {
      "epoch": 3.5140971316818774,
      "grad_norm": 6.076929092407227,
      "learning_rate": 3.242961619947849e-05,
      "loss": 0.4339,
      "step": 172500
    },
    {
      "epoch": 3.524282920469361,
      "grad_norm": 7.477870464324951,
      "learning_rate": 3.2378687255541065e-05,
      "loss": 0.4204,
      "step": 173000
    },
    {
      "epoch": 3.5344687092568448,
      "grad_norm": 2.110318899154663,
      "learning_rate": 3.232775831160365e-05,
      "loss": 0.4112,
      "step": 173500
    },
    {
      "epoch": 3.5446544980443284,
      "grad_norm": 15.360536575317383,
      "learning_rate": 3.227682936766623e-05,
      "loss": 0.4183,
      "step": 174000
    },
    {
      "epoch": 3.554840286831812,
      "grad_norm": 17.269716262817383,
      "learning_rate": 3.2225900423728814e-05,
      "loss": 0.4109,
      "step": 174500
    },
    {
      "epoch": 3.565026075619296,
      "grad_norm": 13.931702613830566,
      "learning_rate": 3.21749714797914e-05,
      "loss": 0.4316,
      "step": 175000
    },
    {
      "epoch": 3.5752118644067794,
      "grad_norm": 6.8495774269104,
      "learning_rate": 3.2124042535853975e-05,
      "loss": 0.4056,
      "step": 175500
    },
    {
      "epoch": 3.5853976531942635,
      "grad_norm": 13.693291664123535,
      "learning_rate": 3.2073113591916556e-05,
      "loss": 0.4138,
      "step": 176000
    },
    {
      "epoch": 3.595583441981747,
      "grad_norm": 6.421601295471191,
      "learning_rate": 3.2022184647979144e-05,
      "loss": 0.4289,
      "step": 176500
    },
    {
      "epoch": 3.605769230769231,
      "grad_norm": 3.743605613708496,
      "learning_rate": 3.1971255704041724e-05,
      "loss": 0.4205,
      "step": 177000
    },
    {
      "epoch": 3.6159550195567145,
      "grad_norm": 12.723485946655273,
      "learning_rate": 3.1920326760104305e-05,
      "loss": 0.4284,
      "step": 177500
    },
    {
      "epoch": 3.626140808344198,
      "grad_norm": 11.510444641113281,
      "learning_rate": 3.1869397816166886e-05,
      "loss": 0.4128,
      "step": 178000
    },
    {
      "epoch": 3.636326597131682,
      "grad_norm": 15.777366638183594,
      "learning_rate": 3.1818468872229466e-05,
      "loss": 0.4,
      "step": 178500
    },
    {
      "epoch": 3.6465123859191655,
      "grad_norm": 14.257345199584961,
      "learning_rate": 3.176753992829205e-05,
      "loss": 0.4351,
      "step": 179000
    },
    {
      "epoch": 3.656698174706649,
      "grad_norm": 1.9089229106903076,
      "learning_rate": 3.171661098435463e-05,
      "loss": 0.4307,
      "step": 179500
    },
    {
      "epoch": 3.666883963494133,
      "grad_norm": 8.267412185668945,
      "learning_rate": 3.166568204041721e-05,
      "loss": 0.4268,
      "step": 180000
    },
    {
      "epoch": 3.677069752281617,
      "grad_norm": 4.275253772735596,
      "learning_rate": 3.1614753096479796e-05,
      "loss": 0.4078,
      "step": 180500
    },
    {
      "epoch": 3.6872555410691,
      "grad_norm": 18.60237693786621,
      "learning_rate": 3.156382415254238e-05,
      "loss": 0.4078,
      "step": 181000
    },
    {
      "epoch": 3.6974413298565842,
      "grad_norm": 13.860483169555664,
      "learning_rate": 3.151289520860495e-05,
      "loss": 0.4327,
      "step": 181500
    },
    {
      "epoch": 3.707627118644068,
      "grad_norm": 11.22157096862793,
      "learning_rate": 3.146196626466754e-05,
      "loss": 0.4069,
      "step": 182000
    },
    {
      "epoch": 3.7178129074315516,
      "grad_norm": 1.2389146089553833,
      "learning_rate": 3.141103732073012e-05,
      "loss": 0.4138,
      "step": 182500
    },
    {
      "epoch": 3.7279986962190352,
      "grad_norm": 7.805619716644287,
      "learning_rate": 3.13601083767927e-05,
      "loss": 0.4045,
      "step": 183000
    },
    {
      "epoch": 3.738184485006519,
      "grad_norm": 14.554062843322754,
      "learning_rate": 3.130917943285529e-05,
      "loss": 0.4091,
      "step": 183500
    },
    {
      "epoch": 3.7483702737940026,
      "grad_norm": 18.172637939453125,
      "learning_rate": 3.125825048891786e-05,
      "loss": 0.4269,
      "step": 184000
    },
    {
      "epoch": 3.7585560625814862,
      "grad_norm": 7.693304061889648,
      "learning_rate": 3.120732154498044e-05,
      "loss": 0.4136,
      "step": 184500
    },
    {
      "epoch": 3.76874185136897,
      "grad_norm": 10.11719036102295,
      "learning_rate": 3.115639260104303e-05,
      "loss": 0.3915,
      "step": 185000
    },
    {
      "epoch": 3.7789276401564535,
      "grad_norm": 20.525514602661133,
      "learning_rate": 3.110546365710561e-05,
      "loss": 0.4206,
      "step": 185500
    },
    {
      "epoch": 3.7891134289439377,
      "grad_norm": 10.108946800231934,
      "learning_rate": 3.1054534713168184e-05,
      "loss": 0.4114,
      "step": 186000
    },
    {
      "epoch": 3.799299217731421,
      "grad_norm": 6.667686939239502,
      "learning_rate": 3.100360576923077e-05,
      "loss": 0.4087,
      "step": 186500
    },
    {
      "epoch": 3.809485006518905,
      "grad_norm": 5.93087911605835,
      "learning_rate": 3.095267682529335e-05,
      "loss": 0.4385,
      "step": 187000
    },
    {
      "epoch": 3.8196707953063886,
      "grad_norm": 11.941499710083008,
      "learning_rate": 3.090174788135593e-05,
      "loss": 0.4211,
      "step": 187500
    },
    {
      "epoch": 3.8298565840938723,
      "grad_norm": 10.799093246459961,
      "learning_rate": 3.0850818937418514e-05,
      "loss": 0.4225,
      "step": 188000
    },
    {
      "epoch": 3.840042372881356,
      "grad_norm": 12.425516128540039,
      "learning_rate": 3.0799889993481094e-05,
      "loss": 0.419,
      "step": 188500
    },
    {
      "epoch": 3.8502281616688396,
      "grad_norm": 4.358801364898682,
      "learning_rate": 3.074896104954368e-05,
      "loss": 0.4238,
      "step": 189000
    },
    {
      "epoch": 3.8604139504563233,
      "grad_norm": 6.861616134643555,
      "learning_rate": 3.069803210560626e-05,
      "loss": 0.4321,
      "step": 189500
    },
    {
      "epoch": 3.870599739243807,
      "grad_norm": 9.741515159606934,
      "learning_rate": 3.064710316166884e-05,
      "loss": 0.3962,
      "step": 190000
    },
    {
      "epoch": 3.8807855280312906,
      "grad_norm": 8.144820213317871,
      "learning_rate": 3.0596174217731424e-05,
      "loss": 0.4376,
      "step": 190500
    },
    {
      "epoch": 3.8909713168187743,
      "grad_norm": 11.887240409851074,
      "learning_rate": 3.0545245273794005e-05,
      "loss": 0.4453,
      "step": 191000
    },
    {
      "epoch": 3.9011571056062584,
      "grad_norm": 9.857717514038086,
      "learning_rate": 3.0494316329856586e-05,
      "loss": 0.4106,
      "step": 191500
    },
    {
      "epoch": 3.9113428943937416,
      "grad_norm": 1.6961411237716675,
      "learning_rate": 3.0443387385919163e-05,
      "loss": 0.4325,
      "step": 192000
    },
    {
      "epoch": 3.9215286831812257,
      "grad_norm": 8.024698257446289,
      "learning_rate": 3.0392458441981747e-05,
      "loss": 0.3862,
      "step": 192500
    },
    {
      "epoch": 3.9317144719687094,
      "grad_norm": 12.130078315734863,
      "learning_rate": 3.034152949804433e-05,
      "loss": 0.4136,
      "step": 193000
    },
    {
      "epoch": 3.941900260756193,
      "grad_norm": 6.310614109039307,
      "learning_rate": 3.0290600554106912e-05,
      "loss": 0.4317,
      "step": 193500
    },
    {
      "epoch": 3.9520860495436767,
      "grad_norm": 9.501548767089844,
      "learning_rate": 3.0239671610169496e-05,
      "loss": 0.4153,
      "step": 194000
    },
    {
      "epoch": 3.9622718383311604,
      "grad_norm": 8.072147369384766,
      "learning_rate": 3.0188742666232073e-05,
      "loss": 0.4166,
      "step": 194500
    },
    {
      "epoch": 3.972457627118644,
      "grad_norm": 7.740509510040283,
      "learning_rate": 3.0137813722294654e-05,
      "loss": 0.414,
      "step": 195000
    },
    {
      "epoch": 3.9826434159061277,
      "grad_norm": 16.44202995300293,
      "learning_rate": 3.0086884778357238e-05,
      "loss": 0.4152,
      "step": 195500
    },
    {
      "epoch": 3.9928292046936114,
      "grad_norm": 7.935160160064697,
      "learning_rate": 3.0035955834419822e-05,
      "loss": 0.4423,
      "step": 196000
    },
    {
      "epoch": 4.003014993481095,
      "grad_norm": 15.268577575683594,
      "learning_rate": 2.99850268904824e-05,
      "loss": 0.4024,
      "step": 196500
    },
    {
      "epoch": 4.013200782268579,
      "grad_norm": 2.3403985500335693,
      "learning_rate": 2.993409794654498e-05,
      "loss": 0.4135,
      "step": 197000
    },
    {
      "epoch": 4.023386571056062,
      "grad_norm": 8.477494239807129,
      "learning_rate": 2.9883169002607564e-05,
      "loss": 0.4075,
      "step": 197500
    },
    {
      "epoch": 4.0335723598435465,
      "grad_norm": 1.3470218181610107,
      "learning_rate": 2.9832240058670145e-05,
      "loss": 0.4043,
      "step": 198000
    },
    {
      "epoch": 4.04375814863103,
      "grad_norm": 4.443010330200195,
      "learning_rate": 2.9781311114732723e-05,
      "loss": 0.3934,
      "step": 198500
    },
    {
      "epoch": 4.053943937418514,
      "grad_norm": 15.127839088439941,
      "learning_rate": 2.9730382170795307e-05,
      "loss": 0.3892,
      "step": 199000
    },
    {
      "epoch": 4.064129726205997,
      "grad_norm": 11.799897193908691,
      "learning_rate": 2.967945322685789e-05,
      "loss": 0.4009,
      "step": 199500
    },
    {
      "epoch": 4.074315514993481,
      "grad_norm": 8.103788375854492,
      "learning_rate": 2.962852428292047e-05,
      "loss": 0.3779,
      "step": 200000
    },
    {
      "epoch": 4.084501303780965,
      "grad_norm": 5.966071605682373,
      "learning_rate": 2.957759533898305e-05,
      "loss": 0.3923,
      "step": 200500
    },
    {
      "epoch": 4.094687092568448,
      "grad_norm": 12.02983283996582,
      "learning_rate": 2.9526666395045633e-05,
      "loss": 0.3985,
      "step": 201000
    },
    {
      "epoch": 4.1048728813559325,
      "grad_norm": 15.014695167541504,
      "learning_rate": 2.9475737451108214e-05,
      "loss": 0.3981,
      "step": 201500
    },
    {
      "epoch": 4.115058670143416,
      "grad_norm": 12.245820045471191,
      "learning_rate": 2.9424808507170798e-05,
      "loss": 0.4117,
      "step": 202000
    },
    {
      "epoch": 4.1252444589309,
      "grad_norm": 14.015369415283203,
      "learning_rate": 2.9373879563233382e-05,
      "loss": 0.3902,
      "step": 202500
    },
    {
      "epoch": 4.135430247718383,
      "grad_norm": 2.7434234619140625,
      "learning_rate": 2.932295061929596e-05,
      "loss": 0.3844,
      "step": 203000
    },
    {
      "epoch": 4.145616036505867,
      "grad_norm": 5.364775657653809,
      "learning_rate": 2.927202167535854e-05,
      "loss": 0.3969,
      "step": 203500
    },
    {
      "epoch": 4.15580182529335,
      "grad_norm": 1.6733417510986328,
      "learning_rate": 2.9221092731421124e-05,
      "loss": 0.4125,
      "step": 204000
    },
    {
      "epoch": 4.1659876140808345,
      "grad_norm": 8.307292938232422,
      "learning_rate": 2.9170163787483705e-05,
      "loss": 0.3909,
      "step": 204500
    },
    {
      "epoch": 4.176173402868318,
      "grad_norm": 8.901971817016602,
      "learning_rate": 2.9119234843546282e-05,
      "loss": 0.4029,
      "step": 205000
    },
    {
      "epoch": 4.186359191655802,
      "grad_norm": 7.630775451660156,
      "learning_rate": 2.9068305899608866e-05,
      "loss": 0.3909,
      "step": 205500
    },
    {
      "epoch": 4.196544980443286,
      "grad_norm": 9.992165565490723,
      "learning_rate": 2.901737695567145e-05,
      "loss": 0.4034,
      "step": 206000
    },
    {
      "epoch": 4.206730769230769,
      "grad_norm": 9.522208213806152,
      "learning_rate": 2.896644801173403e-05,
      "loss": 0.4212,
      "step": 206500
    },
    {
      "epoch": 4.216916558018253,
      "grad_norm": 18.379709243774414,
      "learning_rate": 2.891551906779661e-05,
      "loss": 0.4028,
      "step": 207000
    },
    {
      "epoch": 4.2271023468057365,
      "grad_norm": 13.148025512695312,
      "learning_rate": 2.8864590123859193e-05,
      "loss": 0.4057,
      "step": 207500
    },
    {
      "epoch": 4.237288135593221,
      "grad_norm": 11.812298774719238,
      "learning_rate": 2.8813661179921773e-05,
      "loss": 0.4029,
      "step": 208000
    },
    {
      "epoch": 4.247473924380704,
      "grad_norm": 7.595677375793457,
      "learning_rate": 2.8762732235984357e-05,
      "loss": 0.3875,
      "step": 208500
    },
    {
      "epoch": 4.257659713168188,
      "grad_norm": 5.699001312255859,
      "learning_rate": 2.8711803292046935e-05,
      "loss": 0.4128,
      "step": 209000
    },
    {
      "epoch": 4.267845501955671,
      "grad_norm": 7.020178318023682,
      "learning_rate": 2.866087434810952e-05,
      "loss": 0.4049,
      "step": 209500
    },
    {
      "epoch": 4.278031290743155,
      "grad_norm": 15.784027099609375,
      "learning_rate": 2.86099454041721e-05,
      "loss": 0.3993,
      "step": 210000
    },
    {
      "epoch": 4.2882170795306385,
      "grad_norm": 5.318665504455566,
      "learning_rate": 2.8559016460234684e-05,
      "loss": 0.3962,
      "step": 210500
    },
    {
      "epoch": 4.298402868318123,
      "grad_norm": 1.7727038860321045,
      "learning_rate": 2.8508087516297264e-05,
      "loss": 0.4107,
      "step": 211000
    },
    {
      "epoch": 4.308588657105606,
      "grad_norm": 8.197172164916992,
      "learning_rate": 2.8457158572359842e-05,
      "loss": 0.4094,
      "step": 211500
    },
    {
      "epoch": 4.31877444589309,
      "grad_norm": 11.756793022155762,
      "learning_rate": 2.8406229628422426e-05,
      "loss": 0.4087,
      "step": 212000
    },
    {
      "epoch": 4.328960234680574,
      "grad_norm": 11.136591911315918,
      "learning_rate": 2.835530068448501e-05,
      "loss": 0.3913,
      "step": 212500
    },
    {
      "epoch": 4.339146023468057,
      "grad_norm": 7.2593770027160645,
      "learning_rate": 2.830437174054759e-05,
      "loss": 0.4181,
      "step": 213000
    },
    {
      "epoch": 4.349331812255541,
      "grad_norm": 21.70712661743164,
      "learning_rate": 2.8253442796610168e-05,
      "loss": 0.3832,
      "step": 213500
    },
    {
      "epoch": 4.3595176010430245,
      "grad_norm": 9.850152015686035,
      "learning_rate": 2.8202513852672752e-05,
      "loss": 0.4191,
      "step": 214000
    },
    {
      "epoch": 4.369703389830509,
      "grad_norm": 9.704840660095215,
      "learning_rate": 2.8151584908735333e-05,
      "loss": 0.4155,
      "step": 214500
    },
    {
      "epoch": 4.379889178617992,
      "grad_norm": 22.2630672454834,
      "learning_rate": 2.8100655964797917e-05,
      "loss": 0.3883,
      "step": 215000
    },
    {
      "epoch": 4.390074967405476,
      "grad_norm": 7.57332181930542,
      "learning_rate": 2.8049727020860494e-05,
      "loss": 0.4028,
      "step": 215500
    },
    {
      "epoch": 4.400260756192959,
      "grad_norm": 10.435712814331055,
      "learning_rate": 2.799879807692308e-05,
      "loss": 0.4148,
      "step": 216000
    },
    {
      "epoch": 4.410446544980443,
      "grad_norm": 12.748071670532227,
      "learning_rate": 2.794786913298566e-05,
      "loss": 0.4011,
      "step": 216500
    },
    {
      "epoch": 4.4206323337679265,
      "grad_norm": 1.2155042886734009,
      "learning_rate": 2.7896940189048243e-05,
      "loss": 0.3943,
      "step": 217000
    },
    {
      "epoch": 4.430818122555411,
      "grad_norm": 6.084359169006348,
      "learning_rate": 2.784601124511082e-05,
      "loss": 0.3968,
      "step": 217500
    },
    {
      "epoch": 4.441003911342895,
      "grad_norm": 7.690877914428711,
      "learning_rate": 2.77950823011734e-05,
      "loss": 0.4245,
      "step": 218000
    },
    {
      "epoch": 4.451189700130378,
      "grad_norm": 19.815326690673828,
      "learning_rate": 2.7744153357235985e-05,
      "loss": 0.4108,
      "step": 218500
    },
    {
      "epoch": 4.461375488917862,
      "grad_norm": 16.70404052734375,
      "learning_rate": 2.769322441329857e-05,
      "loss": 0.4123,
      "step": 219000
    },
    {
      "epoch": 4.471561277705345,
      "grad_norm": 12.675461769104004,
      "learning_rate": 2.764229546936115e-05,
      "loss": 0.3918,
      "step": 219500
    },
    {
      "epoch": 4.481747066492829,
      "grad_norm": 3.7451791763305664,
      "learning_rate": 2.7591366525423728e-05,
      "loss": 0.4074,
      "step": 220000
    },
    {
      "epoch": 4.491932855280313,
      "grad_norm": 9.781428337097168,
      "learning_rate": 2.7540437581486312e-05,
      "loss": 0.3958,
      "step": 220500
    },
    {
      "epoch": 4.502118644067797,
      "grad_norm": 17.255794525146484,
      "learning_rate": 2.7489508637548896e-05,
      "loss": 0.406,
      "step": 221000
    },
    {
      "epoch": 4.51230443285528,
      "grad_norm": 3.267996311187744,
      "learning_rate": 2.7438579693611477e-05,
      "loss": 0.4061,
      "step": 221500
    },
    {
      "epoch": 4.522490221642764,
      "grad_norm": 14.126673698425293,
      "learning_rate": 2.7387650749674054e-05,
      "loss": 0.3849,
      "step": 222000
    },
    {
      "epoch": 4.532676010430247,
      "grad_norm": 6.542229652404785,
      "learning_rate": 2.7336721805736638e-05,
      "loss": 0.4188,
      "step": 222500
    },
    {
      "epoch": 4.542861799217731,
      "grad_norm": 8.303082466125488,
      "learning_rate": 2.728579286179922e-05,
      "loss": 0.4049,
      "step": 223000
    },
    {
      "epoch": 4.5530475880052155,
      "grad_norm": 11.623039245605469,
      "learning_rate": 2.7234863917861803e-05,
      "loss": 0.3927,
      "step": 223500
    },
    {
      "epoch": 4.563233376792699,
      "grad_norm": 10.908291816711426,
      "learning_rate": 2.718393497392438e-05,
      "loss": 0.4059,
      "step": 224000
    },
    {
      "epoch": 4.573419165580183,
      "grad_norm": 10.593589782714844,
      "learning_rate": 2.713300602998696e-05,
      "loss": 0.4025,
      "step": 224500
    },
    {
      "epoch": 4.583604954367666,
      "grad_norm": 10.071372032165527,
      "learning_rate": 2.7082077086049545e-05,
      "loss": 0.42,
      "step": 225000
    },
    {
      "epoch": 4.59379074315515,
      "grad_norm": 3.7169175148010254,
      "learning_rate": 2.703114814211213e-05,
      "loss": 0.4121,
      "step": 225500
    },
    {
      "epoch": 4.603976531942633,
      "grad_norm": 8.189173698425293,
      "learning_rate": 2.6980219198174706e-05,
      "loss": 0.4122,
      "step": 226000
    },
    {
      "epoch": 4.6141623207301175,
      "grad_norm": 9.167790412902832,
      "learning_rate": 2.6929290254237287e-05,
      "loss": 0.3987,
      "step": 226500
    },
    {
      "epoch": 4.624348109517601,
      "grad_norm": 5.326823711395264,
      "learning_rate": 2.687836131029987e-05,
      "loss": 0.4201,
      "step": 227000
    },
    {
      "epoch": 4.634533898305085,
      "grad_norm": 0.5637549757957458,
      "learning_rate": 2.6827432366362455e-05,
      "loss": 0.4206,
      "step": 227500
    },
    {
      "epoch": 4.644719687092568,
      "grad_norm": 13.590521812438965,
      "learning_rate": 2.6776503422425036e-05,
      "loss": 0.3922,
      "step": 228000
    },
    {
      "epoch": 4.654905475880052,
      "grad_norm": 12.79975700378418,
      "learning_rate": 2.6725574478487614e-05,
      "loss": 0.4132,
      "step": 228500
    },
    {
      "epoch": 4.665091264667536,
      "grad_norm": 11.213377952575684,
      "learning_rate": 2.6674645534550198e-05,
      "loss": 0.4171,
      "step": 229000
    },
    {
      "epoch": 4.675277053455019,
      "grad_norm": 4.151089191436768,
      "learning_rate": 2.662371659061278e-05,
      "loss": 0.42,
      "step": 229500
    },
    {
      "epoch": 4.6854628422425035,
      "grad_norm": 15.0936279296875,
      "learning_rate": 2.6572787646675362e-05,
      "loss": 0.3992,
      "step": 230000
    },
    {
      "epoch": 4.695648631029987,
      "grad_norm": 1.744660496711731,
      "learning_rate": 2.652185870273794e-05,
      "loss": 0.384,
      "step": 230500
    },
    {
      "epoch": 4.705834419817471,
      "grad_norm": 7.735109329223633,
      "learning_rate": 2.6470929758800524e-05,
      "loss": 0.409,
      "step": 231000
    },
    {
      "epoch": 4.716020208604954,
      "grad_norm": 16.68280792236328,
      "learning_rate": 2.6420000814863105e-05,
      "loss": 0.4067,
      "step": 231500
    },
    {
      "epoch": 4.726205997392438,
      "grad_norm": 15.827895164489746,
      "learning_rate": 2.636907187092569e-05,
      "loss": 0.4075,
      "step": 232000
    },
    {
      "epoch": 4.736391786179921,
      "grad_norm": 1.4081910848617554,
      "learning_rate": 2.6318142926988266e-05,
      "loss": 0.4053,
      "step": 232500
    },
    {
      "epoch": 4.7465775749674055,
      "grad_norm": 9.399243354797363,
      "learning_rate": 2.6267213983050847e-05,
      "loss": 0.4133,
      "step": 233000
    },
    {
      "epoch": 4.756763363754889,
      "grad_norm": 10.001861572265625,
      "learning_rate": 2.621628503911343e-05,
      "loss": 0.3963,
      "step": 233500
    },
    {
      "epoch": 4.766949152542373,
      "grad_norm": 13.815892219543457,
      "learning_rate": 2.6165356095176015e-05,
      "loss": 0.4287,
      "step": 234000
    },
    {
      "epoch": 4.777134941329857,
      "grad_norm": 10.177109718322754,
      "learning_rate": 2.6114427151238592e-05,
      "loss": 0.4137,
      "step": 234500
    },
    {
      "epoch": 4.78732073011734,
      "grad_norm": 13.487106323242188,
      "learning_rate": 2.6063498207301173e-05,
      "loss": 0.4105,
      "step": 235000
    },
    {
      "epoch": 4.797506518904824,
      "grad_norm": 13.191375732421875,
      "learning_rate": 2.6012569263363757e-05,
      "loss": 0.3891,
      "step": 235500
    },
    {
      "epoch": 4.8076923076923075,
      "grad_norm": 6.895784854888916,
      "learning_rate": 2.5961640319426338e-05,
      "loss": 0.4156,
      "step": 236000
    },
    {
      "epoch": 4.817878096479792,
      "grad_norm": 3.2143328189849854,
      "learning_rate": 2.5910711375488915e-05,
      "loss": 0.3939,
      "step": 236500
    },
    {
      "epoch": 4.828063885267275,
      "grad_norm": 9.242363929748535,
      "learning_rate": 2.58597824315515e-05,
      "loss": 0.3961,
      "step": 237000
    },
    {
      "epoch": 4.838249674054759,
      "grad_norm": 13.273147583007812,
      "learning_rate": 2.5808853487614084e-05,
      "loss": 0.4123,
      "step": 237500
    },
    {
      "epoch": 4.848435462842242,
      "grad_norm": 7.366537094116211,
      "learning_rate": 2.5757924543676664e-05,
      "loss": 0.4099,
      "step": 238000
    },
    {
      "epoch": 4.858621251629726,
      "grad_norm": 1.60019052028656,
      "learning_rate": 2.570699559973925e-05,
      "loss": 0.391,
      "step": 238500
    },
    {
      "epoch": 4.8688070404172095,
      "grad_norm": 7.997812747955322,
      "learning_rate": 2.5656066655801826e-05,
      "loss": 0.4055,
      "step": 239000
    },
    {
      "epoch": 4.878992829204694,
      "grad_norm": 8.65123462677002,
      "learning_rate": 2.5605137711864406e-05,
      "loss": 0.4183,
      "step": 239500
    },
    {
      "epoch": 4.889178617992178,
      "grad_norm": 3.5895893573760986,
      "learning_rate": 2.555420876792699e-05,
      "loss": 0.4017,
      "step": 240000
    },
    {
      "epoch": 4.899364406779661,
      "grad_norm": 30.85491943359375,
      "learning_rate": 2.5503279823989575e-05,
      "loss": 0.396,
      "step": 240500
    },
    {
      "epoch": 4.909550195567145,
      "grad_norm": 11.34129524230957,
      "learning_rate": 2.5452350880052152e-05,
      "loss": 0.4128,
      "step": 241000
    },
    {
      "epoch": 4.919735984354628,
      "grad_norm": 10.010181427001953,
      "learning_rate": 2.5401421936114733e-05,
      "loss": 0.3926,
      "step": 241500
    },
    {
      "epoch": 4.929921773142112,
      "grad_norm": 21.10512924194336,
      "learning_rate": 2.5350492992177317e-05,
      "loss": 0.4065,
      "step": 242000
    },
    {
      "epoch": 4.9401075619295955,
      "grad_norm": 1.824705958366394,
      "learning_rate": 2.5299564048239898e-05,
      "loss": 0.3899,
      "step": 242500
    },
    {
      "epoch": 4.95029335071708,
      "grad_norm": 4.792509078979492,
      "learning_rate": 2.5248635104302475e-05,
      "loss": 0.3998,
      "step": 243000
    },
    {
      "epoch": 4.960479139504563,
      "grad_norm": 12.82136344909668,
      "learning_rate": 2.519770616036506e-05,
      "loss": 0.39,
      "step": 243500
    },
    {
      "epoch": 4.970664928292047,
      "grad_norm": 8.470712661743164,
      "learning_rate": 2.5146777216427643e-05,
      "loss": 0.4079,
      "step": 244000
    },
    {
      "epoch": 4.98085071707953,
      "grad_norm": 9.464334487915039,
      "learning_rate": 2.5095848272490224e-05,
      "loss": 0.4144,
      "step": 244500
    },
    {
      "epoch": 4.991036505867014,
      "grad_norm": 9.532865524291992,
      "learning_rate": 2.50449193285528e-05,
      "loss": 0.4023,
      "step": 245000
    },
    {
      "epoch": 5.001222294654498,
      "grad_norm": 1.6548304557800293,
      "learning_rate": 2.4993990384615385e-05,
      "loss": 0.4169,
      "step": 245500
    },
    {
      "epoch": 5.011408083441982,
      "grad_norm": 14.136231422424316,
      "learning_rate": 2.4943061440677966e-05,
      "loss": 0.3925,
      "step": 246000
    },
    {
      "epoch": 5.021593872229466,
      "grad_norm": 9.781281471252441,
      "learning_rate": 2.489213249674055e-05,
      "loss": 0.3729,
      "step": 246500
    },
    {
      "epoch": 5.031779661016949,
      "grad_norm": 0.9485311508178711,
      "learning_rate": 2.484120355280313e-05,
      "loss": 0.3873,
      "step": 247000
    },
    {
      "epoch": 5.041965449804433,
      "grad_norm": 31.13900375366211,
      "learning_rate": 2.479027460886571e-05,
      "loss": 0.3882,
      "step": 247500
    },
    {
      "epoch": 5.052151238591916,
      "grad_norm": 9.825727462768555,
      "learning_rate": 2.4739345664928292e-05,
      "loss": 0.4052,
      "step": 248000
    },
    {
      "epoch": 5.0623370273794,
      "grad_norm": 1.4028816223144531,
      "learning_rate": 2.4688416720990876e-05,
      "loss": 0.3943,
      "step": 248500
    },
    {
      "epoch": 5.072522816166884,
      "grad_norm": 2.2628915309906006,
      "learning_rate": 2.4637487777053457e-05,
      "loss": 0.3847,
      "step": 249000
    },
    {
      "epoch": 5.082708604954368,
      "grad_norm": 10.49995231628418,
      "learning_rate": 2.4586558833116038e-05,
      "loss": 0.372,
      "step": 249500
    },
    {
      "epoch": 5.092894393741851,
      "grad_norm": 3.1380975246429443,
      "learning_rate": 2.453562988917862e-05,
      "loss": 0.3661,
      "step": 250000
    },
    {
      "epoch": 5.103080182529335,
      "grad_norm": 6.003718376159668,
      "learning_rate": 2.4484700945241203e-05,
      "loss": 0.365,
      "step": 250500
    },
    {
      "epoch": 5.113265971316819,
      "grad_norm": 18.119831085205078,
      "learning_rate": 2.443377200130378e-05,
      "loss": 0.3879,
      "step": 251000
    },
    {
      "epoch": 5.123451760104302,
      "grad_norm": 2.0192947387695312,
      "learning_rate": 2.4382843057366364e-05,
      "loss": 0.3978,
      "step": 251500
    },
    {
      "epoch": 5.1336375488917865,
      "grad_norm": 0.7245725989341736,
      "learning_rate": 2.4331914113428945e-05,
      "loss": 0.4111,
      "step": 252000
    },
    {
      "epoch": 5.14382333767927,
      "grad_norm": 13.947088241577148,
      "learning_rate": 2.4280985169491526e-05,
      "loss": 0.3857,
      "step": 252500
    },
    {
      "epoch": 5.154009126466754,
      "grad_norm": 19.191864013671875,
      "learning_rate": 2.4230056225554106e-05,
      "loss": 0.4049,
      "step": 253000
    },
    {
      "epoch": 5.164194915254237,
      "grad_norm": 1.7164690494537354,
      "learning_rate": 2.417912728161669e-05,
      "loss": 0.3874,
      "step": 253500
    },
    {
      "epoch": 5.174380704041721,
      "grad_norm": 9.412607192993164,
      "learning_rate": 2.412819833767927e-05,
      "loss": 0.4018,
      "step": 254000
    },
    {
      "epoch": 5.184566492829204,
      "grad_norm": 10.083121299743652,
      "learning_rate": 2.4077269393741852e-05,
      "loss": 0.3682,
      "step": 254500
    },
    {
      "epoch": 5.1947522816166884,
      "grad_norm": 13.126944541931152,
      "learning_rate": 2.4026340449804436e-05,
      "loss": 0.3936,
      "step": 255000
    },
    {
      "epoch": 5.204938070404172,
      "grad_norm": 8.714841842651367,
      "learning_rate": 2.3975411505867017e-05,
      "loss": 0.3856,
      "step": 255500
    },
    {
      "epoch": 5.215123859191656,
      "grad_norm": 9.877606391906738,
      "learning_rate": 2.3924482561929597e-05,
      "loss": 0.3904,
      "step": 256000
    },
    {
      "epoch": 5.22530964797914,
      "grad_norm": 5.035589218139648,
      "learning_rate": 2.3873553617992178e-05,
      "loss": 0.401,
      "step": 256500
    },
    {
      "epoch": 5.235495436766623,
      "grad_norm": 2.2726454734802246,
      "learning_rate": 2.3822624674054762e-05,
      "loss": 0.3749,
      "step": 257000
    },
    {
      "epoch": 5.245681225554107,
      "grad_norm": 18.14357566833496,
      "learning_rate": 2.377169573011734e-05,
      "loss": 0.3869,
      "step": 257500
    },
    {
      "epoch": 5.25586701434159,
      "grad_norm": 5.729868412017822,
      "learning_rate": 2.3720766786179924e-05,
      "loss": 0.3937,
      "step": 258000
    },
    {
      "epoch": 5.2660528031290745,
      "grad_norm": 6.363208293914795,
      "learning_rate": 2.3669837842242504e-05,
      "loss": 0.4097,
      "step": 258500
    },
    {
      "epoch": 5.276238591916558,
      "grad_norm": 16.286863327026367,
      "learning_rate": 2.3618908898305085e-05,
      "loss": 0.4,
      "step": 259000
    },
    {
      "epoch": 5.286424380704042,
      "grad_norm": 10.553498268127441,
      "learning_rate": 2.3567979954367666e-05,
      "loss": 0.3851,
      "step": 259500
    },
    {
      "epoch": 5.296610169491525,
      "grad_norm": 8.643369674682617,
      "learning_rate": 2.351705101043025e-05,
      "loss": 0.4052,
      "step": 260000
    },
    {
      "epoch": 5.306795958279009,
      "grad_norm": 7.114341735839844,
      "learning_rate": 2.346612206649283e-05,
      "loss": 0.3919,
      "step": 260500
    },
    {
      "epoch": 5.316981747066492,
      "grad_norm": 9.113941192626953,
      "learning_rate": 2.341519312255541e-05,
      "loss": 0.3895,
      "step": 261000
    },
    {
      "epoch": 5.3271675358539765,
      "grad_norm": 4.751819610595703,
      "learning_rate": 2.3364264178617992e-05,
      "loss": 0.3889,
      "step": 261500
    },
    {
      "epoch": 5.337353324641461,
      "grad_norm": 1.8900177478790283,
      "learning_rate": 2.3313335234680576e-05,
      "loss": 0.4089,
      "step": 262000
    },
    {
      "epoch": 5.347539113428944,
      "grad_norm": 7.7582831382751465,
      "learning_rate": 2.3262406290743154e-05,
      "loss": 0.3837,
      "step": 262500
    },
    {
      "epoch": 5.357724902216428,
      "grad_norm": 7.547701358795166,
      "learning_rate": 2.3211477346805738e-05,
      "loss": 0.3919,
      "step": 263000
    },
    {
      "epoch": 5.367910691003911,
      "grad_norm": 7.113526821136475,
      "learning_rate": 2.3160548402868322e-05,
      "loss": 0.3941,
      "step": 263500
    },
    {
      "epoch": 5.378096479791395,
      "grad_norm": 0.9807937145233154,
      "learning_rate": 2.31096194589309e-05,
      "loss": 0.4029,
      "step": 264000
    },
    {
      "epoch": 5.3882822685788785,
      "grad_norm": 11.161014556884766,
      "learning_rate": 2.3058690514993483e-05,
      "loss": 0.3938,
      "step": 264500
    },
    {
      "epoch": 5.398468057366363,
      "grad_norm": 9.379629135131836,
      "learning_rate": 2.3007761571056064e-05,
      "loss": 0.4043,
      "step": 265000
    },
    {
      "epoch": 5.408653846153846,
      "grad_norm": 3.787015438079834,
      "learning_rate": 2.2956832627118645e-05,
      "loss": 0.3849,
      "step": 265500
    },
    {
      "epoch": 5.41883963494133,
      "grad_norm": 14.014139175415039,
      "learning_rate": 2.2905903683181226e-05,
      "loss": 0.3871,
      "step": 266000
    },
    {
      "epoch": 5.429025423728813,
      "grad_norm": 8.955631256103516,
      "learning_rate": 2.285497473924381e-05,
      "loss": 0.4234,
      "step": 266500
    },
    {
      "epoch": 5.439211212516297,
      "grad_norm": 10.213120460510254,
      "learning_rate": 2.280404579530639e-05,
      "loss": 0.3905,
      "step": 267000
    },
    {
      "epoch": 5.449397001303781,
      "grad_norm": 4.696506500244141,
      "learning_rate": 2.275311685136897e-05,
      "loss": 0.3784,
      "step": 267500
    },
    {
      "epoch": 5.459582790091265,
      "grad_norm": 16.33498191833496,
      "learning_rate": 2.2702187907431552e-05,
      "loss": 0.3941,
      "step": 268000
    },
    {
      "epoch": 5.469768578878749,
      "grad_norm": 14.362147331237793,
      "learning_rate": 2.2651258963494136e-05,
      "loss": 0.414,
      "step": 268500
    },
    {
      "epoch": 5.479954367666232,
      "grad_norm": 0.873420774936676,
      "learning_rate": 2.2600330019556713e-05,
      "loss": 0.396,
      "step": 269000
    },
    {
      "epoch": 5.490140156453716,
      "grad_norm": 0.8887625336647034,
      "learning_rate": 2.2549401075619297e-05,
      "loss": 0.395,
      "step": 269500
    },
    {
      "epoch": 5.500325945241199,
      "grad_norm": 7.449705600738525,
      "learning_rate": 2.2498472131681878e-05,
      "loss": 0.4119,
      "step": 270000
    },
    {
      "epoch": 5.510511734028683,
      "grad_norm": 22.612197875976562,
      "learning_rate": 2.244754318774446e-05,
      "loss": 0.3946,
      "step": 270500
    },
    {
      "epoch": 5.5206975228161665,
      "grad_norm": 1.35712730884552,
      "learning_rate": 2.239661424380704e-05,
      "loss": 0.3957,
      "step": 271000
    },
    {
      "epoch": 5.530883311603651,
      "grad_norm": 4.95521354675293,
      "learning_rate": 2.2345685299869624e-05,
      "loss": 0.3895,
      "step": 271500
    },
    {
      "epoch": 5.541069100391134,
      "grad_norm": 9.769365310668945,
      "learning_rate": 2.2294756355932204e-05,
      "loss": 0.4197,
      "step": 272000
    },
    {
      "epoch": 5.551254889178618,
      "grad_norm": 14.507726669311523,
      "learning_rate": 2.2243827411994785e-05,
      "loss": 0.3987,
      "step": 272500
    },
    {
      "epoch": 5.561440677966102,
      "grad_norm": 124.93573760986328,
      "learning_rate": 2.219289846805737e-05,
      "loss": 0.3852,
      "step": 273000
    },
    {
      "epoch": 5.571626466753585,
      "grad_norm": 10.923857688903809,
      "learning_rate": 2.214196952411995e-05,
      "loss": 0.3865,
      "step": 273500
    },
    {
      "epoch": 5.581812255541069,
      "grad_norm": 3.21066951751709,
      "learning_rate": 2.209104058018253e-05,
      "loss": 0.4003,
      "step": 274000
    },
    {
      "epoch": 5.591998044328553,
      "grad_norm": 4.287470817565918,
      "learning_rate": 2.204011163624511e-05,
      "loss": 0.3921,
      "step": 274500
    },
    {
      "epoch": 5.602183833116037,
      "grad_norm": 14.822579383850098,
      "learning_rate": 2.1989182692307696e-05,
      "loss": 0.4172,
      "step": 275000
    },
    {
      "epoch": 5.61236962190352,
      "grad_norm": 8.374959945678711,
      "learning_rate": 2.1938253748370273e-05,
      "loss": 0.3758,
      "step": 275500
    },
    {
      "epoch": 5.622555410691004,
      "grad_norm": 13.172654151916504,
      "learning_rate": 2.1887324804432857e-05,
      "loss": 0.3948,
      "step": 276000
    },
    {
      "epoch": 5.632741199478487,
      "grad_norm": 7.598852634429932,
      "learning_rate": 2.1836395860495438e-05,
      "loss": 0.3742,
      "step": 276500
    },
    {
      "epoch": 5.642926988265971,
      "grad_norm": 9.027917861938477,
      "learning_rate": 2.178546691655802e-05,
      "loss": 0.3845,
      "step": 277000
    },
    {
      "epoch": 5.653112777053455,
      "grad_norm": 2.316246509552002,
      "learning_rate": 2.17345379726206e-05,
      "loss": 0.3914,
      "step": 277500
    },
    {
      "epoch": 5.663298565840939,
      "grad_norm": 14.509943008422852,
      "learning_rate": 2.1683609028683183e-05,
      "loss": 0.3793,
      "step": 278000
    },
    {
      "epoch": 5.673484354628423,
      "grad_norm": 0.9338608980178833,
      "learning_rate": 2.1632680084745764e-05,
      "loss": 0.3957,
      "step": 278500
    },
    {
      "epoch": 5.683670143415906,
      "grad_norm": 1.796526312828064,
      "learning_rate": 2.1581751140808345e-05,
      "loss": 0.388,
      "step": 279000
    },
    {
      "epoch": 5.69385593220339,
      "grad_norm": 10.242021560668945,
      "learning_rate": 2.1530822196870925e-05,
      "loss": 0.4185,
      "step": 279500
    },
    {
      "epoch": 5.704041720990873,
      "grad_norm": 12.557647705078125,
      "learning_rate": 2.147989325293351e-05,
      "loss": 0.3842,
      "step": 280000
    },
    {
      "epoch": 5.7142275097783575,
      "grad_norm": 8.943832397460938,
      "learning_rate": 2.1428964308996087e-05,
      "loss": 0.4053,
      "step": 280500
    },
    {
      "epoch": 5.724413298565841,
      "grad_norm": 10.868306159973145,
      "learning_rate": 2.137803536505867e-05,
      "loss": 0.3698,
      "step": 281000
    },
    {
      "epoch": 5.734599087353325,
      "grad_norm": 10.060036659240723,
      "learning_rate": 2.1327106421121255e-05,
      "loss": 0.405,
      "step": 281500
    },
    {
      "epoch": 5.744784876140808,
      "grad_norm": 0.5824926495552063,
      "learning_rate": 2.1276177477183832e-05,
      "loss": 0.4013,
      "step": 282000
    },
    {
      "epoch": 5.754970664928292,
      "grad_norm": 0.5169064998626709,
      "learning_rate": 2.1225248533246417e-05,
      "loss": 0.3837,
      "step": 282500
    },
    {
      "epoch": 5.765156453715775,
      "grad_norm": 12.848015785217285,
      "learning_rate": 2.1174319589308997e-05,
      "loss": 0.4154,
      "step": 283000
    },
    {
      "epoch": 5.7753422425032594,
      "grad_norm": 10.111686706542969,
      "learning_rate": 2.1123390645371578e-05,
      "loss": 0.3871,
      "step": 283500
    },
    {
      "epoch": 5.7855280312907436,
      "grad_norm": 13.397536277770996,
      "learning_rate": 2.107246170143416e-05,
      "loss": 0.4056,
      "step": 284000
    },
    {
      "epoch": 5.795713820078227,
      "grad_norm": 7.21852445602417,
      "learning_rate": 2.1021532757496743e-05,
      "loss": 0.4047,
      "step": 284500
    },
    {
      "epoch": 5.805899608865711,
      "grad_norm": 8.865501403808594,
      "learning_rate": 2.0970603813559324e-05,
      "loss": 0.4041,
      "step": 285000
    },
    {
      "epoch": 5.816085397653194,
      "grad_norm": 22.124370574951172,
      "learning_rate": 2.0919674869621904e-05,
      "loss": 0.3988,
      "step": 285500
    },
    {
      "epoch": 5.826271186440678,
      "grad_norm": 5.372730731964111,
      "learning_rate": 2.0868745925684485e-05,
      "loss": 0.3943,
      "step": 286000
    },
    {
      "epoch": 5.836456975228161,
      "grad_norm": 11.992902755737305,
      "learning_rate": 2.081781698174707e-05,
      "loss": 0.4041,
      "step": 286500
    },
    {
      "epoch": 5.8466427640156455,
      "grad_norm": 5.596325397491455,
      "learning_rate": 2.0766888037809646e-05,
      "loss": 0.4036,
      "step": 287000
    },
    {
      "epoch": 5.856828552803129,
      "grad_norm": 8.14747428894043,
      "learning_rate": 2.071595909387223e-05,
      "loss": 0.4061,
      "step": 287500
    },
    {
      "epoch": 5.867014341590613,
      "grad_norm": 14.279437065124512,
      "learning_rate": 2.066503014993481e-05,
      "loss": 0.3846,
      "step": 288000
    },
    {
      "epoch": 5.877200130378096,
      "grad_norm": 8.954901695251465,
      "learning_rate": 2.0614101205997392e-05,
      "loss": 0.3764,
      "step": 288500
    },
    {
      "epoch": 5.88738591916558,
      "grad_norm": 10.289920806884766,
      "learning_rate": 2.0563172262059973e-05,
      "loss": 0.398,
      "step": 289000
    },
    {
      "epoch": 5.897571707953064,
      "grad_norm": 8.56008243560791,
      "learning_rate": 2.0512243318122557e-05,
      "loss": 0.3976,
      "step": 289500
    },
    {
      "epoch": 5.9077574967405475,
      "grad_norm": 11.614400863647461,
      "learning_rate": 2.0461314374185138e-05,
      "loss": 0.3857,
      "step": 290000
    },
    {
      "epoch": 5.917943285528032,
      "grad_norm": 24.691856384277344,
      "learning_rate": 2.041038543024772e-05,
      "loss": 0.4017,
      "step": 290500
    },
    {
      "epoch": 5.928129074315515,
      "grad_norm": 5.0968017578125,
      "learning_rate": 2.0359456486310302e-05,
      "loss": 0.3919,
      "step": 291000
    },
    {
      "epoch": 5.938314863102999,
      "grad_norm": 16.616586685180664,
      "learning_rate": 2.0308527542372883e-05,
      "loss": 0.4081,
      "step": 291500
    },
    {
      "epoch": 5.948500651890482,
      "grad_norm": 14.869897842407227,
      "learning_rate": 2.0257598598435464e-05,
      "loss": 0.3846,
      "step": 292000
    },
    {
      "epoch": 5.958686440677966,
      "grad_norm": 12.529975891113281,
      "learning_rate": 2.0206669654498045e-05,
      "loss": 0.3872,
      "step": 292500
    },
    {
      "epoch": 5.9688722294654495,
      "grad_norm": 1.0129965543746948,
      "learning_rate": 2.015574071056063e-05,
      "loss": 0.3864,
      "step": 293000
    },
    {
      "epoch": 5.979058018252934,
      "grad_norm": 2.6426312923431396,
      "learning_rate": 2.0104811766623206e-05,
      "loss": 0.3885,
      "step": 293500
    },
    {
      "epoch": 5.989243807040417,
      "grad_norm": 8.298453330993652,
      "learning_rate": 2.005388282268579e-05,
      "loss": 0.3819,
      "step": 294000
    },
    {
      "epoch": 5.999429595827901,
      "grad_norm": 1.282277226448059,
      "learning_rate": 2.000295387874837e-05,
      "loss": 0.4061,
      "step": 294500
    },
    {
      "epoch": 6.009615384615385,
      "grad_norm": 7.754342555999756,
      "learning_rate": 1.9952024934810955e-05,
      "loss": 0.3643,
      "step": 295000
    },
    {
      "epoch": 6.019801173402868,
      "grad_norm": 0.790826141834259,
      "learning_rate": 1.9901095990873532e-05,
      "loss": 0.3786,
      "step": 295500
    },
    {
      "epoch": 6.029986962190352,
      "grad_norm": 11.09930419921875,
      "learning_rate": 1.9850167046936116e-05,
      "loss": 0.3876,
      "step": 296000
    },
    {
      "epoch": 6.040172750977836,
      "grad_norm": 2.4270777702331543,
      "learning_rate": 1.9799238102998697e-05,
      "loss": 0.3775,
      "step": 296500
    },
    {
      "epoch": 6.05035853976532,
      "grad_norm": 2.7424068450927734,
      "learning_rate": 1.9748309159061278e-05,
      "loss": 0.3914,
      "step": 297000
    },
    {
      "epoch": 6.060544328552803,
      "grad_norm": 15.447957038879395,
      "learning_rate": 1.969738021512386e-05,
      "loss": 0.3719,
      "step": 297500
    },
    {
      "epoch": 6.070730117340287,
      "grad_norm": 8.98629093170166,
      "learning_rate": 1.9646451271186443e-05,
      "loss": 0.3717,
      "step": 298000
    },
    {
      "epoch": 6.08091590612777,
      "grad_norm": 7.942131996154785,
      "learning_rate": 1.9595522327249023e-05,
      "loss": 0.3989,
      "step": 298500
    },
    {
      "epoch": 6.091101694915254,
      "grad_norm": 11.17271900177002,
      "learning_rate": 1.9544593383311604e-05,
      "loss": 0.4019,
      "step": 299000
    },
    {
      "epoch": 6.1012874837027375,
      "grad_norm": 10.931309700012207,
      "learning_rate": 1.949366443937419e-05,
      "loss": 0.3712,
      "step": 299500
    },
    {
      "epoch": 6.111473272490222,
      "grad_norm": 16.133262634277344,
      "learning_rate": 1.944273549543677e-05,
      "loss": 0.3589,
      "step": 300000
    },
    {
      "epoch": 6.121659061277706,
      "grad_norm": 13.866044998168945,
      "learning_rate": 1.939180655149935e-05,
      "loss": 0.3709,
      "step": 300500
    },
    {
      "epoch": 6.131844850065189,
      "grad_norm": 2.841244697570801,
      "learning_rate": 1.934087760756193e-05,
      "loss": 0.3759,
      "step": 301000
    },
    {
      "epoch": 6.142030638852673,
      "grad_norm": 9.30204963684082,
      "learning_rate": 1.9289948663624515e-05,
      "loss": 0.3912,
      "step": 301500
    },
    {
      "epoch": 6.152216427640156,
      "grad_norm": 17.776153564453125,
      "learning_rate": 1.9239019719687092e-05,
      "loss": 0.3928,
      "step": 302000
    },
    {
      "epoch": 6.16240221642764,
      "grad_norm": 8.833773612976074,
      "learning_rate": 1.9188090775749676e-05,
      "loss": 0.3806,
      "step": 302500
    },
    {
      "epoch": 6.172588005215124,
      "grad_norm": 18.421142578125,
      "learning_rate": 1.9137161831812257e-05,
      "loss": 0.3804,
      "step": 303000
    },
    {
      "epoch": 6.182773794002608,
      "grad_norm": 6.71204137802124,
      "learning_rate": 1.9086232887874838e-05,
      "loss": 0.381,
      "step": 303500
    },
    {
      "epoch": 6.192959582790091,
      "grad_norm": 13.37483024597168,
      "learning_rate": 1.9035303943937418e-05,
      "loss": 0.3481,
      "step": 304000
    },
    {
      "epoch": 6.203145371577575,
      "grad_norm": 1.5956056118011475,
      "learning_rate": 1.8984375000000002e-05,
      "loss": 0.394,
      "step": 304500
    },
    {
      "epoch": 6.213331160365058,
      "grad_norm": 16.479501724243164,
      "learning_rate": 1.8933446056062583e-05,
      "loss": 0.3811,
      "step": 305000
    },
    {
      "epoch": 6.223516949152542,
      "grad_norm": 16.491931915283203,
      "learning_rate": 1.8882517112125164e-05,
      "loss": 0.384,
      "step": 305500
    },
    {
      "epoch": 6.2337027379400265,
      "grad_norm": 6.975131511688232,
      "learning_rate": 1.8831588168187745e-05,
      "loss": 0.3891,
      "step": 306000
    },
    {
      "epoch": 6.24388852672751,
      "grad_norm": 4.299749851226807,
      "learning_rate": 1.878065922425033e-05,
      "loss": 0.3931,
      "step": 306500
    },
    {
      "epoch": 6.254074315514994,
      "grad_norm": 9.395760536193848,
      "learning_rate": 1.8729730280312906e-05,
      "loss": 0.3883,
      "step": 307000
    },
    {
      "epoch": 6.264260104302477,
      "grad_norm": 3.558459758758545,
      "learning_rate": 1.867880133637549e-05,
      "loss": 0.3768,
      "step": 307500
    },
    {
      "epoch": 6.274445893089961,
      "grad_norm": 1.831947922706604,
      "learning_rate": 1.862787239243807e-05,
      "loss": 0.3818,
      "step": 308000
    },
    {
      "epoch": 6.284631681877444,
      "grad_norm": 9.829533576965332,
      "learning_rate": 1.857694344850065e-05,
      "loss": 0.3785,
      "step": 308500
    },
    {
      "epoch": 6.2948174706649285,
      "grad_norm": 12.996049880981445,
      "learning_rate": 1.8526014504563236e-05,
      "loss": 0.4002,
      "step": 309000
    },
    {
      "epoch": 6.305003259452412,
      "grad_norm": 15.636149406433105,
      "learning_rate": 1.8475085560625816e-05,
      "loss": 0.3977,
      "step": 309500
    },
    {
      "epoch": 6.315189048239896,
      "grad_norm": 20.07384490966797,
      "learning_rate": 1.8424156616688397e-05,
      "loss": 0.3825,
      "step": 310000
    },
    {
      "epoch": 6.325374837027379,
      "grad_norm": 6.101206302642822,
      "learning_rate": 1.8373227672750978e-05,
      "loss": 0.3772,
      "step": 310500
    },
    {
      "epoch": 6.335560625814863,
      "grad_norm": 10.61382007598877,
      "learning_rate": 1.8322298728813562e-05,
      "loss": 0.39,
      "step": 311000
    },
    {
      "epoch": 6.345746414602347,
      "grad_norm": 27.605512619018555,
      "learning_rate": 1.8271369784876143e-05,
      "loss": 0.3901,
      "step": 311500
    },
    {
      "epoch": 6.3559322033898304,
      "grad_norm": 19.2958984375,
      "learning_rate": 1.8220440840938723e-05,
      "loss": 0.3904,
      "step": 312000
    },
    {
      "epoch": 6.3661179921773146,
      "grad_norm": 0.7518132925033569,
      "learning_rate": 1.8169511897001304e-05,
      "loss": 0.3851,
      "step": 312500
    },
    {
      "epoch": 6.376303780964798,
      "grad_norm": 1.6772586107254028,
      "learning_rate": 1.8118582953063888e-05,
      "loss": 0.3913,
      "step": 313000
    },
    {
      "epoch": 6.386489569752282,
      "grad_norm": 1.0172537565231323,
      "learning_rate": 1.8067654009126466e-05,
      "loss": 0.3967,
      "step": 313500
    },
    {
      "epoch": 6.396675358539765,
      "grad_norm": 11.684889793395996,
      "learning_rate": 1.801672506518905e-05,
      "loss": 0.4031,
      "step": 314000
    },
    {
      "epoch": 6.406861147327249,
      "grad_norm": 1.1821123361587524,
      "learning_rate": 1.796579612125163e-05,
      "loss": 0.3825,
      "step": 314500
    },
    {
      "epoch": 6.417046936114732,
      "grad_norm": 7.697792053222656,
      "learning_rate": 1.791486717731421e-05,
      "loss": 0.3701,
      "step": 315000
    },
    {
      "epoch": 6.4272327249022165,
      "grad_norm": 10.795524597167969,
      "learning_rate": 1.7863938233376792e-05,
      "loss": 0.3927,
      "step": 315500
    },
    {
      "epoch": 6.4374185136897,
      "grad_norm": 3.0545763969421387,
      "learning_rate": 1.7813009289439376e-05,
      "loss": 0.3844,
      "step": 316000
    },
    {
      "epoch": 6.447604302477184,
      "grad_norm": 10.448719024658203,
      "learning_rate": 1.7762080345501957e-05,
      "loss": 0.39,
      "step": 316500
    },
    {
      "epoch": 6.457790091264668,
      "grad_norm": 5.341092109680176,
      "learning_rate": 1.7711151401564537e-05,
      "loss": 0.3847,
      "step": 317000
    },
    {
      "epoch": 6.467975880052151,
      "grad_norm": 7.808619022369385,
      "learning_rate": 1.766022245762712e-05,
      "loss": 0.394,
      "step": 317500
    },
    {
      "epoch": 6.478161668839635,
      "grad_norm": 7.8171892166137695,
      "learning_rate": 1.7609293513689702e-05,
      "loss": 0.3884,
      "step": 318000
    },
    {
      "epoch": 6.4883474576271185,
      "grad_norm": 12.212760925292969,
      "learning_rate": 1.7558364569752283e-05,
      "loss": 0.3954,
      "step": 318500
    },
    {
      "epoch": 6.498533246414603,
      "grad_norm": 10.05772590637207,
      "learning_rate": 1.7507435625814864e-05,
      "loss": 0.3679,
      "step": 319000
    },
    {
      "epoch": 6.508719035202086,
      "grad_norm": 18.6081600189209,
      "learning_rate": 1.7456506681877448e-05,
      "loss": 0.3981,
      "step": 319500
    },
    {
      "epoch": 6.51890482398957,
      "grad_norm": 0.6424477100372314,
      "learning_rate": 1.7405577737940025e-05,
      "loss": 0.3811,
      "step": 320000
    },
    {
      "epoch": 6.529090612777053,
      "grad_norm": 11.705473899841309,
      "learning_rate": 1.735464879400261e-05,
      "loss": 0.3939,
      "step": 320500
    },
    {
      "epoch": 6.539276401564537,
      "grad_norm": 2.2202043533325195,
      "learning_rate": 1.730371985006519e-05,
      "loss": 0.3859,
      "step": 321000
    },
    {
      "epoch": 6.5494621903520205,
      "grad_norm": 8.632284164428711,
      "learning_rate": 1.725279090612777e-05,
      "loss": 0.3895,
      "step": 321500
    },
    {
      "epoch": 6.559647979139505,
      "grad_norm": 15.201044082641602,
      "learning_rate": 1.720186196219035e-05,
      "loss": 0.3719,
      "step": 322000
    },
    {
      "epoch": 6.569833767926989,
      "grad_norm": 12.98396110534668,
      "learning_rate": 1.7150933018252936e-05,
      "loss": 0.3725,
      "step": 322500
    },
    {
      "epoch": 6.580019556714472,
      "grad_norm": 15.551775932312012,
      "learning_rate": 1.7100004074315516e-05,
      "loss": 0.3838,
      "step": 323000
    },
    {
      "epoch": 6.590205345501956,
      "grad_norm": 2.7676868438720703,
      "learning_rate": 1.7049075130378097e-05,
      "loss": 0.38,
      "step": 323500
    },
    {
      "epoch": 6.600391134289439,
      "grad_norm": 17.538307189941406,
      "learning_rate": 1.6998146186440678e-05,
      "loss": 0.3964,
      "step": 324000
    },
    {
      "epoch": 6.610576923076923,
      "grad_norm": 12.316848754882812,
      "learning_rate": 1.6947217242503262e-05,
      "loss": 0.3859,
      "step": 324500
    },
    {
      "epoch": 6.620762711864407,
      "grad_norm": 10.203607559204102,
      "learning_rate": 1.689628829856584e-05,
      "loss": 0.3847,
      "step": 325000
    },
    {
      "epoch": 6.630948500651891,
      "grad_norm": 0.8148593902587891,
      "learning_rate": 1.6845359354628423e-05,
      "loss": 0.3801,
      "step": 325500
    },
    {
      "epoch": 6.641134289439374,
      "grad_norm": 11.710694313049316,
      "learning_rate": 1.6794430410691007e-05,
      "loss": 0.3997,
      "step": 326000
    },
    {
      "epoch": 6.651320078226858,
      "grad_norm": 18.676490783691406,
      "learning_rate": 1.6743501466753585e-05,
      "loss": 0.3893,
      "step": 326500
    },
    {
      "epoch": 6.661505867014341,
      "grad_norm": 13.697277069091797,
      "learning_rate": 1.669257252281617e-05,
      "loss": 0.3882,
      "step": 327000
    },
    {
      "epoch": 6.671691655801825,
      "grad_norm": 8.842901229858398,
      "learning_rate": 1.664164357887875e-05,
      "loss": 0.3929,
      "step": 327500
    },
    {
      "epoch": 6.681877444589309,
      "grad_norm": 1.261549472808838,
      "learning_rate": 1.659071463494133e-05,
      "loss": 0.3784,
      "step": 328000
    },
    {
      "epoch": 6.692063233376793,
      "grad_norm": 13.758550643920898,
      "learning_rate": 1.653978569100391e-05,
      "loss": 0.3869,
      "step": 328500
    },
    {
      "epoch": 6.702249022164277,
      "grad_norm": 10.424388885498047,
      "learning_rate": 1.6488856747066495e-05,
      "loss": 0.3796,
      "step": 329000
    },
    {
      "epoch": 6.71243481095176,
      "grad_norm": 9.93979263305664,
      "learning_rate": 1.6437927803129076e-05,
      "loss": 0.3648,
      "step": 329500
    },
    {
      "epoch": 6.722620599739244,
      "grad_norm": 3.8493294715881348,
      "learning_rate": 1.6386998859191657e-05,
      "loss": 0.3862,
      "step": 330000
    },
    {
      "epoch": 6.732806388526727,
      "grad_norm": 11.89888858795166,
      "learning_rate": 1.6336069915254237e-05,
      "loss": 0.3938,
      "step": 330500
    },
    {
      "epoch": 6.742992177314211,
      "grad_norm": 1.138919472694397,
      "learning_rate": 1.628514097131682e-05,
      "loss": 0.3922,
      "step": 331000
    },
    {
      "epoch": 6.753177966101695,
      "grad_norm": 3.5944101810455322,
      "learning_rate": 1.62342120273794e-05,
      "loss": 0.3905,
      "step": 331500
    },
    {
      "epoch": 6.763363754889179,
      "grad_norm": 10.151657104492188,
      "learning_rate": 1.6183283083441983e-05,
      "loss": 0.371,
      "step": 332000
    },
    {
      "epoch": 6.773549543676662,
      "grad_norm": 2.743898868560791,
      "learning_rate": 1.6132354139504564e-05,
      "loss": 0.4121,
      "step": 332500
    },
    {
      "epoch": 6.783735332464146,
      "grad_norm": 4.864498138427734,
      "learning_rate": 1.6081425195567144e-05,
      "loss": 0.3767,
      "step": 333000
    },
    {
      "epoch": 6.79392112125163,
      "grad_norm": 2.4860167503356934,
      "learning_rate": 1.6030496251629725e-05,
      "loss": 0.3992,
      "step": 333500
    },
    {
      "epoch": 6.804106910039113,
      "grad_norm": 10.243704795837402,
      "learning_rate": 1.597956730769231e-05,
      "loss": 0.3838,
      "step": 334000
    },
    {
      "epoch": 6.8142926988265975,
      "grad_norm": 4.524470806121826,
      "learning_rate": 1.592863836375489e-05,
      "loss": 0.3533,
      "step": 334500
    },
    {
      "epoch": 6.824478487614081,
      "grad_norm": 9.753538131713867,
      "learning_rate": 1.587770941981747e-05,
      "loss": 0.3665,
      "step": 335000
    },
    {
      "epoch": 6.834664276401565,
      "grad_norm": 0.5988380312919617,
      "learning_rate": 1.5826780475880055e-05,
      "loss": 0.3882,
      "step": 335500
    },
    {
      "epoch": 6.844850065189048,
      "grad_norm": 17.76557159423828,
      "learning_rate": 1.5775851531942635e-05,
      "loss": 0.3878,
      "step": 336000
    },
    {
      "epoch": 6.855035853976532,
      "grad_norm": 22.027685165405273,
      "learning_rate": 1.5724922588005216e-05,
      "loss": 0.3745,
      "step": 336500
    },
    {
      "epoch": 6.865221642764015,
      "grad_norm": 15.061382293701172,
      "learning_rate": 1.5673993644067797e-05,
      "loss": 0.3634,
      "step": 337000
    },
    {
      "epoch": 6.8754074315514995,
      "grad_norm": 1.5125173330307007,
      "learning_rate": 1.562306470013038e-05,
      "loss": 0.4,
      "step": 337500
    },
    {
      "epoch": 6.885593220338983,
      "grad_norm": 3.2371411323547363,
      "learning_rate": 1.557213575619296e-05,
      "loss": 0.3718,
      "step": 338000
    },
    {
      "epoch": 6.895779009126467,
      "grad_norm": 4.706023216247559,
      "learning_rate": 1.5521206812255543e-05,
      "loss": 0.367,
      "step": 338500
    },
    {
      "epoch": 6.905964797913951,
      "grad_norm": 14.162470817565918,
      "learning_rate": 1.5470277868318123e-05,
      "loss": 0.3558,
      "step": 339000
    },
    {
      "epoch": 6.916150586701434,
      "grad_norm": 10.370302200317383,
      "learning_rate": 1.5419348924380704e-05,
      "loss": 0.3879,
      "step": 339500
    },
    {
      "epoch": 6.926336375488918,
      "grad_norm": 11.532611846923828,
      "learning_rate": 1.5368419980443285e-05,
      "loss": 0.3867,
      "step": 340000
    },
    {
      "epoch": 6.9365221642764014,
      "grad_norm": 9.656462669372559,
      "learning_rate": 1.531749103650587e-05,
      "loss": 0.3846,
      "step": 340500
    },
    {
      "epoch": 6.9467079530638856,
      "grad_norm": 6.1834187507629395,
      "learning_rate": 1.526656209256845e-05,
      "loss": 0.3685,
      "step": 341000
    },
    {
      "epoch": 6.956893741851369,
      "grad_norm": 5.8108086585998535,
      "learning_rate": 1.521563314863103e-05,
      "loss": 0.3836,
      "step": 341500
    },
    {
      "epoch": 6.967079530638853,
      "grad_norm": 11.8114652633667,
      "learning_rate": 1.5164704204693611e-05,
      "loss": 0.395,
      "step": 342000
    },
    {
      "epoch": 6.977265319426336,
      "grad_norm": 16.2764892578125,
      "learning_rate": 1.5113775260756193e-05,
      "loss": 0.3712,
      "step": 342500
    },
    {
      "epoch": 6.98745110821382,
      "grad_norm": 10.375921249389648,
      "learning_rate": 1.5062846316818774e-05,
      "loss": 0.3929,
      "step": 343000
    },
    {
      "epoch": 6.997636897001303,
      "grad_norm": 5.733703136444092,
      "learning_rate": 1.5011917372881357e-05,
      "loss": 0.3818,
      "step": 343500
    },
    {
      "epoch": 7.0078226857887875,
      "grad_norm": 5.876563549041748,
      "learning_rate": 1.4960988428943939e-05,
      "loss": 0.3743,
      "step": 344000
    },
    {
      "epoch": 7.018008474576271,
      "grad_norm": 0.42569151520729065,
      "learning_rate": 1.491005948500652e-05,
      "loss": 0.3648,
      "step": 344500
    },
    {
      "epoch": 7.028194263363755,
      "grad_norm": 14.575827598571777,
      "learning_rate": 1.4859130541069102e-05,
      "loss": 0.368,
      "step": 345000
    },
    {
      "epoch": 7.038380052151239,
      "grad_norm": 7.087462425231934,
      "learning_rate": 1.4808201597131683e-05,
      "loss": 0.3607,
      "step": 345500
    },
    {
      "epoch": 7.048565840938722,
      "grad_norm": 10.351123809814453,
      "learning_rate": 1.4757272653194265e-05,
      "loss": 0.3525,
      "step": 346000
    },
    {
      "epoch": 7.058751629726206,
      "grad_norm": 4.18763542175293,
      "learning_rate": 1.4706343709256844e-05,
      "loss": 0.392,
      "step": 346500
    },
    {
      "epoch": 7.0689374185136895,
      "grad_norm": 13.258096694946289,
      "learning_rate": 1.4655414765319428e-05,
      "loss": 0.37,
      "step": 347000
    },
    {
      "epoch": 7.079123207301174,
      "grad_norm": 8.315428733825684,
      "learning_rate": 1.4604485821382007e-05,
      "loss": 0.4004,
      "step": 347500
    },
    {
      "epoch": 7.089308996088657,
      "grad_norm": 6.136110782623291,
      "learning_rate": 1.4553556877444592e-05,
      "loss": 0.3581,
      "step": 348000
    },
    {
      "epoch": 7.099494784876141,
      "grad_norm": 15.832286834716797,
      "learning_rate": 1.450262793350717e-05,
      "loss": 0.3808,
      "step": 348500
    },
    {
      "epoch": 7.109680573663624,
      "grad_norm": 1.1888922452926636,
      "learning_rate": 1.4451698989569753e-05,
      "loss": 0.3574,
      "step": 349000
    },
    {
      "epoch": 7.119866362451108,
      "grad_norm": 10.791431427001953,
      "learning_rate": 1.4400770045632334e-05,
      "loss": 0.3566,
      "step": 349500
    },
    {
      "epoch": 7.130052151238592,
      "grad_norm": 8.822840690612793,
      "learning_rate": 1.4349841101694916e-05,
      "loss": 0.3705,
      "step": 350000
    },
    {
      "epoch": 7.140237940026076,
      "grad_norm": 12.285748481750488,
      "learning_rate": 1.4298912157757497e-05,
      "loss": 0.3862,
      "step": 350500
    },
    {
      "epoch": 7.15042372881356,
      "grad_norm": 16.840286254882812,
      "learning_rate": 1.424798321382008e-05,
      "loss": 0.3542,
      "step": 351000
    },
    {
      "epoch": 7.160609517601043,
      "grad_norm": 17.60918617248535,
      "learning_rate": 1.419705426988266e-05,
      "loss": 0.3942,
      "step": 351500
    },
    {
      "epoch": 7.170795306388527,
      "grad_norm": 8.797198295593262,
      "learning_rate": 1.4146125325945242e-05,
      "loss": 0.384,
      "step": 352000
    },
    {
      "epoch": 7.18098109517601,
      "grad_norm": 7.805654525756836,
      "learning_rate": 1.4095196382007821e-05,
      "loss": 0.3558,
      "step": 352500
    },
    {
      "epoch": 7.191166883963494,
      "grad_norm": 10.909255981445312,
      "learning_rate": 1.4044267438070406e-05,
      "loss": 0.3836,
      "step": 353000
    },
    {
      "epoch": 7.201352672750978,
      "grad_norm": 13.472633361816406,
      "learning_rate": 1.3993338494132988e-05,
      "loss": 0.3829,
      "step": 353500
    },
    {
      "epoch": 7.211538461538462,
      "grad_norm": 4.988961219787598,
      "learning_rate": 1.3942409550195567e-05,
      "loss": 0.3766,
      "step": 354000
    },
    {
      "epoch": 7.221724250325945,
      "grad_norm": 7.217696189880371,
      "learning_rate": 1.3891480606258151e-05,
      "loss": 0.37,
      "step": 354500
    },
    {
      "epoch": 7.231910039113429,
      "grad_norm": 3.7685816287994385,
      "learning_rate": 1.384055166232073e-05,
      "loss": 0.358,
      "step": 355000
    },
    {
      "epoch": 7.242095827900912,
      "grad_norm": 16.717302322387695,
      "learning_rate": 1.3789622718383313e-05,
      "loss": 0.3894,
      "step": 355500
    },
    {
      "epoch": 7.252281616688396,
      "grad_norm": 14.937047004699707,
      "learning_rate": 1.3738693774445893e-05,
      "loss": 0.3757,
      "step": 356000
    },
    {
      "epoch": 7.26246740547588,
      "grad_norm": 10.522089958190918,
      "learning_rate": 1.3687764830508476e-05,
      "loss": 0.3777,
      "step": 356500
    },
    {
      "epoch": 7.272653194263364,
      "grad_norm": 8.601746559143066,
      "learning_rate": 1.3636835886571056e-05,
      "loss": 0.3846,
      "step": 357000
    },
    {
      "epoch": 7.282838983050848,
      "grad_norm": 12.20914077758789,
      "learning_rate": 1.3585906942633639e-05,
      "loss": 0.3911,
      "step": 357500
    },
    {
      "epoch": 7.293024771838331,
      "grad_norm": 9.797807693481445,
      "learning_rate": 1.353497799869622e-05,
      "loss": 0.3988,
      "step": 358000
    },
    {
      "epoch": 7.303210560625815,
      "grad_norm": 4.438987731933594,
      "learning_rate": 1.3484049054758802e-05,
      "loss": 0.3758,
      "step": 358500
    },
    {
      "epoch": 7.313396349413298,
      "grad_norm": 12.777297973632812,
      "learning_rate": 1.3433120110821381e-05,
      "loss": 0.364,
      "step": 359000
    },
    {
      "epoch": 7.323582138200782,
      "grad_norm": 7.394062519073486,
      "learning_rate": 1.3382191166883965e-05,
      "loss": 0.3862,
      "step": 359500
    },
    {
      "epoch": 7.333767926988266,
      "grad_norm": 9.930566787719727,
      "learning_rate": 1.3331262222946544e-05,
      "loss": 0.3835,
      "step": 360000
    },
    {
      "epoch": 7.34395371577575,
      "grad_norm": 21.9543399810791,
      "learning_rate": 1.3280333279009127e-05,
      "loss": 0.4103,
      "step": 360500
    },
    {
      "epoch": 7.354139504563234,
      "grad_norm": 8.87610149383545,
      "learning_rate": 1.3229404335071707e-05,
      "loss": 0.379,
      "step": 361000
    },
    {
      "epoch": 7.364325293350717,
      "grad_norm": 10.238451957702637,
      "learning_rate": 1.317847539113429e-05,
      "loss": 0.353,
      "step": 361500
    },
    {
      "epoch": 7.374511082138201,
      "grad_norm": 12.050806999206543,
      "learning_rate": 1.3127546447196872e-05,
      "loss": 0.376,
      "step": 362000
    },
    {
      "epoch": 7.384696870925684,
      "grad_norm": 6.291297912597656,
      "learning_rate": 1.3076617503259453e-05,
      "loss": 0.3641,
      "step": 362500
    },
    {
      "epoch": 7.3948826597131685,
      "grad_norm": 14.922341346740723,
      "learning_rate": 1.3025688559322035e-05,
      "loss": 0.3841,
      "step": 363000
    },
    {
      "epoch": 7.405068448500652,
      "grad_norm": 14.846951484680176,
      "learning_rate": 1.2974759615384616e-05,
      "loss": 0.3558,
      "step": 363500
    },
    {
      "epoch": 7.415254237288136,
      "grad_norm": 11.005878448486328,
      "learning_rate": 1.2923830671447198e-05,
      "loss": 0.3698,
      "step": 364000
    },
    {
      "epoch": 7.425440026075619,
      "grad_norm": 6.60953426361084,
      "learning_rate": 1.287290172750978e-05,
      "loss": 0.3583,
      "step": 364500
    },
    {
      "epoch": 7.435625814863103,
      "grad_norm": 4.8644819259643555,
      "learning_rate": 1.2821972783572362e-05,
      "loss": 0.3816,
      "step": 365000
    },
    {
      "epoch": 7.445811603650586,
      "grad_norm": 0.8962101936340332,
      "learning_rate": 1.277104383963494e-05,
      "loss": 0.3779,
      "step": 365500
    },
    {
      "epoch": 7.4559973924380705,
      "grad_norm": 26.016328811645508,
      "learning_rate": 1.2720114895697525e-05,
      "loss": 0.3905,
      "step": 366000
    },
    {
      "epoch": 7.466183181225555,
      "grad_norm": 15.141844749450684,
      "learning_rate": 1.2669185951760104e-05,
      "loss": 0.3635,
      "step": 366500
    },
    {
      "epoch": 7.476368970013038,
      "grad_norm": 0.3021068274974823,
      "learning_rate": 1.2618257007822686e-05,
      "loss": 0.3764,
      "step": 367000
    },
    {
      "epoch": 7.486554758800522,
      "grad_norm": 11.508829116821289,
      "learning_rate": 1.2567328063885267e-05,
      "loss": 0.3906,
      "step": 367500
    },
    {
      "epoch": 7.496740547588005,
      "grad_norm": 3.906755208969116,
      "learning_rate": 1.251639911994785e-05,
      "loss": 0.3938,
      "step": 368000
    },
    {
      "epoch": 7.506926336375489,
      "grad_norm": 16.732757568359375,
      "learning_rate": 1.2465470176010432e-05,
      "loss": 0.3662,
      "step": 368500
    },
    {
      "epoch": 7.5171121251629724,
      "grad_norm": 13.350677490234375,
      "learning_rate": 1.2414541232073013e-05,
      "loss": 0.3676,
      "step": 369000
    },
    {
      "epoch": 7.5272979139504566,
      "grad_norm": 14.520928382873535,
      "learning_rate": 1.2363612288135593e-05,
      "loss": 0.3729,
      "step": 369500
    },
    {
      "epoch": 7.53748370273794,
      "grad_norm": 9.785638809204102,
      "learning_rate": 1.2312683344198176e-05,
      "loss": 0.3855,
      "step": 370000
    },
    {
      "epoch": 7.547669491525424,
      "grad_norm": 14.136678695678711,
      "learning_rate": 1.2261754400260756e-05,
      "loss": 0.3962,
      "step": 370500
    },
    {
      "epoch": 7.557855280312907,
      "grad_norm": 6.7698445320129395,
      "learning_rate": 1.2210825456323339e-05,
      "loss": 0.3633,
      "step": 371000
    },
    {
      "epoch": 7.568041069100391,
      "grad_norm": 8.65478801727295,
      "learning_rate": 1.215989651238592e-05,
      "loss": 0.4008,
      "step": 371500
    },
    {
      "epoch": 7.578226857887875,
      "grad_norm": 6.680449485778809,
      "learning_rate": 1.21089675684485e-05,
      "loss": 0.3836,
      "step": 372000
    },
    {
      "epoch": 7.5884126466753585,
      "grad_norm": 5.147963523864746,
      "learning_rate": 1.2058038624511083e-05,
      "loss": 0.3728,
      "step": 372500
    },
    {
      "epoch": 7.598598435462842,
      "grad_norm": 0.5160101652145386,
      "learning_rate": 1.2007109680573663e-05,
      "loss": 0.368,
      "step": 373000
    },
    {
      "epoch": 7.608784224250326,
      "grad_norm": 15.52830696105957,
      "learning_rate": 1.1956180736636246e-05,
      "loss": 0.3889,
      "step": 373500
    },
    {
      "epoch": 7.61897001303781,
      "grad_norm": 16.965503692626953,
      "learning_rate": 1.1905251792698827e-05,
      "loss": 0.3907,
      "step": 374000
    },
    {
      "epoch": 7.629155801825293,
      "grad_norm": 12.097764015197754,
      "learning_rate": 1.1854322848761407e-05,
      "loss": 0.352,
      "step": 374500
    },
    {
      "epoch": 7.639341590612777,
      "grad_norm": 2.516423225402832,
      "learning_rate": 1.1803393904823991e-05,
      "loss": 0.3865,
      "step": 375000
    },
    {
      "epoch": 7.6495273794002605,
      "grad_norm": 14.348915100097656,
      "learning_rate": 1.1752464960886572e-05,
      "loss": 0.3688,
      "step": 375500
    },
    {
      "epoch": 7.659713168187745,
      "grad_norm": 26.508943557739258,
      "learning_rate": 1.1701536016949153e-05,
      "loss": 0.3636,
      "step": 376000
    },
    {
      "epoch": 7.669898956975228,
      "grad_norm": 5.5436482429504395,
      "learning_rate": 1.1650607073011735e-05,
      "loss": 0.3821,
      "step": 376500
    },
    {
      "epoch": 7.680084745762712,
      "grad_norm": 0.5836724638938904,
      "learning_rate": 1.1599678129074316e-05,
      "loss": 0.3887,
      "step": 377000
    },
    {
      "epoch": 7.690270534550196,
      "grad_norm": 5.806914806365967,
      "learning_rate": 1.1548749185136898e-05,
      "loss": 0.3659,
      "step": 377500
    },
    {
      "epoch": 7.700456323337679,
      "grad_norm": 2.5479555130004883,
      "learning_rate": 1.1497820241199479e-05,
      "loss": 0.3503,
      "step": 378000
    },
    {
      "epoch": 7.7106421121251625,
      "grad_norm": 1.5511831045150757,
      "learning_rate": 1.144689129726206e-05,
      "loss": 0.4156,
      "step": 378500
    },
    {
      "epoch": 7.720827900912647,
      "grad_norm": 13.384458541870117,
      "learning_rate": 1.1395962353324642e-05,
      "loss": 0.3808,
      "step": 379000
    },
    {
      "epoch": 7.731013689700131,
      "grad_norm": 5.685850143432617,
      "learning_rate": 1.1345033409387223e-05,
      "loss": 0.3775,
      "step": 379500
    },
    {
      "epoch": 7.741199478487614,
      "grad_norm": 14.015962600708008,
      "learning_rate": 1.1294104465449805e-05,
      "loss": 0.372,
      "step": 380000
    },
    {
      "epoch": 7.751385267275098,
      "grad_norm": 2.3595902919769287,
      "learning_rate": 1.1243175521512386e-05,
      "loss": 0.3689,
      "step": 380500
    },
    {
      "epoch": 7.761571056062581,
      "grad_norm": 1.8684459924697876,
      "learning_rate": 1.1192246577574967e-05,
      "loss": 0.3843,
      "step": 381000
    },
    {
      "epoch": 7.771756844850065,
      "grad_norm": 15.35169506072998,
      "learning_rate": 1.114131763363755e-05,
      "loss": 0.3837,
      "step": 381500
    },
    {
      "epoch": 7.781942633637549,
      "grad_norm": 5.403034687042236,
      "learning_rate": 1.109038868970013e-05,
      "loss": 0.384,
      "step": 382000
    },
    {
      "epoch": 7.792128422425033,
      "grad_norm": 18.265209197998047,
      "learning_rate": 1.1039459745762712e-05,
      "loss": 0.3871,
      "step": 382500
    },
    {
      "epoch": 7.802314211212517,
      "grad_norm": 17.798025131225586,
      "learning_rate": 1.0988530801825293e-05,
      "loss": 0.389,
      "step": 383000
    },
    {
      "epoch": 7.8125,
      "grad_norm": 6.94778299331665,
      "learning_rate": 1.0937601857887876e-05,
      "loss": 0.3727,
      "step": 383500
    },
    {
      "epoch": 7.822685788787483,
      "grad_norm": 14.730717658996582,
      "learning_rate": 1.0886672913950458e-05,
      "loss": 0.3657,
      "step": 384000
    },
    {
      "epoch": 7.832871577574967,
      "grad_norm": 13.086844444274902,
      "learning_rate": 1.0835743970013039e-05,
      "loss": 0.3899,
      "step": 384500
    },
    {
      "epoch": 7.843057366362451,
      "grad_norm": 6.723066329956055,
      "learning_rate": 1.0784815026075621e-05,
      "loss": 0.3706,
      "step": 385000
    },
    {
      "epoch": 7.853243155149935,
      "grad_norm": 2.161780595779419,
      "learning_rate": 1.0733886082138202e-05,
      "loss": 0.3706,
      "step": 385500
    },
    {
      "epoch": 7.863428943937419,
      "grad_norm": 1.2214587926864624,
      "learning_rate": 1.0682957138200783e-05,
      "loss": 0.3965,
      "step": 386000
    },
    {
      "epoch": 7.873614732724902,
      "grad_norm": 14.842206001281738,
      "learning_rate": 1.0632028194263365e-05,
      "loss": 0.3683,
      "step": 386500
    },
    {
      "epoch": 7.883800521512386,
      "grad_norm": 7.244915008544922,
      "learning_rate": 1.0581099250325946e-05,
      "loss": 0.3649,
      "step": 387000
    },
    {
      "epoch": 7.893986310299869,
      "grad_norm": 15.698479652404785,
      "learning_rate": 1.0530170306388528e-05,
      "loss": 0.3714,
      "step": 387500
    },
    {
      "epoch": 7.904172099087353,
      "grad_norm": 16.992177963256836,
      "learning_rate": 1.0479241362451109e-05,
      "loss": 0.4006,
      "step": 388000
    },
    {
      "epoch": 7.9143578878748375,
      "grad_norm": 2.03429913520813,
      "learning_rate": 1.042831241851369e-05,
      "loss": 0.3749,
      "step": 388500
    },
    {
      "epoch": 7.924543676662321,
      "grad_norm": 0.22026115655899048,
      "learning_rate": 1.0377383474576272e-05,
      "loss": 0.3639,
      "step": 389000
    },
    {
      "epoch": 7.934729465449804,
      "grad_norm": 23.63441276550293,
      "learning_rate": 1.0326454530638853e-05,
      "loss": 0.3982,
      "step": 389500
    },
    {
      "epoch": 7.944915254237288,
      "grad_norm": 16.20482063293457,
      "learning_rate": 1.0275525586701435e-05,
      "loss": 0.3904,
      "step": 390000
    },
    {
      "epoch": 7.955101043024772,
      "grad_norm": 4.793302059173584,
      "learning_rate": 1.0224596642764016e-05,
      "loss": 0.4017,
      "step": 390500
    },
    {
      "epoch": 7.965286831812255,
      "grad_norm": 4.5826802253723145,
      "learning_rate": 1.0173667698826597e-05,
      "loss": 0.3848,
      "step": 391000
    },
    {
      "epoch": 7.9754726205997395,
      "grad_norm": 13.582854270935059,
      "learning_rate": 1.0122738754889179e-05,
      "loss": 0.3728,
      "step": 391500
    },
    {
      "epoch": 7.985658409387223,
      "grad_norm": 3.339411735534668,
      "learning_rate": 1.007180981095176e-05,
      "loss": 0.3551,
      "step": 392000
    },
    {
      "epoch": 7.995844198174707,
      "grad_norm": 1.55934476852417,
      "learning_rate": 1.0020880867014342e-05,
      "loss": 0.3912,
      "step": 392500
    },
    {
      "epoch": 8.00602998696219,
      "grad_norm": 14.266277313232422,
      "learning_rate": 9.969951923076925e-06,
      "loss": 0.3556,
      "step": 393000
    },
    {
      "epoch": 8.016215775749673,
      "grad_norm": 10.562520980834961,
      "learning_rate": 9.919022979139505e-06,
      "loss": 0.3714,
      "step": 393500
    },
    {
      "epoch": 8.026401564537158,
      "grad_norm": 5.415472984313965,
      "learning_rate": 9.868094035202088e-06,
      "loss": 0.3606,
      "step": 394000
    },
    {
      "epoch": 8.036587353324641,
      "grad_norm": 0.5472511649131775,
      "learning_rate": 9.817165091264668e-06,
      "loss": 0.3591,
      "step": 394500
    },
    {
      "epoch": 8.046773142112125,
      "grad_norm": 8.17412281036377,
      "learning_rate": 9.76623614732725e-06,
      "loss": 0.375,
      "step": 395000
    },
    {
      "epoch": 8.05695893089961,
      "grad_norm": 14.530375480651855,
      "learning_rate": 9.715307203389832e-06,
      "loss": 0.3526,
      "step": 395500
    },
    {
      "epoch": 8.067144719687093,
      "grad_norm": 8.240200996398926,
      "learning_rate": 9.664378259452412e-06,
      "loss": 0.3745,
      "step": 396000
    },
    {
      "epoch": 8.077330508474576,
      "grad_norm": 9.192708969116211,
      "learning_rate": 9.613449315514995e-06,
      "loss": 0.3853,
      "step": 396500
    },
    {
      "epoch": 8.08751629726206,
      "grad_norm": 17.59284019470215,
      "learning_rate": 9.562520371577575e-06,
      "loss": 0.3618,
      "step": 397000
    },
    {
      "epoch": 8.097702086049544,
      "grad_norm": 13.29923152923584,
      "learning_rate": 9.511591427640156e-06,
      "loss": 0.3887,
      "step": 397500
    },
    {
      "epoch": 8.107887874837028,
      "grad_norm": 13.960816383361816,
      "learning_rate": 9.460662483702739e-06,
      "loss": 0.3625,
      "step": 398000
    },
    {
      "epoch": 8.11807366362451,
      "grad_norm": 0.8355950117111206,
      "learning_rate": 9.40973353976532e-06,
      "loss": 0.3543,
      "step": 398500
    },
    {
      "epoch": 8.128259452411994,
      "grad_norm": 13.489482879638672,
      "learning_rate": 9.358804595827902e-06,
      "loss": 0.3686,
      "step": 399000
    },
    {
      "epoch": 8.138445241199479,
      "grad_norm": 9.4583740234375,
      "learning_rate": 9.307875651890482e-06,
      "loss": 0.3825,
      "step": 399500
    },
    {
      "epoch": 8.148631029986962,
      "grad_norm": 5.886244297027588,
      "learning_rate": 9.256946707953063e-06,
      "loss": 0.3823,
      "step": 400000
    },
    {
      "epoch": 8.158816818774445,
      "grad_norm": 12.191303253173828,
      "learning_rate": 9.206017764015646e-06,
      "loss": 0.3455,
      "step": 400500
    },
    {
      "epoch": 8.16900260756193,
      "grad_norm": 4.204944133758545,
      "learning_rate": 9.155088820078226e-06,
      "loss": 0.3794,
      "step": 401000
    },
    {
      "epoch": 8.179188396349414,
      "grad_norm": 1.7726877927780151,
      "learning_rate": 9.104159876140809e-06,
      "loss": 0.3584,
      "step": 401500
    },
    {
      "epoch": 8.189374185136897,
      "grad_norm": 8.446268081665039,
      "learning_rate": 9.053230932203391e-06,
      "loss": 0.3626,
      "step": 402000
    },
    {
      "epoch": 8.19955997392438,
      "grad_norm": 0.36469566822052,
      "learning_rate": 9.002301988265972e-06,
      "loss": 0.3886,
      "step": 402500
    },
    {
      "epoch": 8.209745762711865,
      "grad_norm": 18.576087951660156,
      "learning_rate": 8.951373044328554e-06,
      "loss": 0.3509,
      "step": 403000
    },
    {
      "epoch": 8.219931551499348,
      "grad_norm": 14.238641738891602,
      "learning_rate": 8.900444100391135e-06,
      "loss": 0.3472,
      "step": 403500
    },
    {
      "epoch": 8.230117340286832,
      "grad_norm": 9.330131530761719,
      "learning_rate": 8.849515156453716e-06,
      "loss": 0.3812,
      "step": 404000
    },
    {
      "epoch": 8.240303129074315,
      "grad_norm": 1.303929090499878,
      "learning_rate": 8.798586212516298e-06,
      "loss": 0.3743,
      "step": 404500
    },
    {
      "epoch": 8.2504889178618,
      "grad_norm": 16.649307250976562,
      "learning_rate": 8.747657268578879e-06,
      "loss": 0.3959,
      "step": 405000
    },
    {
      "epoch": 8.260674706649283,
      "grad_norm": 17.892887115478516,
      "learning_rate": 8.696728324641461e-06,
      "loss": 0.3916,
      "step": 405500
    },
    {
      "epoch": 8.270860495436766,
      "grad_norm": 2.9013092517852783,
      "learning_rate": 8.645799380704042e-06,
      "loss": 0.3501,
      "step": 406000
    },
    {
      "epoch": 8.281046284224251,
      "grad_norm": 16.11052131652832,
      "learning_rate": 8.594870436766623e-06,
      "loss": 0.3765,
      "step": 406500
    },
    {
      "epoch": 8.291232073011734,
      "grad_norm": 23.153188705444336,
      "learning_rate": 8.543941492829205e-06,
      "loss": 0.3461,
      "step": 407000
    },
    {
      "epoch": 8.301417861799218,
      "grad_norm": 11.937457084655762,
      "learning_rate": 8.493012548891786e-06,
      "loss": 0.3718,
      "step": 407500
    },
    {
      "epoch": 8.3116036505867,
      "grad_norm": 0.9255813360214233,
      "learning_rate": 8.442083604954368e-06,
      "loss": 0.3757,
      "step": 408000
    },
    {
      "epoch": 8.321789439374186,
      "grad_norm": 14.357876777648926,
      "learning_rate": 8.391154661016949e-06,
      "loss": 0.3662,
      "step": 408500
    },
    {
      "epoch": 8.331975228161669,
      "grad_norm": 1.7420510053634644,
      "learning_rate": 8.34022571707953e-06,
      "loss": 0.3702,
      "step": 409000
    },
    {
      "epoch": 8.342161016949152,
      "grad_norm": 0.58798748254776,
      "learning_rate": 8.289296773142112e-06,
      "loss": 0.3671,
      "step": 409500
    },
    {
      "epoch": 8.352346805736635,
      "grad_norm": 0.4691517949104309,
      "learning_rate": 8.238367829204693e-06,
      "loss": 0.3635,
      "step": 410000
    },
    {
      "epoch": 8.36253259452412,
      "grad_norm": 14.313126564025879,
      "learning_rate": 8.187438885267275e-06,
      "loss": 0.3626,
      "step": 410500
    },
    {
      "epoch": 8.372718383311604,
      "grad_norm": 9.508600234985352,
      "learning_rate": 8.136509941329858e-06,
      "loss": 0.3877,
      "step": 411000
    },
    {
      "epoch": 8.382904172099087,
      "grad_norm": 20.17707633972168,
      "learning_rate": 8.085580997392439e-06,
      "loss": 0.4009,
      "step": 411500
    },
    {
      "epoch": 8.393089960886572,
      "grad_norm": 13.109078407287598,
      "learning_rate": 8.034652053455021e-06,
      "loss": 0.3735,
      "step": 412000
    },
    {
      "epoch": 8.403275749674055,
      "grad_norm": 8.600504875183105,
      "learning_rate": 7.983723109517602e-06,
      "loss": 0.3627,
      "step": 412500
    },
    {
      "epoch": 8.413461538461538,
      "grad_norm": 0.4347766637802124,
      "learning_rate": 7.932794165580182e-06,
      "loss": 0.3627,
      "step": 413000
    },
    {
      "epoch": 8.423647327249022,
      "grad_norm": 4.800215244293213,
      "learning_rate": 7.881865221642765e-06,
      "loss": 0.3742,
      "step": 413500
    },
    {
      "epoch": 8.433833116036507,
      "grad_norm": 13.257757186889648,
      "learning_rate": 7.830936277705346e-06,
      "loss": 0.3846,
      "step": 414000
    },
    {
      "epoch": 8.44401890482399,
      "grad_norm": 6.879996299743652,
      "learning_rate": 7.780007333767928e-06,
      "loss": 0.355,
      "step": 414500
    },
    {
      "epoch": 8.454204693611473,
      "grad_norm": 10.132291793823242,
      "learning_rate": 7.729078389830509e-06,
      "loss": 0.378,
      "step": 415000
    },
    {
      "epoch": 8.464390482398956,
      "grad_norm": 8.619658470153809,
      "learning_rate": 7.67814944589309e-06,
      "loss": 0.3584,
      "step": 415500
    },
    {
      "epoch": 8.474576271186441,
      "grad_norm": 9.949280738830566,
      "learning_rate": 7.627220501955672e-06,
      "loss": 0.3614,
      "step": 416000
    },
    {
      "epoch": 8.484762059973924,
      "grad_norm": 4.047336101531982,
      "learning_rate": 7.576291558018253e-06,
      "loss": 0.3565,
      "step": 416500
    },
    {
      "epoch": 8.494947848761408,
      "grad_norm": 6.64638090133667,
      "learning_rate": 7.525362614080834e-06,
      "loss": 0.3831,
      "step": 417000
    },
    {
      "epoch": 8.50513363754889,
      "grad_norm": 6.740242004394531,
      "learning_rate": 7.474433670143416e-06,
      "loss": 0.3981,
      "step": 417500
    },
    {
      "epoch": 8.515319426336376,
      "grad_norm": 10.722803115844727,
      "learning_rate": 7.423504726205997e-06,
      "loss": 0.3584,
      "step": 418000
    },
    {
      "epoch": 8.525505215123859,
      "grad_norm": 8.942999839782715,
      "learning_rate": 7.372575782268579e-06,
      "loss": 0.386,
      "step": 418500
    },
    {
      "epoch": 8.535691003911342,
      "grad_norm": 17.671037673950195,
      "learning_rate": 7.3216468383311604e-06,
      "loss": 0.3798,
      "step": 419000
    },
    {
      "epoch": 8.545876792698827,
      "grad_norm": 13.627341270446777,
      "learning_rate": 7.270717894393741e-06,
      "loss": 0.3621,
      "step": 419500
    },
    {
      "epoch": 8.55606258148631,
      "grad_norm": 7.411539554595947,
      "learning_rate": 7.2197889504563244e-06,
      "loss": 0.3697,
      "step": 420000
    },
    {
      "epoch": 8.566248370273794,
      "grad_norm": 14.123669624328613,
      "learning_rate": 7.168860006518906e-06,
      "loss": 0.3609,
      "step": 420500
    },
    {
      "epoch": 8.576434159061277,
      "grad_norm": 7.909858226776123,
      "learning_rate": 7.117931062581487e-06,
      "loss": 0.3349,
      "step": 421000
    },
    {
      "epoch": 8.586619947848762,
      "grad_norm": 3.1924731731414795,
      "learning_rate": 7.067002118644068e-06,
      "loss": 0.3647,
      "step": 421500
    },
    {
      "epoch": 8.596805736636245,
      "grad_norm": 18.755977630615234,
      "learning_rate": 7.01607317470665e-06,
      "loss": 0.3934,
      "step": 422000
    },
    {
      "epoch": 8.606991525423728,
      "grad_norm": 1.1282291412353516,
      "learning_rate": 6.9651442307692314e-06,
      "loss": 0.3622,
      "step": 422500
    },
    {
      "epoch": 8.617177314211212,
      "grad_norm": 16.264537811279297,
      "learning_rate": 6.914215286831813e-06,
      "loss": 0.3797,
      "step": 423000
    },
    {
      "epoch": 8.627363102998697,
      "grad_norm": 8.78809642791748,
      "learning_rate": 6.863286342894394e-06,
      "loss": 0.388,
      "step": 423500
    },
    {
      "epoch": 8.63754889178618,
      "grad_norm": 12.650059700012207,
      "learning_rate": 6.812357398956975e-06,
      "loss": 0.3716,
      "step": 424000
    },
    {
      "epoch": 8.647734680573663,
      "grad_norm": 11.738743782043457,
      "learning_rate": 6.761428455019557e-06,
      "loss": 0.3859,
      "step": 424500
    },
    {
      "epoch": 8.657920469361148,
      "grad_norm": 5.896023273468018,
      "learning_rate": 6.7104995110821385e-06,
      "loss": 0.3704,
      "step": 425000
    },
    {
      "epoch": 8.668106258148631,
      "grad_norm": 2.607734203338623,
      "learning_rate": 6.65957056714472e-06,
      "loss": 0.4028,
      "step": 425500
    },
    {
      "epoch": 8.678292046936114,
      "grad_norm": 12.511457443237305,
      "learning_rate": 6.608641623207301e-06,
      "loss": 0.3623,
      "step": 426000
    },
    {
      "epoch": 8.688477835723598,
      "grad_norm": 8.350826263427734,
      "learning_rate": 6.557712679269882e-06,
      "loss": 0.3759,
      "step": 426500
    },
    {
      "epoch": 8.698663624511083,
      "grad_norm": 1.5048482418060303,
      "learning_rate": 6.506783735332464e-06,
      "loss": 0.3519,
      "step": 427000
    },
    {
      "epoch": 8.708849413298566,
      "grad_norm": 10.451533317565918,
      "learning_rate": 6.4558547913950455e-06,
      "loss": 0.3789,
      "step": 427500
    },
    {
      "epoch": 8.719035202086049,
      "grad_norm": 8.767901420593262,
      "learning_rate": 6.404925847457627e-06,
      "loss": 0.3637,
      "step": 428000
    },
    {
      "epoch": 8.729220990873532,
      "grad_norm": 19.800128936767578,
      "learning_rate": 6.3539969035202095e-06,
      "loss": 0.3705,
      "step": 428500
    },
    {
      "epoch": 8.739406779661017,
      "grad_norm": 12.445290565490723,
      "learning_rate": 6.303067959582791e-06,
      "loss": 0.361,
      "step": 429000
    },
    {
      "epoch": 8.7495925684485,
      "grad_norm": 12.19664192199707,
      "learning_rate": 6.252139015645373e-06,
      "loss": 0.3619,
      "step": 429500
    },
    {
      "epoch": 8.759778357235984,
      "grad_norm": 10.311057090759277,
      "learning_rate": 6.201210071707953e-06,
      "loss": 0.3757,
      "step": 430000
    },
    {
      "epoch": 8.769964146023469,
      "grad_norm": 5.46469783782959,
      "learning_rate": 6.150281127770535e-06,
      "loss": 0.3651,
      "step": 430500
    },
    {
      "epoch": 8.780149934810952,
      "grad_norm": 5.59713888168335,
      "learning_rate": 6.0993521838331165e-06,
      "loss": 0.3616,
      "step": 431000
    },
    {
      "epoch": 8.790335723598435,
      "grad_norm": 8.243121147155762,
      "learning_rate": 6.048423239895698e-06,
      "loss": 0.3912,
      "step": 431500
    },
    {
      "epoch": 8.800521512385918,
      "grad_norm": 1.5688707828521729,
      "learning_rate": 5.99749429595828e-06,
      "loss": 0.3766,
      "step": 432000
    },
    {
      "epoch": 8.810707301173403,
      "grad_norm": 8.875506401062012,
      "learning_rate": 5.946565352020861e-06,
      "loss": 0.3657,
      "step": 432500
    },
    {
      "epoch": 8.820893089960887,
      "grad_norm": 7.234671115875244,
      "learning_rate": 5.895636408083442e-06,
      "loss": 0.3657,
      "step": 433000
    },
    {
      "epoch": 8.83107887874837,
      "grad_norm": 10.306873321533203,
      "learning_rate": 5.8447074641460235e-06,
      "loss": 0.3744,
      "step": 433500
    },
    {
      "epoch": 8.841264667535853,
      "grad_norm": 0.21253645420074463,
      "learning_rate": 5.793778520208605e-06,
      "loss": 0.3564,
      "step": 434000
    },
    {
      "epoch": 8.851450456323338,
      "grad_norm": 19.21792221069336,
      "learning_rate": 5.742849576271187e-06,
      "loss": 0.3887,
      "step": 434500
    },
    {
      "epoch": 8.861636245110821,
      "grad_norm": 4.354195594787598,
      "learning_rate": 5.691920632333768e-06,
      "loss": 0.3814,
      "step": 435000
    },
    {
      "epoch": 8.871822033898304,
      "grad_norm": 12.754733085632324,
      "learning_rate": 5.64099168839635e-06,
      "loss": 0.362,
      "step": 435500
    },
    {
      "epoch": 8.88200782268579,
      "grad_norm": 11.548566818237305,
      "learning_rate": 5.590062744458931e-06,
      "loss": 0.3653,
      "step": 436000
    },
    {
      "epoch": 8.892193611473273,
      "grad_norm": 3.303117036819458,
      "learning_rate": 5.539133800521513e-06,
      "loss": 0.3657,
      "step": 436500
    },
    {
      "epoch": 8.902379400260756,
      "grad_norm": 23.371248245239258,
      "learning_rate": 5.4882048565840945e-06,
      "loss": 0.3681,
      "step": 437000
    },
    {
      "epoch": 8.91256518904824,
      "grad_norm": 0.6297703385353088,
      "learning_rate": 5.437275912646675e-06,
      "loss": 0.3649,
      "step": 437500
    },
    {
      "epoch": 8.922750977835724,
      "grad_norm": 7.30040168762207,
      "learning_rate": 5.386346968709257e-06,
      "loss": 0.3749,
      "step": 438000
    },
    {
      "epoch": 8.932936766623207,
      "grad_norm": 6.9770121574401855,
      "learning_rate": 5.335418024771838e-06,
      "loss": 0.3805,
      "step": 438500
    },
    {
      "epoch": 8.94312255541069,
      "grad_norm": 7.873815536499023,
      "learning_rate": 5.28448908083442e-06,
      "loss": 0.3903,
      "step": 439000
    },
    {
      "epoch": 8.953308344198174,
      "grad_norm": 12.255332946777344,
      "learning_rate": 5.2335601368970015e-06,
      "loss": 0.3776,
      "step": 439500
    },
    {
      "epoch": 8.963494132985659,
      "grad_norm": 0.7379159927368164,
      "learning_rate": 5.182631192959583e-06,
      "loss": 0.3639,
      "step": 440000
    },
    {
      "epoch": 8.973679921773142,
      "grad_norm": 20.07736587524414,
      "learning_rate": 5.131702249022165e-06,
      "loss": 0.3733,
      "step": 440500
    },
    {
      "epoch": 8.983865710560625,
      "grad_norm": 14.658787727355957,
      "learning_rate": 5.080773305084746e-06,
      "loss": 0.3545,
      "step": 441000
    },
    {
      "epoch": 8.99405149934811,
      "grad_norm": 0.5499163866043091,
      "learning_rate": 5.029844361147328e-06,
      "loss": 0.3576,
      "step": 441500
    },
    {
      "epoch": 9.004237288135593,
      "grad_norm": 8.45683479309082,
      "learning_rate": 4.9789154172099085e-06,
      "loss": 0.3744,
      "step": 442000
    },
    {
      "epoch": 9.014423076923077,
      "grad_norm": 0.553778350353241,
      "learning_rate": 4.92798647327249e-06,
      "loss": 0.3644,
      "step": 442500
    },
    {
      "epoch": 9.02460886571056,
      "grad_norm": 0.8118293881416321,
      "learning_rate": 4.877057529335072e-06,
      "loss": 0.3414,
      "step": 443000
    },
    {
      "epoch": 9.034794654498045,
      "grad_norm": 11.800994873046875,
      "learning_rate": 4.826128585397653e-06,
      "loss": 0.3602,
      "step": 443500
    },
    {
      "epoch": 9.044980443285528,
      "grad_norm": 10.313807487487793,
      "learning_rate": 4.775199641460235e-06,
      "loss": 0.3787,
      "step": 444000
    },
    {
      "epoch": 9.055166232073011,
      "grad_norm": 16.658180236816406,
      "learning_rate": 4.724270697522816e-06,
      "loss": 0.3581,
      "step": 444500
    },
    {
      "epoch": 9.065352020860496,
      "grad_norm": 10.086774826049805,
      "learning_rate": 4.673341753585398e-06,
      "loss": 0.3392,
      "step": 445000
    },
    {
      "epoch": 9.07553780964798,
      "grad_norm": 16.305002212524414,
      "learning_rate": 4.6224128096479795e-06,
      "loss": 0.3526,
      "step": 445500
    },
    {
      "epoch": 9.085723598435463,
      "grad_norm": 5.340000629425049,
      "learning_rate": 4.571483865710561e-06,
      "loss": 0.3683,
      "step": 446000
    },
    {
      "epoch": 9.095909387222946,
      "grad_norm": 16.549835205078125,
      "learning_rate": 4.520554921773143e-06,
      "loss": 0.3885,
      "step": 446500
    },
    {
      "epoch": 9.106095176010431,
      "grad_norm": 8.723512649536133,
      "learning_rate": 4.469625977835723e-06,
      "loss": 0.3962,
      "step": 447000
    },
    {
      "epoch": 9.116280964797914,
      "grad_norm": 6.502577304840088,
      "learning_rate": 4.418697033898305e-06,
      "loss": 0.3648,
      "step": 447500
    },
    {
      "epoch": 9.126466753585397,
      "grad_norm": 6.856087684631348,
      "learning_rate": 4.3677680899608866e-06,
      "loss": 0.3432,
      "step": 448000
    },
    {
      "epoch": 9.13665254237288,
      "grad_norm": 13.229925155639648,
      "learning_rate": 4.316839146023468e-06,
      "loss": 0.3746,
      "step": 448500
    },
    {
      "epoch": 9.146838331160366,
      "grad_norm": 13.217467308044434,
      "learning_rate": 4.26591020208605e-06,
      "loss": 0.3855,
      "step": 449000
    },
    {
      "epoch": 9.157024119947849,
      "grad_norm": 3.719346761703491,
      "learning_rate": 4.214981258148631e-06,
      "loss": 0.3516,
      "step": 449500
    },
    {
      "epoch": 9.167209908735332,
      "grad_norm": 9.46267318725586,
      "learning_rate": 4.164052314211213e-06,
      "loss": 0.3598,
      "step": 450000
    },
    {
      "epoch": 9.177395697522817,
      "grad_norm": 3.235654830932617,
      "learning_rate": 4.113123370273794e-06,
      "loss": 0.3603,
      "step": 450500
    },
    {
      "epoch": 9.1875814863103,
      "grad_norm": 1.4643198251724243,
      "learning_rate": 4.062194426336376e-06,
      "loss": 0.346,
      "step": 451000
    },
    {
      "epoch": 9.197767275097783,
      "grad_norm": 12.98554515838623,
      "learning_rate": 4.011265482398957e-06,
      "loss": 0.3679,
      "step": 451500
    },
    {
      "epoch": 9.207953063885267,
      "grad_norm": 1.434821367263794,
      "learning_rate": 3.960336538461538e-06,
      "loss": 0.401,
      "step": 452000
    },
    {
      "epoch": 9.218138852672752,
      "grad_norm": 14.40749454498291,
      "learning_rate": 3.90940759452412e-06,
      "loss": 0.3698,
      "step": 452500
    },
    {
      "epoch": 9.228324641460235,
      "grad_norm": 24.185752868652344,
      "learning_rate": 3.858478650586702e-06,
      "loss": 0.3544,
      "step": 453000
    },
    {
      "epoch": 9.238510430247718,
      "grad_norm": 15.140535354614258,
      "learning_rate": 3.8075497066492834e-06,
      "loss": 0.357,
      "step": 453500
    },
    {
      "epoch": 9.248696219035201,
      "grad_norm": 19.544395446777344,
      "learning_rate": 3.7566207627118646e-06,
      "loss": 0.3807,
      "step": 454000
    },
    {
      "epoch": 9.258882007822686,
      "grad_norm": 11.698213577270508,
      "learning_rate": 3.705691818774446e-06,
      "loss": 0.3521,
      "step": 454500
    },
    {
      "epoch": 9.26906779661017,
      "grad_norm": 12.622817993164062,
      "learning_rate": 3.6547628748370277e-06,
      "loss": 0.3605,
      "step": 455000
    },
    {
      "epoch": 9.279253585397653,
      "grad_norm": 11.410058975219727,
      "learning_rate": 3.603833930899609e-06,
      "loss": 0.3785,
      "step": 455500
    },
    {
      "epoch": 9.289439374185136,
      "grad_norm": 18.14484214782715,
      "learning_rate": 3.5529049869621905e-06,
      "loss": 0.3728,
      "step": 456000
    },
    {
      "epoch": 9.299625162972621,
      "grad_norm": 21.794666290283203,
      "learning_rate": 3.5019760430247716e-06,
      "loss": 0.3426,
      "step": 456500
    },
    {
      "epoch": 9.309810951760104,
      "grad_norm": 16.788776397705078,
      "learning_rate": 3.451047099087353e-06,
      "loss": 0.3823,
      "step": 457000
    },
    {
      "epoch": 9.319996740547587,
      "grad_norm": 0.6756789088249207,
      "learning_rate": 3.400118155149935e-06,
      "loss": 0.395,
      "step": 457500
    },
    {
      "epoch": 9.330182529335072,
      "grad_norm": 0.8369967341423035,
      "learning_rate": 3.3491892112125167e-06,
      "loss": 0.4007,
      "step": 458000
    },
    {
      "epoch": 9.340368318122556,
      "grad_norm": 2.100008726119995,
      "learning_rate": 3.2982602672750983e-06,
      "loss": 0.356,
      "step": 458500
    },
    {
      "epoch": 9.350554106910039,
      "grad_norm": 7.690422534942627,
      "learning_rate": 3.2473313233376795e-06,
      "loss": 0.354,
      "step": 459000
    },
    {
      "epoch": 9.360739895697522,
      "grad_norm": 18.316835403442383,
      "learning_rate": 3.196402379400261e-06,
      "loss": 0.3796,
      "step": 459500
    },
    {
      "epoch": 9.370925684485007,
      "grad_norm": 5.806652545928955,
      "learning_rate": 3.145473435462842e-06,
      "loss": 0.3722,
      "step": 460000
    },
    {
      "epoch": 9.38111147327249,
      "grad_norm": 10.178348541259766,
      "learning_rate": 3.0945444915254238e-06,
      "loss": 0.3788,
      "step": 460500
    },
    {
      "epoch": 9.391297262059974,
      "grad_norm": 15.279345512390137,
      "learning_rate": 3.0436155475880053e-06,
      "loss": 0.3542,
      "step": 461000
    },
    {
      "epoch": 9.401483050847457,
      "grad_norm": 1.9593571424484253,
      "learning_rate": 2.992686603650587e-06,
      "loss": 0.3702,
      "step": 461500
    },
    {
      "epoch": 9.411668839634942,
      "grad_norm": 12.608344078063965,
      "learning_rate": 2.9417576597131685e-06,
      "loss": 0.3491,
      "step": 462000
    },
    {
      "epoch": 9.421854628422425,
      "grad_norm": 9.165721893310547,
      "learning_rate": 2.8908287157757496e-06,
      "loss": 0.3802,
      "step": 462500
    },
    {
      "epoch": 9.432040417209908,
      "grad_norm": 12.768163681030273,
      "learning_rate": 2.839899771838331e-06,
      "loss": 0.3819,
      "step": 463000
    },
    {
      "epoch": 9.442226205997393,
      "grad_norm": 19.169231414794922,
      "learning_rate": 2.7889708279009128e-06,
      "loss": 0.3413,
      "step": 463500
    },
    {
      "epoch": 9.452411994784876,
      "grad_norm": 1.3248894214630127,
      "learning_rate": 2.7380418839634943e-06,
      "loss": 0.3715,
      "step": 464000
    },
    {
      "epoch": 9.46259778357236,
      "grad_norm": 2.143557548522949,
      "learning_rate": 2.6871129400260755e-06,
      "loss": 0.3384,
      "step": 464500
    },
    {
      "epoch": 9.472783572359843,
      "grad_norm": 26.246511459350586,
      "learning_rate": 2.636183996088657e-06,
      "loss": 0.3873,
      "step": 465000
    },
    {
      "epoch": 9.482969361147328,
      "grad_norm": 23.62848663330078,
      "learning_rate": 2.585255052151239e-06,
      "loss": 0.3781,
      "step": 465500
    },
    {
      "epoch": 9.493155149934811,
      "grad_norm": 17.795028686523438,
      "learning_rate": 2.53432610821382e-06,
      "loss": 0.3697,
      "step": 466000
    },
    {
      "epoch": 9.503340938722294,
      "grad_norm": 5.64040470123291,
      "learning_rate": 2.4833971642764018e-06,
      "loss": 0.3627,
      "step": 466500
    },
    {
      "epoch": 9.513526727509777,
      "grad_norm": 19.828981399536133,
      "learning_rate": 2.432468220338983e-06,
      "loss": 0.3653,
      "step": 467000
    },
    {
      "epoch": 9.523712516297262,
      "grad_norm": 7.251101493835449,
      "learning_rate": 2.381539276401565e-06,
      "loss": 0.3826,
      "step": 467500
    },
    {
      "epoch": 9.533898305084746,
      "grad_norm": 9.50715160369873,
      "learning_rate": 2.330610332464146e-06,
      "loss": 0.3798,
      "step": 468000
    },
    {
      "epoch": 9.544084093872229,
      "grad_norm": 13.432284355163574,
      "learning_rate": 2.2796813885267276e-06,
      "loss": 0.3453,
      "step": 468500
    },
    {
      "epoch": 9.554269882659714,
      "grad_norm": 12.3894624710083,
      "learning_rate": 2.2287524445893092e-06,
      "loss": 0.3666,
      "step": 469000
    },
    {
      "epoch": 9.564455671447197,
      "grad_norm": 17.06966781616211,
      "learning_rate": 2.1778235006518904e-06,
      "loss": 0.348,
      "step": 469500
    },
    {
      "epoch": 9.57464146023468,
      "grad_norm": 6.854415416717529,
      "learning_rate": 2.1268945567144724e-06,
      "loss": 0.3748,
      "step": 470000
    },
    {
      "epoch": 9.584827249022164,
      "grad_norm": 1.2926026582717896,
      "learning_rate": 2.0759656127770535e-06,
      "loss": 0.3728,
      "step": 470500
    },
    {
      "epoch": 9.595013037809649,
      "grad_norm": 10.185200691223145,
      "learning_rate": 2.025036668839635e-06,
      "loss": 0.3638,
      "step": 471000
    },
    {
      "epoch": 9.605198826597132,
      "grad_norm": 20.161108016967773,
      "learning_rate": 1.9741077249022162e-06,
      "loss": 0.3493,
      "step": 471500
    },
    {
      "epoch": 9.615384615384615,
      "grad_norm": 7.827825546264648,
      "learning_rate": 1.9231787809647982e-06,
      "loss": 0.3516,
      "step": 472000
    },
    {
      "epoch": 9.625570404172098,
      "grad_norm": 20.55687141418457,
      "learning_rate": 1.8722498370273796e-06,
      "loss": 0.3637,
      "step": 472500
    },
    {
      "epoch": 9.635756192959583,
      "grad_norm": 2.8905246257781982,
      "learning_rate": 1.821320893089961e-06,
      "loss": 0.3627,
      "step": 473000
    },
    {
      "epoch": 9.645941981747066,
      "grad_norm": 11.090469360351562,
      "learning_rate": 1.7703919491525423e-06,
      "loss": 0.3321,
      "step": 473500
    },
    {
      "epoch": 9.65612777053455,
      "grad_norm": 20.05911636352539,
      "learning_rate": 1.7194630052151239e-06,
      "loss": 0.3622,
      "step": 474000
    },
    {
      "epoch": 9.666313559322035,
      "grad_norm": 14.225946426391602,
      "learning_rate": 1.6685340612777055e-06,
      "loss": 0.3693,
      "step": 474500
    },
    {
      "epoch": 9.676499348109518,
      "grad_norm": 1.411246657371521,
      "learning_rate": 1.617605117340287e-06,
      "loss": 0.3896,
      "step": 475000
    },
    {
      "epoch": 9.686685136897001,
      "grad_norm": 19.906570434570312,
      "learning_rate": 1.5666761734028684e-06,
      "loss": 0.3769,
      "step": 475500
    },
    {
      "epoch": 9.696870925684484,
      "grad_norm": 16.541772842407227,
      "learning_rate": 1.51574722946545e-06,
      "loss": 0.3609,
      "step": 476000
    },
    {
      "epoch": 9.70705671447197,
      "grad_norm": 14.787666320800781,
      "learning_rate": 1.4648182855280313e-06,
      "loss": 0.3702,
      "step": 476500
    },
    {
      "epoch": 9.717242503259452,
      "grad_norm": 7.485085964202881,
      "learning_rate": 1.413889341590613e-06,
      "loss": 0.3772,
      "step": 477000
    },
    {
      "epoch": 9.727428292046936,
      "grad_norm": 25.312602996826172,
      "learning_rate": 1.3629603976531943e-06,
      "loss": 0.3366,
      "step": 477500
    },
    {
      "epoch": 9.737614080834419,
      "grad_norm": 15.03630256652832,
      "learning_rate": 1.3120314537157758e-06,
      "loss": 0.3679,
      "step": 478000
    },
    {
      "epoch": 9.747799869621904,
      "grad_norm": 1.3218810558319092,
      "learning_rate": 1.2611025097783574e-06,
      "loss": 0.3623,
      "step": 478500
    },
    {
      "epoch": 9.757985658409387,
      "grad_norm": 8.924858093261719,
      "learning_rate": 1.2101735658409388e-06,
      "loss": 0.3647,
      "step": 479000
    },
    {
      "epoch": 9.76817144719687,
      "grad_norm": 8.094156265258789,
      "learning_rate": 1.1592446219035203e-06,
      "loss": 0.3902,
      "step": 479500
    },
    {
      "epoch": 9.778357235984355,
      "grad_norm": 5.921751976013184,
      "learning_rate": 1.1083156779661017e-06,
      "loss": 0.3601,
      "step": 480000
    },
    {
      "epoch": 9.788543024771839,
      "grad_norm": 10.410297393798828,
      "learning_rate": 1.0573867340286833e-06,
      "loss": 0.3705,
      "step": 480500
    },
    {
      "epoch": 9.798728813559322,
      "grad_norm": 18.429630279541016,
      "learning_rate": 1.0064577900912646e-06,
      "loss": 0.3582,
      "step": 481000
    },
    {
      "epoch": 9.808914602346805,
      "grad_norm": 29.042818069458008,
      "learning_rate": 9.555288461538462e-07,
      "loss": 0.3617,
      "step": 481500
    },
    {
      "epoch": 9.81910039113429,
      "grad_norm": 22.078920364379883,
      "learning_rate": 9.045999022164277e-07,
      "loss": 0.352,
      "step": 482000
    },
    {
      "epoch": 9.829286179921773,
      "grad_norm": 10.162459373474121,
      "learning_rate": 8.536709582790091e-07,
      "loss": 0.3595,
      "step": 482500
    },
    {
      "epoch": 9.839471968709256,
      "grad_norm": 9.22619342803955,
      "learning_rate": 8.027420143415907e-07,
      "loss": 0.3621,
      "step": 483000
    },
    {
      "epoch": 9.84965775749674,
      "grad_norm": 0.5626074075698853,
      "learning_rate": 7.518130704041722e-07,
      "loss": 0.3681,
      "step": 483500
    },
    {
      "epoch": 9.859843546284225,
      "grad_norm": 15.529434204101562,
      "learning_rate": 7.008841264667536e-07,
      "loss": 0.3816,
      "step": 484000
    },
    {
      "epoch": 9.870029335071708,
      "grad_norm": 13.261473655700684,
      "learning_rate": 6.499551825293351e-07,
      "loss": 0.3563,
      "step": 484500
    },
    {
      "epoch": 9.880215123859191,
      "grad_norm": 10.644469261169434,
      "learning_rate": 5.990262385919166e-07,
      "loss": 0.3648,
      "step": 485000
    },
    {
      "epoch": 9.890400912646676,
      "grad_norm": 0.8997551798820496,
      "learning_rate": 5.48097294654498e-07,
      "loss": 0.3415,
      "step": 485500
    },
    {
      "epoch": 9.90058670143416,
      "grad_norm": 16.082136154174805,
      "learning_rate": 4.971683507170795e-07,
      "loss": 0.3992,
      "step": 486000
    },
    {
      "epoch": 9.910772490221643,
      "grad_norm": 8.369975090026855,
      "learning_rate": 4.46239406779661e-07,
      "loss": 0.3458,
      "step": 486500
    },
    {
      "epoch": 9.920958279009126,
      "grad_norm": 11.344599723815918,
      "learning_rate": 3.9531046284224254e-07,
      "loss": 0.3516,
      "step": 487000
    },
    {
      "epoch": 9.93114406779661,
      "grad_norm": 15.96269416809082,
      "learning_rate": 3.44381518904824e-07,
      "loss": 0.3597,
      "step": 487500
    },
    {
      "epoch": 9.941329856584094,
      "grad_norm": 9.860033988952637,
      "learning_rate": 2.9345257496740553e-07,
      "loss": 0.3828,
      "step": 488000
    },
    {
      "epoch": 9.951515645371577,
      "grad_norm": 6.501396179199219,
      "learning_rate": 2.4252363102998694e-07,
      "loss": 0.37,
      "step": 488500
    },
    {
      "epoch": 9.96170143415906,
      "grad_norm": 7.235980033874512,
      "learning_rate": 1.9159468709256844e-07,
      "loss": 0.364,
      "step": 489000
    },
    {
      "epoch": 9.971887222946545,
      "grad_norm": 2.7200093269348145,
      "learning_rate": 1.4066574315514993e-07,
      "loss": 0.3433,
      "step": 489500
    },
    {
      "epoch": 9.982073011734029,
      "grad_norm": 8.561784744262695,
      "learning_rate": 8.973679921773142e-08,
      "loss": 0.3552,
      "step": 490000
    },
    {
      "epoch": 9.992258800521512,
      "grad_norm": 13.532429695129395,
      "learning_rate": 3.880785528031291e-08,
      "loss": 0.3639,
      "step": 490500
    },
    {
      "epoch": 10.0,
      "step": 490880,
      "total_flos": 9.167195214996173e+17,
      "train_loss": 0.4184407115759719,
      "train_runtime": 74966.0369,
      "train_samples_per_second": 52.384,
      "train_steps_per_second": 6.548
    }
  ],
  "logging_steps": 500,
  "max_steps": 490880,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9.167195214996173e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
