{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 490880,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.010185788787483703,
      "grad_norm": 10.007597923278809,
      "learning_rate": 4.994917291395046e-05,
      "loss": 1.1192,
      "step": 500
    },
    {
      "epoch": 0.020371577574967405,
      "grad_norm": 5.845461368560791,
      "learning_rate": 4.9898243970013045e-05,
      "loss": 1.0504,
      "step": 1000
    },
    {
      "epoch": 0.03055736636245111,
      "grad_norm": 11.532530784606934,
      "learning_rate": 4.984731502607562e-05,
      "loss": 0.9118,
      "step": 1500
    },
    {
      "epoch": 0.04074315514993481,
      "grad_norm": 8.022780418395996,
      "learning_rate": 4.97963860821382e-05,
      "loss": 0.7973,
      "step": 2000
    },
    {
      "epoch": 0.050928943937418515,
      "grad_norm": 7.850565433502197,
      "learning_rate": 4.974545713820079e-05,
      "loss": 0.7765,
      "step": 2500
    },
    {
      "epoch": 0.06111473272490222,
      "grad_norm": 6.9862470626831055,
      "learning_rate": 4.969452819426337e-05,
      "loss": 0.7384,
      "step": 3000
    },
    {
      "epoch": 0.07130052151238592,
      "grad_norm": 8.865849494934082,
      "learning_rate": 4.964359925032595e-05,
      "loss": 0.7298,
      "step": 3500
    },
    {
      "epoch": 0.08148631029986962,
      "grad_norm": 8.861352920532227,
      "learning_rate": 4.959267030638853e-05,
      "loss": 0.7102,
      "step": 4000
    },
    {
      "epoch": 0.09167209908735333,
      "grad_norm": 5.155445575714111,
      "learning_rate": 4.954174136245111e-05,
      "loss": 0.6915,
      "step": 4500
    },
    {
      "epoch": 0.10185788787483703,
      "grad_norm": 9.302899360656738,
      "learning_rate": 4.949081241851369e-05,
      "loss": 0.6868,
      "step": 5000
    },
    {
      "epoch": 0.11204367666232073,
      "grad_norm": 5.192394256591797,
      "learning_rate": 4.943988347457628e-05,
      "loss": 0.649,
      "step": 5500
    },
    {
      "epoch": 0.12222946544980444,
      "grad_norm": 6.716037750244141,
      "learning_rate": 4.938895453063885e-05,
      "loss": 0.6318,
      "step": 6000
    },
    {
      "epoch": 0.13241525423728814,
      "grad_norm": 5.188283443450928,
      "learning_rate": 4.933802558670144e-05,
      "loss": 0.6345,
      "step": 6500
    },
    {
      "epoch": 0.14260104302477183,
      "grad_norm": 6.4745073318481445,
      "learning_rate": 4.928709664276402e-05,
      "loss": 0.6365,
      "step": 7000
    },
    {
      "epoch": 0.15278683181225555,
      "grad_norm": 8.544636726379395,
      "learning_rate": 4.92361676988266e-05,
      "loss": 0.6379,
      "step": 7500
    },
    {
      "epoch": 0.16297262059973924,
      "grad_norm": 5.642955303192139,
      "learning_rate": 4.918523875488918e-05,
      "loss": 0.6272,
      "step": 8000
    },
    {
      "epoch": 0.17315840938722293,
      "grad_norm": 5.398521423339844,
      "learning_rate": 4.913430981095176e-05,
      "loss": 0.6194,
      "step": 8500
    },
    {
      "epoch": 0.18334419817470665,
      "grad_norm": 6.520516872406006,
      "learning_rate": 4.908338086701434e-05,
      "loss": 0.6129,
      "step": 9000
    },
    {
      "epoch": 0.19352998696219034,
      "grad_norm": 5.600179672241211,
      "learning_rate": 4.903245192307693e-05,
      "loss": 0.5884,
      "step": 9500
    },
    {
      "epoch": 0.20371577574967406,
      "grad_norm": 6.569949150085449,
      "learning_rate": 4.8981522979139504e-05,
      "loss": 0.6093,
      "step": 10000
    },
    {
      "epoch": 0.21390156453715775,
      "grad_norm": 21.149824142456055,
      "learning_rate": 4.8930594035202085e-05,
      "loss": 0.596,
      "step": 10500
    },
    {
      "epoch": 0.22408735332464147,
      "grad_norm": 7.378972053527832,
      "learning_rate": 4.887966509126467e-05,
      "loss": 0.5972,
      "step": 11000
    },
    {
      "epoch": 0.23427314211212516,
      "grad_norm": 10.64529800415039,
      "learning_rate": 4.8828736147327253e-05,
      "loss": 0.5914,
      "step": 11500
    },
    {
      "epoch": 0.24445893089960888,
      "grad_norm": 7.1362738609313965,
      "learning_rate": 4.877780720338983e-05,
      "loss": 0.5784,
      "step": 12000
    },
    {
      "epoch": 0.25464471968709257,
      "grad_norm": 21.285118103027344,
      "learning_rate": 4.8726878259452415e-05,
      "loss": 0.5773,
      "step": 12500
    },
    {
      "epoch": 0.2648305084745763,
      "grad_norm": 9.944624900817871,
      "learning_rate": 4.8675949315514996e-05,
      "loss": 0.6056,
      "step": 13000
    },
    {
      "epoch": 0.27501629726205995,
      "grad_norm": 9.155661582946777,
      "learning_rate": 4.8625020371577576e-05,
      "loss": 0.5755,
      "step": 13500
    },
    {
      "epoch": 0.28520208604954367,
      "grad_norm": 12.586216926574707,
      "learning_rate": 4.8574091427640164e-05,
      "loss": 0.5679,
      "step": 14000
    },
    {
      "epoch": 0.2953878748370274,
      "grad_norm": 13.869895935058594,
      "learning_rate": 4.852316248370274e-05,
      "loss": 0.5588,
      "step": 14500
    },
    {
      "epoch": 0.3055736636245111,
      "grad_norm": 7.964175701141357,
      "learning_rate": 4.847223353976532e-05,
      "loss": 0.5639,
      "step": 15000
    },
    {
      "epoch": 0.31575945241199477,
      "grad_norm": 7.460739612579346,
      "learning_rate": 4.8421304595827906e-05,
      "loss": 0.5656,
      "step": 15500
    },
    {
      "epoch": 0.3259452411994785,
      "grad_norm": 5.336625576019287,
      "learning_rate": 4.837037565189049e-05,
      "loss": 0.5383,
      "step": 16000
    },
    {
      "epoch": 0.3361310299869622,
      "grad_norm": 15.894767761230469,
      "learning_rate": 4.831944670795307e-05,
      "loss": 0.54,
      "step": 16500
    },
    {
      "epoch": 0.34631681877444587,
      "grad_norm": 7.05601692199707,
      "learning_rate": 4.826851776401565e-05,
      "loss": 0.5463,
      "step": 17000
    },
    {
      "epoch": 0.3565026075619296,
      "grad_norm": 10.49486255645752,
      "learning_rate": 4.821758882007823e-05,
      "loss": 0.5833,
      "step": 17500
    },
    {
      "epoch": 0.3666883963494133,
      "grad_norm": 10.158638000488281,
      "learning_rate": 4.816665987614081e-05,
      "loss": 0.5497,
      "step": 18000
    },
    {
      "epoch": 0.376874185136897,
      "grad_norm": 11.732033729553223,
      "learning_rate": 4.811573093220339e-05,
      "loss": 0.5627,
      "step": 18500
    },
    {
      "epoch": 0.3870599739243807,
      "grad_norm": 8.300926208496094,
      "learning_rate": 4.806480198826597e-05,
      "loss": 0.5412,
      "step": 19000
    },
    {
      "epoch": 0.3972457627118644,
      "grad_norm": 4.805948257446289,
      "learning_rate": 4.801387304432856e-05,
      "loss": 0.5303,
      "step": 19500
    },
    {
      "epoch": 0.4074315514993481,
      "grad_norm": 10.348479270935059,
      "learning_rate": 4.796294410039114e-05,
      "loss": 0.5556,
      "step": 20000
    },
    {
      "epoch": 0.41761734028683184,
      "grad_norm": 5.989911079406738,
      "learning_rate": 4.791201515645371e-05,
      "loss": 0.5382,
      "step": 20500
    },
    {
      "epoch": 0.4278031290743155,
      "grad_norm": 19.4819393157959,
      "learning_rate": 4.78610862125163e-05,
      "loss": 0.53,
      "step": 21000
    },
    {
      "epoch": 0.4379889178617992,
      "grad_norm": 8.32656478881836,
      "learning_rate": 4.781015726857888e-05,
      "loss": 0.5595,
      "step": 21500
    },
    {
      "epoch": 0.44817470664928294,
      "grad_norm": 16.594892501831055,
      "learning_rate": 4.775922832464146e-05,
      "loss": 0.542,
      "step": 22000
    },
    {
      "epoch": 0.4583604954367666,
      "grad_norm": 7.145063877105713,
      "learning_rate": 4.770829938070404e-05,
      "loss": 0.5448,
      "step": 22500
    },
    {
      "epoch": 0.4685462842242503,
      "grad_norm": 13.60429573059082,
      "learning_rate": 4.7657370436766624e-05,
      "loss": 0.5412,
      "step": 23000
    },
    {
      "epoch": 0.47873207301173404,
      "grad_norm": 5.566845893859863,
      "learning_rate": 4.7606441492829204e-05,
      "loss": 0.5352,
      "step": 23500
    },
    {
      "epoch": 0.48891786179921776,
      "grad_norm": 7.769102096557617,
      "learning_rate": 4.755551254889179e-05,
      "loss": 0.5506,
      "step": 24000
    },
    {
      "epoch": 0.4991036505867014,
      "grad_norm": 10.403070449829102,
      "learning_rate": 4.750458360495437e-05,
      "loss": 0.5274,
      "step": 24500
    },
    {
      "epoch": 0.5092894393741851,
      "grad_norm": 11.211812973022461,
      "learning_rate": 4.7453654661016947e-05,
      "loss": 0.5301,
      "step": 25000
    },
    {
      "epoch": 0.5194752281616688,
      "grad_norm": 7.675836563110352,
      "learning_rate": 4.7402725717079534e-05,
      "loss": 0.5549,
      "step": 25500
    },
    {
      "epoch": 0.5296610169491526,
      "grad_norm": 5.49832010269165,
      "learning_rate": 4.7351796773142115e-05,
      "loss": 0.5336,
      "step": 26000
    },
    {
      "epoch": 0.5398468057366362,
      "grad_norm": 6.954571723937988,
      "learning_rate": 4.7300867829204695e-05,
      "loss": 0.5216,
      "step": 26500
    },
    {
      "epoch": 0.5500325945241199,
      "grad_norm": 12.289632797241211,
      "learning_rate": 4.7249938885267276e-05,
      "loss": 0.508,
      "step": 27000
    },
    {
      "epoch": 0.5602183833116037,
      "grad_norm": 14.775038719177246,
      "learning_rate": 4.719900994132986e-05,
      "loss": 0.5287,
      "step": 27500
    },
    {
      "epoch": 0.5704041720990873,
      "grad_norm": 3.0326788425445557,
      "learning_rate": 4.714808099739244e-05,
      "loss": 0.5283,
      "step": 28000
    },
    {
      "epoch": 0.5805899608865711,
      "grad_norm": 5.9695892333984375,
      "learning_rate": 4.7097152053455025e-05,
      "loss": 0.512,
      "step": 28500
    },
    {
      "epoch": 0.5907757496740548,
      "grad_norm": 9.069087028503418,
      "learning_rate": 4.70462231095176e-05,
      "loss": 0.5086,
      "step": 29000
    },
    {
      "epoch": 0.6009615384615384,
      "grad_norm": 9.365386009216309,
      "learning_rate": 4.699529416558019e-05,
      "loss": 0.5368,
      "step": 29500
    },
    {
      "epoch": 0.6111473272490222,
      "grad_norm": 3.8707451820373535,
      "learning_rate": 4.694436522164277e-05,
      "loss": 0.5324,
      "step": 30000
    },
    {
      "epoch": 0.6213331160365059,
      "grad_norm": 3.0102360248565674,
      "learning_rate": 4.689343627770535e-05,
      "loss": 0.5205,
      "step": 30500
    },
    {
      "epoch": 0.6315189048239895,
      "grad_norm": 4.323484897613525,
      "learning_rate": 4.684250733376793e-05,
      "loss": 0.5313,
      "step": 31000
    },
    {
      "epoch": 0.6417046936114733,
      "grad_norm": 6.461000919342041,
      "learning_rate": 4.679157838983051e-05,
      "loss": 0.5299,
      "step": 31500
    },
    {
      "epoch": 0.651890482398957,
      "grad_norm": 9.54321575164795,
      "learning_rate": 4.674064944589309e-05,
      "loss": 0.5302,
      "step": 32000
    },
    {
      "epoch": 0.6620762711864406,
      "grad_norm": 8.140965461730957,
      "learning_rate": 4.668972050195568e-05,
      "loss": 0.5186,
      "step": 32500
    },
    {
      "epoch": 0.6722620599739244,
      "grad_norm": 9.38175106048584,
      "learning_rate": 4.663879155801826e-05,
      "loss": 0.537,
      "step": 33000
    },
    {
      "epoch": 0.6824478487614081,
      "grad_norm": 10.874849319458008,
      "learning_rate": 4.658786261408083e-05,
      "loss": 0.5131,
      "step": 33500
    },
    {
      "epoch": 0.6926336375488917,
      "grad_norm": 9.041991233825684,
      "learning_rate": 4.653693367014342e-05,
      "loss": 0.5096,
      "step": 34000
    },
    {
      "epoch": 0.7028194263363755,
      "grad_norm": 4.047967433929443,
      "learning_rate": 4.6486004726206e-05,
      "loss": 0.5166,
      "step": 34500
    },
    {
      "epoch": 0.7130052151238592,
      "grad_norm": 6.721231937408447,
      "learning_rate": 4.643507578226858e-05,
      "loss": 0.5326,
      "step": 35000
    },
    {
      "epoch": 0.7231910039113429,
      "grad_norm": 7.435022354125977,
      "learning_rate": 4.638414683833116e-05,
      "loss": 0.5264,
      "step": 35500
    },
    {
      "epoch": 0.7333767926988266,
      "grad_norm": 4.792122840881348,
      "learning_rate": 4.633321789439374e-05,
      "loss": 0.5338,
      "step": 36000
    },
    {
      "epoch": 0.7435625814863103,
      "grad_norm": 9.611244201660156,
      "learning_rate": 4.6282288950456324e-05,
      "loss": 0.5099,
      "step": 36500
    },
    {
      "epoch": 0.753748370273794,
      "grad_norm": 11.994964599609375,
      "learning_rate": 4.623136000651891e-05,
      "loss": 0.5215,
      "step": 37000
    },
    {
      "epoch": 0.7639341590612777,
      "grad_norm": 6.910139560699463,
      "learning_rate": 4.6180431062581485e-05,
      "loss": 0.5061,
      "step": 37500
    },
    {
      "epoch": 0.7741199478487614,
      "grad_norm": 12.286547660827637,
      "learning_rate": 4.6129502118644066e-05,
      "loss": 0.5021,
      "step": 38000
    },
    {
      "epoch": 0.7843057366362451,
      "grad_norm": 2.7257561683654785,
      "learning_rate": 4.607857317470665e-05,
      "loss": 0.5003,
      "step": 38500
    },
    {
      "epoch": 0.7944915254237288,
      "grad_norm": 12.209939002990723,
      "learning_rate": 4.6027644230769234e-05,
      "loss": 0.5235,
      "step": 39000
    },
    {
      "epoch": 0.8046773142112125,
      "grad_norm": 6.817516803741455,
      "learning_rate": 4.5976715286831815e-05,
      "loss": 0.5005,
      "step": 39500
    },
    {
      "epoch": 0.8148631029986962,
      "grad_norm": 12.793383598327637,
      "learning_rate": 4.5925786342894395e-05,
      "loss": 0.4938,
      "step": 40000
    },
    {
      "epoch": 0.8250488917861799,
      "grad_norm": 10.715030670166016,
      "learning_rate": 4.5874857398956976e-05,
      "loss": 0.5123,
      "step": 40500
    },
    {
      "epoch": 0.8352346805736637,
      "grad_norm": 7.083134651184082,
      "learning_rate": 4.582392845501956e-05,
      "loss": 0.5279,
      "step": 41000
    },
    {
      "epoch": 0.8454204693611473,
      "grad_norm": 10.86488151550293,
      "learning_rate": 4.5772999511082144e-05,
      "loss": 0.4995,
      "step": 41500
    },
    {
      "epoch": 0.855606258148631,
      "grad_norm": 9.857651710510254,
      "learning_rate": 4.572207056714472e-05,
      "loss": 0.4987,
      "step": 42000
    },
    {
      "epoch": 0.8657920469361148,
      "grad_norm": 8.225199699401855,
      "learning_rate": 4.5671141623207306e-05,
      "loss": 0.5178,
      "step": 42500
    },
    {
      "epoch": 0.8759778357235984,
      "grad_norm": 11.63556957244873,
      "learning_rate": 4.5620212679269887e-05,
      "loss": 0.5232,
      "step": 43000
    },
    {
      "epoch": 0.8861636245110821,
      "grad_norm": 7.048758506774902,
      "learning_rate": 4.556928373533247e-05,
      "loss": 0.5103,
      "step": 43500
    },
    {
      "epoch": 0.8963494132985659,
      "grad_norm": 2.1924798488616943,
      "learning_rate": 4.551835479139505e-05,
      "loss": 0.5024,
      "step": 44000
    },
    {
      "epoch": 0.9065352020860495,
      "grad_norm": 6.942378997802734,
      "learning_rate": 4.546742584745763e-05,
      "loss": 0.5322,
      "step": 44500
    },
    {
      "epoch": 0.9167209908735332,
      "grad_norm": 8.318870544433594,
      "learning_rate": 4.541649690352021e-05,
      "loss": 0.4962,
      "step": 45000
    },
    {
      "epoch": 0.926906779661017,
      "grad_norm": 4.541521072387695,
      "learning_rate": 4.53655679595828e-05,
      "loss": 0.5078,
      "step": 45500
    },
    {
      "epoch": 0.9370925684485006,
      "grad_norm": 11.43796443939209,
      "learning_rate": 4.531463901564537e-05,
      "loss": 0.4915,
      "step": 46000
    },
    {
      "epoch": 0.9472783572359843,
      "grad_norm": 8.344085693359375,
      "learning_rate": 4.526371007170795e-05,
      "loss": 0.5147,
      "step": 46500
    },
    {
      "epoch": 0.9574641460234681,
      "grad_norm": 3.4163131713867188,
      "learning_rate": 4.521278112777054e-05,
      "loss": 0.4922,
      "step": 47000
    },
    {
      "epoch": 0.9676499348109517,
      "grad_norm": 8.685168266296387,
      "learning_rate": 4.516185218383312e-05,
      "loss": 0.5006,
      "step": 47500
    },
    {
      "epoch": 0.9778357235984355,
      "grad_norm": 10.837403297424316,
      "learning_rate": 4.5110923239895694e-05,
      "loss": 0.4801,
      "step": 48000
    },
    {
      "epoch": 0.9880215123859192,
      "grad_norm": 8.325851440429688,
      "learning_rate": 4.505999429595828e-05,
      "loss": 0.4981,
      "step": 48500
    },
    {
      "epoch": 0.9982073011734028,
      "grad_norm": 12.670217514038086,
      "learning_rate": 4.500906535202086e-05,
      "loss": 0.5101,
      "step": 49000
    },
    {
      "epoch": 1.0083930899608866,
      "grad_norm": 13.781088829040527,
      "learning_rate": 4.495813640808344e-05,
      "loss": 0.5018,
      "step": 49500
    },
    {
      "epoch": 1.0185788787483703,
      "grad_norm": 7.5899553298950195,
      "learning_rate": 4.490720746414603e-05,
      "loss": 0.4747,
      "step": 50000
    },
    {
      "epoch": 1.028764667535854,
      "grad_norm": 6.419172286987305,
      "learning_rate": 4.4856278520208604e-05,
      "loss": 0.477,
      "step": 50500
    },
    {
      "epoch": 1.0389504563233376,
      "grad_norm": 6.4826836585998535,
      "learning_rate": 4.480534957627119e-05,
      "loss": 0.4894,
      "step": 51000
    },
    {
      "epoch": 1.0491362451108215,
      "grad_norm": 5.5415940284729,
      "learning_rate": 4.475442063233377e-05,
      "loss": 0.4774,
      "step": 51500
    },
    {
      "epoch": 1.0593220338983051,
      "grad_norm": 6.915538311004639,
      "learning_rate": 4.470349168839635e-05,
      "loss": 0.475,
      "step": 52000
    },
    {
      "epoch": 1.0695078226857888,
      "grad_norm": 9.105144500732422,
      "learning_rate": 4.4652562744458934e-05,
      "loss": 0.4905,
      "step": 52500
    },
    {
      "epoch": 1.0796936114732725,
      "grad_norm": 5.6747822761535645,
      "learning_rate": 4.4601633800521515e-05,
      "loss": 0.4886,
      "step": 53000
    },
    {
      "epoch": 1.0898794002607561,
      "grad_norm": 3.4728853702545166,
      "learning_rate": 4.4550704856584095e-05,
      "loss": 0.4811,
      "step": 53500
    },
    {
      "epoch": 1.1000651890482398,
      "grad_norm": 11.605137825012207,
      "learning_rate": 4.449977591264668e-05,
      "loss": 0.4958,
      "step": 54000
    },
    {
      "epoch": 1.1102509778357237,
      "grad_norm": 10.487143516540527,
      "learning_rate": 4.444884696870926e-05,
      "loss": 0.4773,
      "step": 54500
    },
    {
      "epoch": 1.1204367666232073,
      "grad_norm": 3.1569766998291016,
      "learning_rate": 4.439791802477184e-05,
      "loss": 0.4878,
      "step": 55000
    },
    {
      "epoch": 1.130622555410691,
      "grad_norm": 13.370224952697754,
      "learning_rate": 4.4346989080834425e-05,
      "loss": 0.4796,
      "step": 55500
    },
    {
      "epoch": 1.1408083441981747,
      "grad_norm": 7.069026947021484,
      "learning_rate": 4.4296060136897006e-05,
      "loss": 0.5019,
      "step": 56000
    },
    {
      "epoch": 1.1509941329856583,
      "grad_norm": 14.963642120361328,
      "learning_rate": 4.424513119295958e-05,
      "loss": 0.4696,
      "step": 56500
    },
    {
      "epoch": 1.161179921773142,
      "grad_norm": 11.484002113342285,
      "learning_rate": 4.419420224902217e-05,
      "loss": 0.5025,
      "step": 57000
    },
    {
      "epoch": 1.1713657105606259,
      "grad_norm": 5.625272274017334,
      "learning_rate": 4.414327330508475e-05,
      "loss": 0.4962,
      "step": 57500
    },
    {
      "epoch": 1.1815514993481095,
      "grad_norm": 10.035353660583496,
      "learning_rate": 4.409234436114733e-05,
      "loss": 0.4899,
      "step": 58000
    },
    {
      "epoch": 1.1917372881355932,
      "grad_norm": 6.944583892822266,
      "learning_rate": 4.4041415417209916e-05,
      "loss": 0.4691,
      "step": 58500
    },
    {
      "epoch": 1.2019230769230769,
      "grad_norm": 4.2878923416137695,
      "learning_rate": 4.399048647327249e-05,
      "loss": 0.5004,
      "step": 59000
    },
    {
      "epoch": 1.2121088657105605,
      "grad_norm": 11.808344841003418,
      "learning_rate": 4.393955752933507e-05,
      "loss": 0.4716,
      "step": 59500
    },
    {
      "epoch": 1.2222946544980444,
      "grad_norm": 9.198448181152344,
      "learning_rate": 4.388862858539766e-05,
      "loss": 0.4698,
      "step": 60000
    },
    {
      "epoch": 1.232480443285528,
      "grad_norm": 8.751873970031738,
      "learning_rate": 4.383769964146024e-05,
      "loss": 0.492,
      "step": 60500
    },
    {
      "epoch": 1.2426662320730117,
      "grad_norm": 7.313394069671631,
      "learning_rate": 4.378677069752282e-05,
      "loss": 0.4973,
      "step": 61000
    },
    {
      "epoch": 1.2528520208604954,
      "grad_norm": 6.551813125610352,
      "learning_rate": 4.37358417535854e-05,
      "loss": 0.498,
      "step": 61500
    },
    {
      "epoch": 1.263037809647979,
      "grad_norm": 18.395362854003906,
      "learning_rate": 4.368491280964798e-05,
      "loss": 0.4839,
      "step": 62000
    },
    {
      "epoch": 1.2732235984354627,
      "grad_norm": 4.862782001495361,
      "learning_rate": 4.363398386571056e-05,
      "loss": 0.4969,
      "step": 62500
    },
    {
      "epoch": 1.2834093872229466,
      "grad_norm": 5.902966022491455,
      "learning_rate": 4.358305492177314e-05,
      "loss": 0.4709,
      "step": 63000
    },
    {
      "epoch": 1.2935951760104303,
      "grad_norm": 7.644299030303955,
      "learning_rate": 4.353212597783572e-05,
      "loss": 0.4738,
      "step": 63500
    },
    {
      "epoch": 1.303780964797914,
      "grad_norm": 12.062520027160645,
      "learning_rate": 4.348119703389831e-05,
      "loss": 0.4832,
      "step": 64000
    },
    {
      "epoch": 1.3139667535853976,
      "grad_norm": 9.379124641418457,
      "learning_rate": 4.343026808996089e-05,
      "loss": 0.4739,
      "step": 64500
    },
    {
      "epoch": 1.3241525423728815,
      "grad_norm": 9.702716827392578,
      "learning_rate": 4.3379339146023466e-05,
      "loss": 0.4709,
      "step": 65000
    },
    {
      "epoch": 1.3343383311603652,
      "grad_norm": 5.447597980499268,
      "learning_rate": 4.332841020208605e-05,
      "loss": 0.4775,
      "step": 65500
    },
    {
      "epoch": 1.3445241199478488,
      "grad_norm": 8.008336067199707,
      "learning_rate": 4.3277481258148634e-05,
      "loss": 0.4566,
      "step": 66000
    },
    {
      "epoch": 1.3547099087353325,
      "grad_norm": 10.37303638458252,
      "learning_rate": 4.3226552314211215e-05,
      "loss": 0.4918,
      "step": 66500
    },
    {
      "epoch": 1.3648956975228161,
      "grad_norm": 7.317714691162109,
      "learning_rate": 4.3175623370273795e-05,
      "loss": 0.4871,
      "step": 67000
    },
    {
      "epoch": 1.3750814863102998,
      "grad_norm": 8.46983814239502,
      "learning_rate": 4.3124694426336376e-05,
      "loss": 0.4888,
      "step": 67500
    },
    {
      "epoch": 1.3852672750977835,
      "grad_norm": 7.305872917175293,
      "learning_rate": 4.307376548239896e-05,
      "loss": 0.4785,
      "step": 68000
    },
    {
      "epoch": 1.3954530638852674,
      "grad_norm": 3.974360704421997,
      "learning_rate": 4.3022836538461544e-05,
      "loss": 0.4923,
      "step": 68500
    },
    {
      "epoch": 1.405638852672751,
      "grad_norm": 7.031236171722412,
      "learning_rate": 4.2971907594524125e-05,
      "loss": 0.4935,
      "step": 69000
    },
    {
      "epoch": 1.4158246414602347,
      "grad_norm": 4.333521366119385,
      "learning_rate": 4.29209786505867e-05,
      "loss": 0.4956,
      "step": 69500
    },
    {
      "epoch": 1.4260104302477183,
      "grad_norm": 8.787294387817383,
      "learning_rate": 4.2870049706649286e-05,
      "loss": 0.4654,
      "step": 70000
    },
    {
      "epoch": 1.436196219035202,
      "grad_norm": 10.297895431518555,
      "learning_rate": 4.281912076271187e-05,
      "loss": 0.4844,
      "step": 70500
    },
    {
      "epoch": 1.4463820078226859,
      "grad_norm": 10.258440017700195,
      "learning_rate": 4.276819181877445e-05,
      "loss": 0.4599,
      "step": 71000
    },
    {
      "epoch": 1.4565677966101696,
      "grad_norm": 10.338373184204102,
      "learning_rate": 4.271726287483703e-05,
      "loss": 0.4725,
      "step": 71500
    },
    {
      "epoch": 1.4667535853976532,
      "grad_norm": 6.631515979766846,
      "learning_rate": 4.266633393089961e-05,
      "loss": 0.4567,
      "step": 72000
    },
    {
      "epoch": 1.4769393741851369,
      "grad_norm": 8.059633255004883,
      "learning_rate": 4.261540498696219e-05,
      "loss": 0.4898,
      "step": 72500
    },
    {
      "epoch": 1.4871251629726205,
      "grad_norm": 1.8305431604385376,
      "learning_rate": 4.256447604302478e-05,
      "loss": 0.4696,
      "step": 73000
    },
    {
      "epoch": 1.4973109517601042,
      "grad_norm": 8.375441551208496,
      "learning_rate": 4.251354709908735e-05,
      "loss": 0.4668,
      "step": 73500
    },
    {
      "epoch": 1.5074967405475879,
      "grad_norm": 4.69639253616333,
      "learning_rate": 4.246261815514994e-05,
      "loss": 0.4636,
      "step": 74000
    },
    {
      "epoch": 1.5176825293350718,
      "grad_norm": 9.221155166625977,
      "learning_rate": 4.241168921121252e-05,
      "loss": 0.4676,
      "step": 74500
    },
    {
      "epoch": 1.5278683181225554,
      "grad_norm": 17.040802001953125,
      "learning_rate": 4.23607602672751e-05,
      "loss": 0.4662,
      "step": 75000
    },
    {
      "epoch": 1.538054106910039,
      "grad_norm": 5.459781169891357,
      "learning_rate": 4.230983132333768e-05,
      "loss": 0.464,
      "step": 75500
    },
    {
      "epoch": 1.548239895697523,
      "grad_norm": 5.433460712432861,
      "learning_rate": 4.225890237940026e-05,
      "loss": 0.4828,
      "step": 76000
    },
    {
      "epoch": 1.5584256844850066,
      "grad_norm": 9.149493217468262,
      "learning_rate": 4.220797343546284e-05,
      "loss": 0.4697,
      "step": 76500
    },
    {
      "epoch": 1.5686114732724903,
      "grad_norm": 7.0799479484558105,
      "learning_rate": 4.215704449152543e-05,
      "loss": 0.4641,
      "step": 77000
    },
    {
      "epoch": 1.578797262059974,
      "grad_norm": 5.539448261260986,
      "learning_rate": 4.210611554758801e-05,
      "loss": 0.4775,
      "step": 77500
    },
    {
      "epoch": 1.5889830508474576,
      "grad_norm": 11.451518058776855,
      "learning_rate": 4.2055186603650585e-05,
      "loss": 0.4838,
      "step": 78000
    },
    {
      "epoch": 1.5991688396349413,
      "grad_norm": 9.999706268310547,
      "learning_rate": 4.200425765971317e-05,
      "loss": 0.4669,
      "step": 78500
    },
    {
      "epoch": 1.609354628422425,
      "grad_norm": 6.10065221786499,
      "learning_rate": 4.195332871577575e-05,
      "loss": 0.4902,
      "step": 79000
    },
    {
      "epoch": 1.6195404172099086,
      "grad_norm": 10.325986862182617,
      "learning_rate": 4.1902399771838334e-05,
      "loss": 0.4583,
      "step": 79500
    },
    {
      "epoch": 1.6297262059973925,
      "grad_norm": 7.319058895111084,
      "learning_rate": 4.1851470827900914e-05,
      "loss": 0.4667,
      "step": 80000
    },
    {
      "epoch": 1.6399119947848761,
      "grad_norm": 7.791489601135254,
      "learning_rate": 4.1800541883963495e-05,
      "loss": 0.4742,
      "step": 80500
    },
    {
      "epoch": 1.6500977835723598,
      "grad_norm": 4.461919784545898,
      "learning_rate": 4.1749612940026076e-05,
      "loss": 0.4574,
      "step": 81000
    },
    {
      "epoch": 1.6602835723598437,
      "grad_norm": 8.902009010314941,
      "learning_rate": 4.169868399608866e-05,
      "loss": 0.4558,
      "step": 81500
    },
    {
      "epoch": 1.6704693611473274,
      "grad_norm": 9.97192668914795,
      "learning_rate": 4.164775505215124e-05,
      "loss": 0.4759,
      "step": 82000
    },
    {
      "epoch": 1.680655149934811,
      "grad_norm": 13.34089183807373,
      "learning_rate": 4.159682610821382e-05,
      "loss": 0.4667,
      "step": 82500
    },
    {
      "epoch": 1.6908409387222947,
      "grad_norm": 8.145942687988281,
      "learning_rate": 4.1545897164276406e-05,
      "loss": 0.4659,
      "step": 83000
    },
    {
      "epoch": 1.7010267275097783,
      "grad_norm": 19.160747528076172,
      "learning_rate": 4.1494968220338986e-05,
      "loss": 0.4695,
      "step": 83500
    },
    {
      "epoch": 1.711212516297262,
      "grad_norm": 12.305388450622559,
      "learning_rate": 4.144403927640157e-05,
      "loss": 0.4618,
      "step": 84000
    },
    {
      "epoch": 1.7213983050847457,
      "grad_norm": 4.789829730987549,
      "learning_rate": 4.139311033246415e-05,
      "loss": 0.4815,
      "step": 84500
    },
    {
      "epoch": 1.7315840938722293,
      "grad_norm": 2.8541316986083984,
      "learning_rate": 4.134218138852673e-05,
      "loss": 0.4655,
      "step": 85000
    },
    {
      "epoch": 1.7417698826597132,
      "grad_norm": 3.4919116497039795,
      "learning_rate": 4.129125244458931e-05,
      "loss": 0.472,
      "step": 85500
    },
    {
      "epoch": 1.7519556714471969,
      "grad_norm": 16.304248809814453,
      "learning_rate": 4.12403235006519e-05,
      "loss": 0.4403,
      "step": 86000
    },
    {
      "epoch": 1.7621414602346805,
      "grad_norm": 5.850127220153809,
      "learning_rate": 4.118939455671447e-05,
      "loss": 0.4798,
      "step": 86500
    },
    {
      "epoch": 1.7723272490221644,
      "grad_norm": 10.399096488952637,
      "learning_rate": 4.113846561277706e-05,
      "loss": 0.4744,
      "step": 87000
    },
    {
      "epoch": 1.782513037809648,
      "grad_norm": 8.72945785522461,
      "learning_rate": 4.108753666883964e-05,
      "loss": 0.4598,
      "step": 87500
    },
    {
      "epoch": 1.7926988265971318,
      "grad_norm": 8.09701919555664,
      "learning_rate": 4.103660772490222e-05,
      "loss": 0.449,
      "step": 88000
    },
    {
      "epoch": 1.8028846153846154,
      "grad_norm": 4.31442928314209,
      "learning_rate": 4.09856787809648e-05,
      "loss": 0.4822,
      "step": 88500
    },
    {
      "epoch": 1.813070404172099,
      "grad_norm": 5.672591686248779,
      "learning_rate": 4.093474983702738e-05,
      "loss": 0.4733,
      "step": 89000
    },
    {
      "epoch": 1.8232561929595827,
      "grad_norm": 2.427098035812378,
      "learning_rate": 4.088382089308996e-05,
      "loss": 0.4397,
      "step": 89500
    },
    {
      "epoch": 1.8334419817470664,
      "grad_norm": 10.87921142578125,
      "learning_rate": 4.083289194915255e-05,
      "loss": 0.4874,
      "step": 90000
    },
    {
      "epoch": 1.84362777053455,
      "grad_norm": 10.75042724609375,
      "learning_rate": 4.078196300521512e-05,
      "loss": 0.4518,
      "step": 90500
    },
    {
      "epoch": 1.8538135593220337,
      "grad_norm": 3.8364131450653076,
      "learning_rate": 4.0731034061277704e-05,
      "loss": 0.4675,
      "step": 91000
    },
    {
      "epoch": 1.8639993481095176,
      "grad_norm": 8.147485733032227,
      "learning_rate": 4.068010511734029e-05,
      "loss": 0.446,
      "step": 91500
    },
    {
      "epoch": 1.8741851368970013,
      "grad_norm": 10.477482795715332,
      "learning_rate": 4.062917617340287e-05,
      "loss": 0.4728,
      "step": 92000
    },
    {
      "epoch": 1.8843709256844852,
      "grad_norm": 10.703990936279297,
      "learning_rate": 4.0578247229465446e-05,
      "loss": 0.4901,
      "step": 92500
    },
    {
      "epoch": 1.8945567144719688,
      "grad_norm": 11.731633186340332,
      "learning_rate": 4.0527318285528034e-05,
      "loss": 0.456,
      "step": 93000
    },
    {
      "epoch": 1.9047425032594525,
      "grad_norm": 6.677139759063721,
      "learning_rate": 4.0476389341590614e-05,
      "loss": 0.4609,
      "step": 93500
    },
    {
      "epoch": 1.9149282920469362,
      "grad_norm": 1.0644692182540894,
      "learning_rate": 4.0425460397653195e-05,
      "loss": 0.4773,
      "step": 94000
    },
    {
      "epoch": 1.9251140808344198,
      "grad_norm": 2.2629666328430176,
      "learning_rate": 4.037453145371578e-05,
      "loss": 0.4524,
      "step": 94500
    },
    {
      "epoch": 1.9352998696219035,
      "grad_norm": 9.630969047546387,
      "learning_rate": 4.0323602509778357e-05,
      "loss": 0.482,
      "step": 95000
    },
    {
      "epoch": 1.9454856584093871,
      "grad_norm": 5.622994422912598,
      "learning_rate": 4.027267356584094e-05,
      "loss": 0.4583,
      "step": 95500
    },
    {
      "epoch": 1.9556714471968708,
      "grad_norm": 6.82352876663208,
      "learning_rate": 4.0221744621903525e-05,
      "loss": 0.4778,
      "step": 96000
    },
    {
      "epoch": 1.9658572359843545,
      "grad_norm": 12.102866172790527,
      "learning_rate": 4.0170815677966105e-05,
      "loss": 0.4431,
      "step": 96500
    },
    {
      "epoch": 1.9760430247718384,
      "grad_norm": 13.911796569824219,
      "learning_rate": 4.0119886734028686e-05,
      "loss": 0.4593,
      "step": 97000
    },
    {
      "epoch": 1.986228813559322,
      "grad_norm": 10.538349151611328,
      "learning_rate": 4.006895779009127e-05,
      "loss": 0.4651,
      "step": 97500
    },
    {
      "epoch": 1.996414602346806,
      "grad_norm": 1.2614984512329102,
      "learning_rate": 4.001802884615385e-05,
      "loss": 0.458,
      "step": 98000
    },
    {
      "epoch": 2.0066003911342896,
      "grad_norm": 3.5551512241363525,
      "learning_rate": 3.996709990221643e-05,
      "loss": 0.4392,
      "step": 98500
    },
    {
      "epoch": 2.0167861799217732,
      "grad_norm": 12.976408004760742,
      "learning_rate": 3.991617095827901e-05,
      "loss": 0.4379,
      "step": 99000
    },
    {
      "epoch": 2.026971968709257,
      "grad_norm": 13.771697998046875,
      "learning_rate": 3.986524201434159e-05,
      "loss": 0.4603,
      "step": 99500
    },
    {
      "epoch": 2.0371577574967406,
      "grad_norm": 10.135859489440918,
      "learning_rate": 3.981431307040418e-05,
      "loss": 0.4654,
      "step": 100000
    },
    {
      "epoch": 2.047343546284224,
      "grad_norm": 12.743950843811035,
      "learning_rate": 3.976338412646676e-05,
      "loss": 0.44,
      "step": 100500
    },
    {
      "epoch": 2.057529335071708,
      "grad_norm": 9.762621879577637,
      "learning_rate": 3.971245518252933e-05,
      "loss": 0.42,
      "step": 101000
    },
    {
      "epoch": 2.0677151238591915,
      "grad_norm": 1.922711968421936,
      "learning_rate": 3.966152623859192e-05,
      "loss": 0.4503,
      "step": 101500
    },
    {
      "epoch": 2.077900912646675,
      "grad_norm": 6.104671955108643,
      "learning_rate": 3.96105972946545e-05,
      "loss": 0.4556,
      "step": 102000
    },
    {
      "epoch": 2.088086701434159,
      "grad_norm": 5.507967472076416,
      "learning_rate": 3.955966835071708e-05,
      "loss": 0.4493,
      "step": 102500
    },
    {
      "epoch": 2.098272490221643,
      "grad_norm": 4.27385950088501,
      "learning_rate": 3.950873940677967e-05,
      "loss": 0.4639,
      "step": 103000
    },
    {
      "epoch": 2.1084582790091266,
      "grad_norm": 6.932498931884766,
      "learning_rate": 3.945781046284224e-05,
      "loss": 0.4512,
      "step": 103500
    },
    {
      "epoch": 2.1186440677966103,
      "grad_norm": 7.8755412101745605,
      "learning_rate": 3.940688151890482e-05,
      "loss": 0.454,
      "step": 104000
    },
    {
      "epoch": 2.128829856584094,
      "grad_norm": 8.759586334228516,
      "learning_rate": 3.935595257496741e-05,
      "loss": 0.4555,
      "step": 104500
    },
    {
      "epoch": 2.1390156453715776,
      "grad_norm": 4.931230068206787,
      "learning_rate": 3.930502363102999e-05,
      "loss": 0.4411,
      "step": 105000
    },
    {
      "epoch": 2.1492014341590613,
      "grad_norm": 7.033292293548584,
      "learning_rate": 3.9254094687092565e-05,
      "loss": 0.4585,
      "step": 105500
    },
    {
      "epoch": 2.159387222946545,
      "grad_norm": 7.179849147796631,
      "learning_rate": 3.920316574315515e-05,
      "loss": 0.4648,
      "step": 106000
    },
    {
      "epoch": 2.1695730117340286,
      "grad_norm": 9.124664306640625,
      "learning_rate": 3.9152236799217734e-05,
      "loss": 0.4542,
      "step": 106500
    },
    {
      "epoch": 2.1797588005215123,
      "grad_norm": 5.491971015930176,
      "learning_rate": 3.9101307855280314e-05,
      "loss": 0.4348,
      "step": 107000
    },
    {
      "epoch": 2.189944589308996,
      "grad_norm": 13.063499450683594,
      "learning_rate": 3.9050378911342895e-05,
      "loss": 0.4466,
      "step": 107500
    },
    {
      "epoch": 2.2001303780964796,
      "grad_norm": 4.230403423309326,
      "learning_rate": 3.8999449967405476e-05,
      "loss": 0.4483,
      "step": 108000
    },
    {
      "epoch": 2.2103161668839633,
      "grad_norm": 6.997844219207764,
      "learning_rate": 3.8948521023468056e-05,
      "loss": 0.4529,
      "step": 108500
    },
    {
      "epoch": 2.2205019556714474,
      "grad_norm": 7.762799263000488,
      "learning_rate": 3.8897592079530644e-05,
      "loss": 0.4429,
      "step": 109000
    },
    {
      "epoch": 2.230687744458931,
      "grad_norm": 5.763533115386963,
      "learning_rate": 3.884666313559322e-05,
      "loss": 0.4337,
      "step": 109500
    },
    {
      "epoch": 2.2408735332464147,
      "grad_norm": 11.916216850280762,
      "learning_rate": 3.8795734191655805e-05,
      "loss": 0.4537,
      "step": 110000
    },
    {
      "epoch": 2.2510593220338984,
      "grad_norm": 12.870357513427734,
      "learning_rate": 3.8744805247718386e-05,
      "loss": 0.4485,
      "step": 110500
    },
    {
      "epoch": 2.261245110821382,
      "grad_norm": 6.651268005371094,
      "learning_rate": 3.869387630378097e-05,
      "loss": 0.4523,
      "step": 111000
    },
    {
      "epoch": 2.2714308996088657,
      "grad_norm": 8.87380599975586,
      "learning_rate": 3.864294735984355e-05,
      "loss": 0.4501,
      "step": 111500
    },
    {
      "epoch": 2.2816166883963493,
      "grad_norm": 5.309725284576416,
      "learning_rate": 3.859201841590613e-05,
      "loss": 0.438,
      "step": 112000
    },
    {
      "epoch": 2.291802477183833,
      "grad_norm": 7.761465549468994,
      "learning_rate": 3.854108947196871e-05,
      "loss": 0.4285,
      "step": 112500
    },
    {
      "epoch": 2.3019882659713167,
      "grad_norm": 4.997422695159912,
      "learning_rate": 3.8490160528031297e-05,
      "loss": 0.4387,
      "step": 113000
    },
    {
      "epoch": 2.3121740547588003,
      "grad_norm": 15.70908260345459,
      "learning_rate": 3.843923158409388e-05,
      "loss": 0.4303,
      "step": 113500
    },
    {
      "epoch": 2.322359843546284,
      "grad_norm": 3.7189106941223145,
      "learning_rate": 3.838830264015645e-05,
      "loss": 0.4212,
      "step": 114000
    },
    {
      "epoch": 2.332545632333768,
      "grad_norm": 6.893543720245361,
      "learning_rate": 3.833737369621904e-05,
      "loss": 0.4457,
      "step": 114500
    },
    {
      "epoch": 2.3427314211212518,
      "grad_norm": 2.407423973083496,
      "learning_rate": 3.828644475228162e-05,
      "loss": 0.4358,
      "step": 115000
    },
    {
      "epoch": 2.3529172099087354,
      "grad_norm": 13.13415241241455,
      "learning_rate": 3.82355158083442e-05,
      "loss": 0.4523,
      "step": 115500
    },
    {
      "epoch": 2.363102998696219,
      "grad_norm": 8.366569519042969,
      "learning_rate": 3.818458686440678e-05,
      "loss": 0.4508,
      "step": 116000
    },
    {
      "epoch": 2.3732887874837028,
      "grad_norm": 3.713616132736206,
      "learning_rate": 3.813365792046936e-05,
      "loss": 0.4399,
      "step": 116500
    },
    {
      "epoch": 2.3834745762711864,
      "grad_norm": 3.6448826789855957,
      "learning_rate": 3.808272897653194e-05,
      "loss": 0.4663,
      "step": 117000
    },
    {
      "epoch": 2.39366036505867,
      "grad_norm": 6.243638038635254,
      "learning_rate": 3.803180003259453e-05,
      "loss": 0.4357,
      "step": 117500
    },
    {
      "epoch": 2.4038461538461537,
      "grad_norm": 6.854045867919922,
      "learning_rate": 3.7980871088657104e-05,
      "loss": 0.4469,
      "step": 118000
    },
    {
      "epoch": 2.4140319426336374,
      "grad_norm": 12.145404815673828,
      "learning_rate": 3.7929942144719684e-05,
      "loss": 0.4414,
      "step": 118500
    },
    {
      "epoch": 2.424217731421121,
      "grad_norm": 1.77471923828125,
      "learning_rate": 3.787901320078227e-05,
      "loss": 0.4433,
      "step": 119000
    },
    {
      "epoch": 2.4344035202086047,
      "grad_norm": 5.9456634521484375,
      "learning_rate": 3.782808425684485e-05,
      "loss": 0.4659,
      "step": 119500
    },
    {
      "epoch": 2.444589308996089,
      "grad_norm": 10.131964683532715,
      "learning_rate": 3.7777155312907433e-05,
      "loss": 0.446,
      "step": 120000
    },
    {
      "epoch": 2.4547750977835725,
      "grad_norm": 9.077653884887695,
      "learning_rate": 3.7726226368970014e-05,
      "loss": 0.4267,
      "step": 120500
    },
    {
      "epoch": 2.464960886571056,
      "grad_norm": 2.600409507751465,
      "learning_rate": 3.7675297425032595e-05,
      "loss": 0.4692,
      "step": 121000
    },
    {
      "epoch": 2.47514667535854,
      "grad_norm": 10.27705192565918,
      "learning_rate": 3.762436848109518e-05,
      "loss": 0.4453,
      "step": 121500
    },
    {
      "epoch": 2.4853324641460235,
      "grad_norm": 2.7935948371887207,
      "learning_rate": 3.757343953715776e-05,
      "loss": 0.4327,
      "step": 122000
    },
    {
      "epoch": 2.495518252933507,
      "grad_norm": 16.455612182617188,
      "learning_rate": 3.752251059322034e-05,
      "loss": 0.4521,
      "step": 122500
    },
    {
      "epoch": 2.505704041720991,
      "grad_norm": 7.339682579040527,
      "learning_rate": 3.7471581649282925e-05,
      "loss": 0.4447,
      "step": 123000
    },
    {
      "epoch": 2.5158898305084745,
      "grad_norm": 9.395026206970215,
      "learning_rate": 3.7420652705345505e-05,
      "loss": 0.4356,
      "step": 123500
    },
    {
      "epoch": 2.526075619295958,
      "grad_norm": 10.36523151397705,
      "learning_rate": 3.7369723761408086e-05,
      "loss": 0.451,
      "step": 124000
    },
    {
      "epoch": 2.5362614080834422,
      "grad_norm": 7.437679290771484,
      "learning_rate": 3.731879481747067e-05,
      "loss": 0.4521,
      "step": 124500
    },
    {
      "epoch": 2.5464471968709255,
      "grad_norm": 7.325385093688965,
      "learning_rate": 3.726786587353325e-05,
      "loss": 0.4639,
      "step": 125000
    },
    {
      "epoch": 2.5566329856584096,
      "grad_norm": 1.3261373043060303,
      "learning_rate": 3.721693692959583e-05,
      "loss": 0.4333,
      "step": 125500
    },
    {
      "epoch": 2.5668187744458932,
      "grad_norm": 11.042800903320312,
      "learning_rate": 3.7166007985658416e-05,
      "loss": 0.4624,
      "step": 126000
    },
    {
      "epoch": 2.577004563233377,
      "grad_norm": 13.565596580505371,
      "learning_rate": 3.711507904172099e-05,
      "loss": 0.4459,
      "step": 126500
    },
    {
      "epoch": 2.5871903520208606,
      "grad_norm": 4.007774829864502,
      "learning_rate": 3.706415009778357e-05,
      "loss": 0.4372,
      "step": 127000
    },
    {
      "epoch": 2.5973761408083442,
      "grad_norm": 2.824951648712158,
      "learning_rate": 3.701322115384616e-05,
      "loss": 0.4381,
      "step": 127500
    },
    {
      "epoch": 2.607561929595828,
      "grad_norm": 8.064384460449219,
      "learning_rate": 3.696229220990874e-05,
      "loss": 0.444,
      "step": 128000
    },
    {
      "epoch": 2.6177477183833116,
      "grad_norm": 18.99576759338379,
      "learning_rate": 3.691136326597132e-05,
      "loss": 0.4328,
      "step": 128500
    },
    {
      "epoch": 2.627933507170795,
      "grad_norm": 6.447726249694824,
      "learning_rate": 3.68604343220339e-05,
      "loss": 0.4655,
      "step": 129000
    },
    {
      "epoch": 2.638119295958279,
      "grad_norm": 6.69405460357666,
      "learning_rate": 3.680950537809648e-05,
      "loss": 0.4439,
      "step": 129500
    },
    {
      "epoch": 2.648305084745763,
      "grad_norm": 10.37585163116455,
      "learning_rate": 3.675857643415906e-05,
      "loss": 0.4391,
      "step": 130000
    },
    {
      "epoch": 2.658490873533246,
      "grad_norm": 10.460826873779297,
      "learning_rate": 3.670764749022165e-05,
      "loss": 0.4473,
      "step": 130500
    },
    {
      "epoch": 2.6686766623207303,
      "grad_norm": 6.271072864532471,
      "learning_rate": 3.665671854628422e-05,
      "loss": 0.4386,
      "step": 131000
    },
    {
      "epoch": 2.678862451108214,
      "grad_norm": 2.212920904159546,
      "learning_rate": 3.660578960234681e-05,
      "loss": 0.4413,
      "step": 131500
    },
    {
      "epoch": 2.6890482398956976,
      "grad_norm": 11.861211776733398,
      "learning_rate": 3.655486065840939e-05,
      "loss": 0.4453,
      "step": 132000
    },
    {
      "epoch": 2.6992340286831813,
      "grad_norm": 11.21560287475586,
      "learning_rate": 3.650393171447197e-05,
      "loss": 0.4585,
      "step": 132500
    },
    {
      "epoch": 2.709419817470665,
      "grad_norm": 10.89380931854248,
      "learning_rate": 3.645300277053455e-05,
      "loss": 0.4325,
      "step": 133000
    },
    {
      "epoch": 2.7196056062581486,
      "grad_norm": 8.042730331420898,
      "learning_rate": 3.640207382659713e-05,
      "loss": 0.4365,
      "step": 133500
    },
    {
      "epoch": 2.7297913950456323,
      "grad_norm": 9.494121551513672,
      "learning_rate": 3.6351144882659714e-05,
      "loss": 0.4499,
      "step": 134000
    },
    {
      "epoch": 2.739977183833116,
      "grad_norm": 8.092448234558105,
      "learning_rate": 3.63002159387223e-05,
      "loss": 0.4346,
      "step": 134500
    },
    {
      "epoch": 2.7501629726205996,
      "grad_norm": 7.883386135101318,
      "learning_rate": 3.6249286994784876e-05,
      "loss": 0.4511,
      "step": 135000
    },
    {
      "epoch": 2.7603487614080837,
      "grad_norm": 6.029465675354004,
      "learning_rate": 3.6198358050847456e-05,
      "loss": 0.4385,
      "step": 135500
    },
    {
      "epoch": 2.770534550195567,
      "grad_norm": 10.163240432739258,
      "learning_rate": 3.6147429106910044e-05,
      "loss": 0.4581,
      "step": 136000
    },
    {
      "epoch": 2.780720338983051,
      "grad_norm": 9.969985961914062,
      "learning_rate": 3.6096500162972624e-05,
      "loss": 0.4469,
      "step": 136500
    },
    {
      "epoch": 2.7909061277705347,
      "grad_norm": 5.155818462371826,
      "learning_rate": 3.60455712190352e-05,
      "loss": 0.4541,
      "step": 137000
    },
    {
      "epoch": 2.8010919165580184,
      "grad_norm": 7.95702600479126,
      "learning_rate": 3.5994642275097786e-05,
      "loss": 0.4465,
      "step": 137500
    },
    {
      "epoch": 2.811277705345502,
      "grad_norm": 6.130152225494385,
      "learning_rate": 3.594371333116037e-05,
      "loss": 0.4751,
      "step": 138000
    },
    {
      "epoch": 2.8214634941329857,
      "grad_norm": 5.694178581237793,
      "learning_rate": 3.589278438722295e-05,
      "loss": 0.4267,
      "step": 138500
    },
    {
      "epoch": 2.8316492829204694,
      "grad_norm": 14.726696968078613,
      "learning_rate": 3.5841855443285535e-05,
      "loss": 0.4277,
      "step": 139000
    },
    {
      "epoch": 2.841835071707953,
      "grad_norm": 6.5778303146362305,
      "learning_rate": 3.579092649934811e-05,
      "loss": 0.434,
      "step": 139500
    },
    {
      "epoch": 2.8520208604954367,
      "grad_norm": 13.916502952575684,
      "learning_rate": 3.573999755541069e-05,
      "loss": 0.4279,
      "step": 140000
    },
    {
      "epoch": 2.8622066492829203,
      "grad_norm": 3.0371477603912354,
      "learning_rate": 3.568906861147328e-05,
      "loss": 0.4395,
      "step": 140500
    },
    {
      "epoch": 2.872392438070404,
      "grad_norm": 3.64876389503479,
      "learning_rate": 3.563813966753586e-05,
      "loss": 0.4259,
      "step": 141000
    },
    {
      "epoch": 2.8825782268578877,
      "grad_norm": 11.23768424987793,
      "learning_rate": 3.558721072359844e-05,
      "loss": 0.4369,
      "step": 141500
    },
    {
      "epoch": 2.8927640156453718,
      "grad_norm": 13.399386405944824,
      "learning_rate": 3.553628177966102e-05,
      "loss": 0.4412,
      "step": 142000
    },
    {
      "epoch": 2.9029498044328554,
      "grad_norm": 9.14038372039795,
      "learning_rate": 3.54853528357236e-05,
      "loss": 0.4609,
      "step": 142500
    },
    {
      "epoch": 2.913135593220339,
      "grad_norm": 8.734009742736816,
      "learning_rate": 3.543442389178618e-05,
      "loss": 0.4394,
      "step": 143000
    },
    {
      "epoch": 2.9233213820078228,
      "grad_norm": 4.547488689422607,
      "learning_rate": 3.538349494784876e-05,
      "loss": 0.4352,
      "step": 143500
    },
    {
      "epoch": 2.9335071707953064,
      "grad_norm": 16.712444305419922,
      "learning_rate": 3.533256600391134e-05,
      "loss": 0.4564,
      "step": 144000
    },
    {
      "epoch": 2.94369295958279,
      "grad_norm": 6.719965934753418,
      "learning_rate": 3.528163705997393e-05,
      "loss": 0.4243,
      "step": 144500
    },
    {
      "epoch": 2.9538787483702738,
      "grad_norm": 4.097908973693848,
      "learning_rate": 3.523070811603651e-05,
      "loss": 0.4302,
      "step": 145000
    },
    {
      "epoch": 2.9640645371577574,
      "grad_norm": 13.10834789276123,
      "learning_rate": 3.5179779172099084e-05,
      "loss": 0.4441,
      "step": 145500
    },
    {
      "epoch": 2.974250325945241,
      "grad_norm": 8.360124588012695,
      "learning_rate": 3.512885022816167e-05,
      "loss": 0.436,
      "step": 146000
    },
    {
      "epoch": 2.9844361147327247,
      "grad_norm": 10.053936004638672,
      "learning_rate": 3.507792128422425e-05,
      "loss": 0.4537,
      "step": 146500
    },
    {
      "epoch": 2.9946219035202084,
      "grad_norm": 6.927604675292969,
      "learning_rate": 3.502699234028683e-05,
      "loss": 0.4495,
      "step": 147000
    },
    {
      "epoch": 3.0048076923076925,
      "grad_norm": 3.8102493286132812,
      "learning_rate": 3.4976063396349414e-05,
      "loss": 0.4504,
      "step": 147500
    },
    {
      "epoch": 3.014993481095176,
      "grad_norm": 11.938340187072754,
      "learning_rate": 3.4925134452411995e-05,
      "loss": 0.4383,
      "step": 148000
    },
    {
      "epoch": 3.02517926988266,
      "grad_norm": 7.8868088722229,
      "learning_rate": 3.4874205508474575e-05,
      "loss": 0.4326,
      "step": 148500
    },
    {
      "epoch": 3.0353650586701435,
      "grad_norm": 7.3001909255981445,
      "learning_rate": 3.482327656453716e-05,
      "loss": 0.4309,
      "step": 149000
    },
    {
      "epoch": 3.045550847457627,
      "grad_norm": 11.97653579711914,
      "learning_rate": 3.4772347620599744e-05,
      "loss": 0.416,
      "step": 149500
    },
    {
      "epoch": 3.055736636245111,
      "grad_norm": 1.429252028465271,
      "learning_rate": 3.472141867666232e-05,
      "loss": 0.4374,
      "step": 150000
    },
    {
      "epoch": 3.0659224250325945,
      "grad_norm": 8.233463287353516,
      "learning_rate": 3.4670489732724905e-05,
      "loss": 0.4065,
      "step": 150500
    },
    {
      "epoch": 3.076108213820078,
      "grad_norm": 9.462811470031738,
      "learning_rate": 3.4619560788787486e-05,
      "loss": 0.4174,
      "step": 151000
    },
    {
      "epoch": 3.086294002607562,
      "grad_norm": 5.782574653625488,
      "learning_rate": 3.4568631844850067e-05,
      "loss": 0.4402,
      "step": 151500
    },
    {
      "epoch": 3.0964797913950455,
      "grad_norm": 10.314330101013184,
      "learning_rate": 3.451770290091265e-05,
      "loss": 0.4143,
      "step": 152000
    },
    {
      "epoch": 3.106665580182529,
      "grad_norm": 9.420683860778809,
      "learning_rate": 3.446677395697523e-05,
      "loss": 0.4329,
      "step": 152500
    },
    {
      "epoch": 3.1168513689700132,
      "grad_norm": 12.175034523010254,
      "learning_rate": 3.441584501303781e-05,
      "loss": 0.4293,
      "step": 153000
    },
    {
      "epoch": 3.127037157757497,
      "grad_norm": 8.21859359741211,
      "learning_rate": 3.4364916069100396e-05,
      "loss": 0.4343,
      "step": 153500
    },
    {
      "epoch": 3.1372229465449806,
      "grad_norm": 10.956506729125977,
      "learning_rate": 3.431398712516297e-05,
      "loss": 0.4499,
      "step": 154000
    },
    {
      "epoch": 3.1474087353324642,
      "grad_norm": 9.675692558288574,
      "learning_rate": 3.426305818122556e-05,
      "loss": 0.4378,
      "step": 154500
    },
    {
      "epoch": 3.157594524119948,
      "grad_norm": 8.080998420715332,
      "learning_rate": 3.421212923728814e-05,
      "loss": 0.4186,
      "step": 155000
    },
    {
      "epoch": 3.1677803129074316,
      "grad_norm": 17.962711334228516,
      "learning_rate": 3.416120029335072e-05,
      "loss": 0.4214,
      "step": 155500
    },
    {
      "epoch": 3.1779661016949152,
      "grad_norm": 2.1962854862213135,
      "learning_rate": 3.41102713494133e-05,
      "loss": 0.4187,
      "step": 156000
    },
    {
      "epoch": 3.188151890482399,
      "grad_norm": 14.06850528717041,
      "learning_rate": 3.405934240547588e-05,
      "loss": 0.4271,
      "step": 156500
    },
    {
      "epoch": 3.1983376792698825,
      "grad_norm": 11.550240516662598,
      "learning_rate": 3.400841346153846e-05,
      "loss": 0.3932,
      "step": 157000
    },
    {
      "epoch": 3.208523468057366,
      "grad_norm": 12.307540893554688,
      "learning_rate": 3.395748451760105e-05,
      "loss": 0.4398,
      "step": 157500
    },
    {
      "epoch": 3.21870925684485,
      "grad_norm": 4.684907913208008,
      "learning_rate": 3.390655557366363e-05,
      "loss": 0.4262,
      "step": 158000
    },
    {
      "epoch": 3.228895045632334,
      "grad_norm": 3.076570510864258,
      "learning_rate": 3.3855626629726204e-05,
      "loss": 0.4414,
      "step": 158500
    },
    {
      "epoch": 3.2390808344198176,
      "grad_norm": 10.635566711425781,
      "learning_rate": 3.380469768578879e-05,
      "loss": 0.4492,
      "step": 159000
    },
    {
      "epoch": 3.2492666232073013,
      "grad_norm": 10.684913635253906,
      "learning_rate": 3.375376874185137e-05,
      "loss": 0.4087,
      "step": 159500
    },
    {
      "epoch": 3.259452411994785,
      "grad_norm": 8.760268211364746,
      "learning_rate": 3.370283979791395e-05,
      "loss": 0.4337,
      "step": 160000
    },
    {
      "epoch": 3.2696382007822686,
      "grad_norm": 12.461752891540527,
      "learning_rate": 3.365191085397653e-05,
      "loss": 0.437,
      "step": 160500
    },
    {
      "epoch": 3.2798239895697523,
      "grad_norm": 10.397443771362305,
      "learning_rate": 3.3600981910039114e-05,
      "loss": 0.4233,
      "step": 161000
    },
    {
      "epoch": 3.290009778357236,
      "grad_norm": 6.171355247497559,
      "learning_rate": 3.3550052966101695e-05,
      "loss": 0.4305,
      "step": 161500
    },
    {
      "epoch": 3.3001955671447196,
      "grad_norm": 6.474238395690918,
      "learning_rate": 3.349912402216428e-05,
      "loss": 0.4357,
      "step": 162000
    },
    {
      "epoch": 3.3103813559322033,
      "grad_norm": 6.27062463760376,
      "learning_rate": 3.3448195078226856e-05,
      "loss": 0.4314,
      "step": 162500
    },
    {
      "epoch": 3.320567144719687,
      "grad_norm": 5.920969009399414,
      "learning_rate": 3.339726613428944e-05,
      "loss": 0.4225,
      "step": 163000
    },
    {
      "epoch": 3.3307529335071706,
      "grad_norm": 19.349529266357422,
      "learning_rate": 3.3346337190352024e-05,
      "loss": 0.4186,
      "step": 163500
    },
    {
      "epoch": 3.3409387222946547,
      "grad_norm": 11.7196626663208,
      "learning_rate": 3.3295408246414605e-05,
      "loss": 0.4052,
      "step": 164000
    },
    {
      "epoch": 3.3511245110821384,
      "grad_norm": 10.163630485534668,
      "learning_rate": 3.3244479302477186e-05,
      "loss": 0.4222,
      "step": 164500
    },
    {
      "epoch": 3.361310299869622,
      "grad_norm": 13.32715129852295,
      "learning_rate": 3.3193550358539767e-05,
      "loss": 0.4293,
      "step": 165000
    },
    {
      "epoch": 3.3714960886571057,
      "grad_norm": 9.462733268737793,
      "learning_rate": 3.314262141460235e-05,
      "loss": 0.4157,
      "step": 165500
    },
    {
      "epoch": 3.3816818774445894,
      "grad_norm": 4.762254238128662,
      "learning_rate": 3.309169247066493e-05,
      "loss": 0.4562,
      "step": 166000
    },
    {
      "epoch": 3.391867666232073,
      "grad_norm": 14.250454902648926,
      "learning_rate": 3.3040763526727515e-05,
      "loss": 0.3892,
      "step": 166500
    },
    {
      "epoch": 3.4020534550195567,
      "grad_norm": 5.609745502471924,
      "learning_rate": 3.298983458279009e-05,
      "loss": 0.4212,
      "step": 167000
    },
    {
      "epoch": 3.4122392438070404,
      "grad_norm": 4.991298198699951,
      "learning_rate": 3.293890563885268e-05,
      "loss": 0.4181,
      "step": 167500
    },
    {
      "epoch": 3.422425032594524,
      "grad_norm": 3.5567095279693604,
      "learning_rate": 3.288797669491526e-05,
      "loss": 0.437,
      "step": 168000
    },
    {
      "epoch": 3.4326108213820077,
      "grad_norm": 4.548188209533691,
      "learning_rate": 3.283704775097784e-05,
      "loss": 0.4506,
      "step": 168500
    },
    {
      "epoch": 3.4427966101694913,
      "grad_norm": 7.6455817222595215,
      "learning_rate": 3.278611880704042e-05,
      "loss": 0.4242,
      "step": 169000
    },
    {
      "epoch": 3.4529823989569755,
      "grad_norm": 17.830543518066406,
      "learning_rate": 3.2735189863103e-05,
      "loss": 0.4386,
      "step": 169500
    },
    {
      "epoch": 3.463168187744459,
      "grad_norm": 8.15147876739502,
      "learning_rate": 3.268426091916558e-05,
      "loss": 0.4268,
      "step": 170000
    },
    {
      "epoch": 3.4733539765319428,
      "grad_norm": 10.574326515197754,
      "learning_rate": 3.263333197522817e-05,
      "loss": 0.4269,
      "step": 170500
    },
    {
      "epoch": 3.4835397653194264,
      "grad_norm": 6.635740756988525,
      "learning_rate": 3.258240303129074e-05,
      "loss": 0.4211,
      "step": 171000
    },
    {
      "epoch": 3.49372555410691,
      "grad_norm": 17.18397331237793,
      "learning_rate": 3.253147408735332e-05,
      "loss": 0.4309,
      "step": 171500
    },
    {
      "epoch": 3.5039113428943938,
      "grad_norm": 7.499168395996094,
      "learning_rate": 3.248054514341591e-05,
      "loss": 0.4302,
      "step": 172000
    },
    {
      "epoch": 3.5140971316818774,
      "grad_norm": 11.109590530395508,
      "learning_rate": 3.242961619947849e-05,
      "loss": 0.4419,
      "step": 172500
    },
    {
      "epoch": 3.524282920469361,
      "grad_norm": 3.4719088077545166,
      "learning_rate": 3.2378687255541065e-05,
      "loss": 0.4302,
      "step": 173000
    },
    {
      "epoch": 3.5344687092568448,
      "grad_norm": 2.457988739013672,
      "learning_rate": 3.232775831160365e-05,
      "loss": 0.4212,
      "step": 173500
    },
    {
      "epoch": 3.5446544980443284,
      "grad_norm": 14.25119686126709,
      "learning_rate": 3.227682936766623e-05,
      "loss": 0.4333,
      "step": 174000
    },
    {
      "epoch": 3.554840286831812,
      "grad_norm": 11.088531494140625,
      "learning_rate": 3.2225900423728814e-05,
      "loss": 0.4126,
      "step": 174500
    },
    {
      "epoch": 3.565026075619296,
      "grad_norm": 10.892101287841797,
      "learning_rate": 3.21749714797914e-05,
      "loss": 0.4262,
      "step": 175000
    },
    {
      "epoch": 3.5752118644067794,
      "grad_norm": 8.11906909942627,
      "learning_rate": 3.2124042535853975e-05,
      "loss": 0.4068,
      "step": 175500
    },
    {
      "epoch": 3.5853976531942635,
      "grad_norm": 14.63786506652832,
      "learning_rate": 3.2073113591916556e-05,
      "loss": 0.4255,
      "step": 176000
    },
    {
      "epoch": 3.595583441981747,
      "grad_norm": 8.716017723083496,
      "learning_rate": 3.2022184647979144e-05,
      "loss": 0.4334,
      "step": 176500
    },
    {
      "epoch": 3.605769230769231,
      "grad_norm": 6.28656005859375,
      "learning_rate": 3.1971255704041724e-05,
      "loss": 0.4242,
      "step": 177000
    },
    {
      "epoch": 3.6159550195567145,
      "grad_norm": 5.02017068862915,
      "learning_rate": 3.1920326760104305e-05,
      "loss": 0.4301,
      "step": 177500
    },
    {
      "epoch": 3.626140808344198,
      "grad_norm": 12.795552253723145,
      "learning_rate": 3.1869397816166886e-05,
      "loss": 0.4232,
      "step": 178000
    },
    {
      "epoch": 3.636326597131682,
      "grad_norm": 8.790449142456055,
      "learning_rate": 3.1818468872229466e-05,
      "loss": 0.4144,
      "step": 178500
    },
    {
      "epoch": 3.6465123859191655,
      "grad_norm": 8.318416595458984,
      "learning_rate": 3.176753992829205e-05,
      "loss": 0.4365,
      "step": 179000
    },
    {
      "epoch": 3.656698174706649,
      "grad_norm": 7.508718967437744,
      "learning_rate": 3.171661098435463e-05,
      "loss": 0.4445,
      "step": 179500
    },
    {
      "epoch": 3.666883963494133,
      "grad_norm": 9.22867202758789,
      "learning_rate": 3.166568204041721e-05,
      "loss": 0.446,
      "step": 180000
    },
    {
      "epoch": 3.677069752281617,
      "grad_norm": 6.447822570800781,
      "learning_rate": 3.1614753096479796e-05,
      "loss": 0.4237,
      "step": 180500
    },
    {
      "epoch": 3.6872555410691,
      "grad_norm": 2.868657350540161,
      "learning_rate": 3.156382415254238e-05,
      "loss": 0.4199,
      "step": 181000
    },
    {
      "epoch": 3.6974413298565842,
      "grad_norm": 5.641506195068359,
      "learning_rate": 3.151289520860495e-05,
      "loss": 0.4336,
      "step": 181500
    },
    {
      "epoch": 3.707627118644068,
      "grad_norm": 10.043843269348145,
      "learning_rate": 3.146196626466754e-05,
      "loss": 0.4137,
      "step": 182000
    },
    {
      "epoch": 3.7178129074315516,
      "grad_norm": 1.6694107055664062,
      "learning_rate": 3.141103732073012e-05,
      "loss": 0.422,
      "step": 182500
    },
    {
      "epoch": 3.7279986962190352,
      "grad_norm": 6.201465129852295,
      "learning_rate": 3.13601083767927e-05,
      "loss": 0.4032,
      "step": 183000
    },
    {
      "epoch": 3.738184485006519,
      "grad_norm": 11.53584098815918,
      "learning_rate": 3.130917943285529e-05,
      "loss": 0.4153,
      "step": 183500
    },
    {
      "epoch": 3.7483702737940026,
      "grad_norm": 14.012738227844238,
      "learning_rate": 3.125825048891786e-05,
      "loss": 0.4314,
      "step": 184000
    },
    {
      "epoch": 3.7585560625814862,
      "grad_norm": 9.899404525756836,
      "learning_rate": 3.120732154498044e-05,
      "loss": 0.4168,
      "step": 184500
    },
    {
      "epoch": 3.76874185136897,
      "grad_norm": 5.274888515472412,
      "learning_rate": 3.115639260104303e-05,
      "loss": 0.403,
      "step": 185000
    },
    {
      "epoch": 3.7789276401564535,
      "grad_norm": 14.714468002319336,
      "learning_rate": 3.110546365710561e-05,
      "loss": 0.4266,
      "step": 185500
    },
    {
      "epoch": 3.7891134289439377,
      "grad_norm": 10.178266525268555,
      "learning_rate": 3.1054534713168184e-05,
      "loss": 0.4314,
      "step": 186000
    },
    {
      "epoch": 3.799299217731421,
      "grad_norm": 5.285598278045654,
      "learning_rate": 3.100360576923077e-05,
      "loss": 0.4253,
      "step": 186500
    },
    {
      "epoch": 3.809485006518905,
      "grad_norm": 8.333234786987305,
      "learning_rate": 3.095267682529335e-05,
      "loss": 0.4347,
      "step": 187000
    },
    {
      "epoch": 3.8196707953063886,
      "grad_norm": 4.245602607727051,
      "learning_rate": 3.090174788135593e-05,
      "loss": 0.4201,
      "step": 187500
    },
    {
      "epoch": 3.8298565840938723,
      "grad_norm": 12.401715278625488,
      "learning_rate": 3.0850818937418514e-05,
      "loss": 0.4269,
      "step": 188000
    },
    {
      "epoch": 3.840042372881356,
      "grad_norm": 8.41383171081543,
      "learning_rate": 3.0799889993481094e-05,
      "loss": 0.426,
      "step": 188500
    },
    {
      "epoch": 3.8502281616688396,
      "grad_norm": 5.656922340393066,
      "learning_rate": 3.074896104954368e-05,
      "loss": 0.4325,
      "step": 189000
    },
    {
      "epoch": 3.8604139504563233,
      "grad_norm": 7.7689361572265625,
      "learning_rate": 3.069803210560626e-05,
      "loss": 0.444,
      "step": 189500
    },
    {
      "epoch": 3.870599739243807,
      "grad_norm": 6.851698398590088,
      "learning_rate": 3.064710316166884e-05,
      "loss": 0.406,
      "step": 190000
    },
    {
      "epoch": 3.8807855280312906,
      "grad_norm": 5.879034042358398,
      "learning_rate": 3.0596174217731424e-05,
      "loss": 0.441,
      "step": 190500
    },
    {
      "epoch": 3.8909713168187743,
      "grad_norm": 8.092108726501465,
      "learning_rate": 3.0545245273794005e-05,
      "loss": 0.4469,
      "step": 191000
    },
    {
      "epoch": 3.9011571056062584,
      "grad_norm": 7.940712928771973,
      "learning_rate": 3.0494316329856586e-05,
      "loss": 0.4203,
      "step": 191500
    },
    {
      "epoch": 3.9113428943937416,
      "grad_norm": 2.449273109436035,
      "learning_rate": 3.0443387385919163e-05,
      "loss": 0.4464,
      "step": 192000
    },
    {
      "epoch": 3.9215286831812257,
      "grad_norm": 8.216251373291016,
      "learning_rate": 3.0392458441981747e-05,
      "loss": 0.4012,
      "step": 192500
    },
    {
      "epoch": 3.9317144719687094,
      "grad_norm": 10.153316497802734,
      "learning_rate": 3.034152949804433e-05,
      "loss": 0.4207,
      "step": 193000
    },
    {
      "epoch": 3.941900260756193,
      "grad_norm": 7.228741645812988,
      "learning_rate": 3.0290600554106912e-05,
      "loss": 0.4207,
      "step": 193500
    },
    {
      "epoch": 3.9520860495436767,
      "grad_norm": 3.5888049602508545,
      "learning_rate": 3.0239671610169496e-05,
      "loss": 0.4175,
      "step": 194000
    },
    {
      "epoch": 3.9622718383311604,
      "grad_norm": 1.4827133417129517,
      "learning_rate": 3.0188742666232073e-05,
      "loss": 0.4347,
      "step": 194500
    },
    {
      "epoch": 3.972457627118644,
      "grad_norm": 13.547689437866211,
      "learning_rate": 3.0137813722294654e-05,
      "loss": 0.4062,
      "step": 195000
    },
    {
      "epoch": 3.9826434159061277,
      "grad_norm": 16.583084106445312,
      "learning_rate": 3.0086884778357238e-05,
      "loss": 0.4276,
      "step": 195500
    },
    {
      "epoch": 3.9928292046936114,
      "grad_norm": 4.168177604675293,
      "learning_rate": 3.0035955834419822e-05,
      "loss": 0.4454,
      "step": 196000
    },
    {
      "epoch": 4.003014993481095,
      "grad_norm": 12.299367904663086,
      "learning_rate": 2.99850268904824e-05,
      "loss": 0.4155,
      "step": 196500
    },
    {
      "epoch": 4.013200782268579,
      "grad_norm": 7.054049491882324,
      "learning_rate": 2.993409794654498e-05,
      "loss": 0.4076,
      "step": 197000
    },
    {
      "epoch": 4.023386571056062,
      "grad_norm": 6.671603202819824,
      "learning_rate": 2.9883169002607564e-05,
      "loss": 0.4081,
      "step": 197500
    },
    {
      "epoch": 4.0335723598435465,
      "grad_norm": 2.571916341781616,
      "learning_rate": 2.9832240058670145e-05,
      "loss": 0.4146,
      "step": 198000
    },
    {
      "epoch": 4.04375814863103,
      "grad_norm": 10.890244483947754,
      "learning_rate": 2.9781311114732723e-05,
      "loss": 0.4014,
      "step": 198500
    },
    {
      "epoch": 4.053943937418514,
      "grad_norm": 12.408102035522461,
      "learning_rate": 2.9730382170795307e-05,
      "loss": 0.3938,
      "step": 199000
    },
    {
      "epoch": 4.064129726205997,
      "grad_norm": 17.27712059020996,
      "learning_rate": 2.967945322685789e-05,
      "loss": 0.3993,
      "step": 199500
    },
    {
      "epoch": 4.074315514993481,
      "grad_norm": 3.8787145614624023,
      "learning_rate": 2.962852428292047e-05,
      "loss": 0.3947,
      "step": 200000
    },
    {
      "epoch": 4.084501303780965,
      "grad_norm": 7.368488788604736,
      "learning_rate": 2.957759533898305e-05,
      "loss": 0.396,
      "step": 200500
    },
    {
      "epoch": 4.094687092568448,
      "grad_norm": 17.056655883789062,
      "learning_rate": 2.9526666395045633e-05,
      "loss": 0.4041,
      "step": 201000
    },
    {
      "epoch": 4.1048728813559325,
      "grad_norm": 10.029211044311523,
      "learning_rate": 2.9475737451108214e-05,
      "loss": 0.4111,
      "step": 201500
    },
    {
      "epoch": 4.115058670143416,
      "grad_norm": 6.271554470062256,
      "learning_rate": 2.9424808507170798e-05,
      "loss": 0.4157,
      "step": 202000
    },
    {
      "epoch": 4.1252444589309,
      "grad_norm": 11.204355239868164,
      "learning_rate": 2.9373879563233382e-05,
      "loss": 0.3943,
      "step": 202500
    },
    {
      "epoch": 4.135430247718383,
      "grad_norm": 2.138756513595581,
      "learning_rate": 2.932295061929596e-05,
      "loss": 0.4051,
      "step": 203000
    },
    {
      "epoch": 4.145616036505867,
      "grad_norm": 7.7544779777526855,
      "learning_rate": 2.927202167535854e-05,
      "loss": 0.4027,
      "step": 203500
    },
    {
      "epoch": 4.15580182529335,
      "grad_norm": 1.4363391399383545,
      "learning_rate": 2.9221092731421124e-05,
      "loss": 0.4217,
      "step": 204000
    },
    {
      "epoch": 4.1659876140808345,
      "grad_norm": 7.119493007659912,
      "learning_rate": 2.9170163787483705e-05,
      "loss": 0.4093,
      "step": 204500
    },
    {
      "epoch": 4.176173402868318,
      "grad_norm": 11.750580787658691,
      "learning_rate": 2.9119234843546282e-05,
      "loss": 0.4119,
      "step": 205000
    },
    {
      "epoch": 4.186359191655802,
      "grad_norm": 9.195052146911621,
      "learning_rate": 2.9068305899608866e-05,
      "loss": 0.407,
      "step": 205500
    },
    {
      "epoch": 4.196544980443286,
      "grad_norm": 11.338869094848633,
      "learning_rate": 2.901737695567145e-05,
      "loss": 0.3958,
      "step": 206000
    },
    {
      "epoch": 4.206730769230769,
      "grad_norm": 13.406786918640137,
      "learning_rate": 2.896644801173403e-05,
      "loss": 0.4385,
      "step": 206500
    },
    {
      "epoch": 4.216916558018253,
      "grad_norm": 8.979273796081543,
      "learning_rate": 2.891551906779661e-05,
      "loss": 0.4089,
      "step": 207000
    },
    {
      "epoch": 4.2271023468057365,
      "grad_norm": 11.221165657043457,
      "learning_rate": 2.8864590123859193e-05,
      "loss": 0.4133,
      "step": 207500
    },
    {
      "epoch": 4.237288135593221,
      "grad_norm": 9.456683158874512,
      "learning_rate": 2.8813661179921773e-05,
      "loss": 0.4115,
      "step": 208000
    },
    {
      "epoch": 4.247473924380704,
      "grad_norm": 6.317917823791504,
      "learning_rate": 2.8762732235984357e-05,
      "loss": 0.3996,
      "step": 208500
    },
    {
      "epoch": 4.257659713168188,
      "grad_norm": 5.092813968658447,
      "learning_rate": 2.8711803292046935e-05,
      "loss": 0.4074,
      "step": 209000
    },
    {
      "epoch": 4.267845501955671,
      "grad_norm": 2.6284995079040527,
      "learning_rate": 2.866087434810952e-05,
      "loss": 0.4154,
      "step": 209500
    },
    {
      "epoch": 4.278031290743155,
      "grad_norm": 21.424896240234375,
      "learning_rate": 2.86099454041721e-05,
      "loss": 0.4169,
      "step": 210000
    },
    {
      "epoch": 4.2882170795306385,
      "grad_norm": 3.4986438751220703,
      "learning_rate": 2.8559016460234684e-05,
      "loss": 0.3865,
      "step": 210500
    },
    {
      "epoch": 4.298402868318123,
      "grad_norm": 2.3319945335388184,
      "learning_rate": 2.8508087516297264e-05,
      "loss": 0.4132,
      "step": 211000
    },
    {
      "epoch": 4.308588657105606,
      "grad_norm": 8.438443183898926,
      "learning_rate": 2.8457158572359842e-05,
      "loss": 0.4159,
      "step": 211500
    },
    {
      "epoch": 4.31877444589309,
      "grad_norm": 10.710570335388184,
      "learning_rate": 2.8406229628422426e-05,
      "loss": 0.4148,
      "step": 212000
    },
    {
      "epoch": 4.328960234680574,
      "grad_norm": 9.005721092224121,
      "learning_rate": 2.835530068448501e-05,
      "loss": 0.3924,
      "step": 212500
    },
    {
      "epoch": 4.339146023468057,
      "grad_norm": 10.325910568237305,
      "learning_rate": 2.830437174054759e-05,
      "loss": 0.43,
      "step": 213000
    },
    {
      "epoch": 4.349331812255541,
      "grad_norm": 10.276771545410156,
      "learning_rate": 2.8253442796610168e-05,
      "loss": 0.3947,
      "step": 213500
    },
    {
      "epoch": 4.3595176010430245,
      "grad_norm": 2.989114999771118,
      "learning_rate": 2.8202513852672752e-05,
      "loss": 0.4282,
      "step": 214000
    },
    {
      "epoch": 4.369703389830509,
      "grad_norm": 8.755563735961914,
      "learning_rate": 2.8151584908735333e-05,
      "loss": 0.4158,
      "step": 214500
    },
    {
      "epoch": 4.379889178617992,
      "grad_norm": 1.748759388923645,
      "learning_rate": 2.8100655964797917e-05,
      "loss": 0.4001,
      "step": 215000
    },
    {
      "epoch": 4.390074967405476,
      "grad_norm": 6.646054744720459,
      "learning_rate": 2.8049727020860494e-05,
      "loss": 0.4129,
      "step": 215500
    },
    {
      "epoch": 4.400260756192959,
      "grad_norm": 10.38259506225586,
      "learning_rate": 2.799879807692308e-05,
      "loss": 0.4205,
      "step": 216000
    },
    {
      "epoch": 4.410446544980443,
      "grad_norm": 18.333168029785156,
      "learning_rate": 2.794786913298566e-05,
      "loss": 0.4095,
      "step": 216500
    },
    {
      "epoch": 4.4206323337679265,
      "grad_norm": 11.978986740112305,
      "learning_rate": 2.7896940189048243e-05,
      "loss": 0.4043,
      "step": 217000
    },
    {
      "epoch": 4.430818122555411,
      "grad_norm": 10.37016773223877,
      "learning_rate": 2.784601124511082e-05,
      "loss": 0.4064,
      "step": 217500
    },
    {
      "epoch": 4.441003911342895,
      "grad_norm": 5.445370197296143,
      "learning_rate": 2.77950823011734e-05,
      "loss": 0.437,
      "step": 218000
    },
    {
      "epoch": 4.451189700130378,
      "grad_norm": 21.018939971923828,
      "learning_rate": 2.7744153357235985e-05,
      "loss": 0.4177,
      "step": 218500
    },
    {
      "epoch": 4.461375488917862,
      "grad_norm": 10.525847434997559,
      "learning_rate": 2.769322441329857e-05,
      "loss": 0.4246,
      "step": 219000
    },
    {
      "epoch": 4.471561277705345,
      "grad_norm": 0.9290366768836975,
      "learning_rate": 2.764229546936115e-05,
      "loss": 0.392,
      "step": 219500
    },
    {
      "epoch": 4.481747066492829,
      "grad_norm": 13.387958526611328,
      "learning_rate": 2.7591366525423728e-05,
      "loss": 0.407,
      "step": 220000
    },
    {
      "epoch": 4.491932855280313,
      "grad_norm": 3.067716598510742,
      "learning_rate": 2.7540437581486312e-05,
      "loss": 0.395,
      "step": 220500
    },
    {
      "epoch": 4.502118644067797,
      "grad_norm": 8.79256820678711,
      "learning_rate": 2.7489508637548896e-05,
      "loss": 0.4208,
      "step": 221000
    },
    {
      "epoch": 4.51230443285528,
      "grad_norm": 5.11510705947876,
      "learning_rate": 2.7438579693611477e-05,
      "loss": 0.4105,
      "step": 221500
    },
    {
      "epoch": 4.522490221642764,
      "grad_norm": 20.15057373046875,
      "learning_rate": 2.7387650749674054e-05,
      "loss": 0.3892,
      "step": 222000
    },
    {
      "epoch": 4.532676010430247,
      "grad_norm": 6.424562454223633,
      "learning_rate": 2.7336721805736638e-05,
      "loss": 0.4251,
      "step": 222500
    },
    {
      "epoch": 4.542861799217731,
      "grad_norm": 3.1851041316986084,
      "learning_rate": 2.728579286179922e-05,
      "loss": 0.4161,
      "step": 223000
    },
    {
      "epoch": 4.5530475880052155,
      "grad_norm": 11.107168197631836,
      "learning_rate": 2.7234863917861803e-05,
      "loss": 0.4075,
      "step": 223500
    },
    {
      "epoch": 4.563233376792699,
      "grad_norm": 9.199618339538574,
      "learning_rate": 2.718393497392438e-05,
      "loss": 0.4165,
      "step": 224000
    },
    {
      "epoch": 4.573419165580183,
      "grad_norm": 9.494927406311035,
      "learning_rate": 2.713300602998696e-05,
      "loss": 0.3994,
      "step": 224500
    },
    {
      "epoch": 4.583604954367666,
      "grad_norm": 8.97567081451416,
      "learning_rate": 2.7082077086049545e-05,
      "loss": 0.4265,
      "step": 225000
    },
    {
      "epoch": 4.59379074315515,
      "grad_norm": 3.0553321838378906,
      "learning_rate": 2.703114814211213e-05,
      "loss": 0.4262,
      "step": 225500
    },
    {
      "epoch": 4.603976531942633,
      "grad_norm": 9.88939380645752,
      "learning_rate": 2.6980219198174706e-05,
      "loss": 0.4108,
      "step": 226000
    },
    {
      "epoch": 4.6141623207301175,
      "grad_norm": 6.45904016494751,
      "learning_rate": 2.6929290254237287e-05,
      "loss": 0.4075,
      "step": 226500
    },
    {
      "epoch": 4.624348109517601,
      "grad_norm": 9.572174072265625,
      "learning_rate": 2.687836131029987e-05,
      "loss": 0.4188,
      "step": 227000
    },
    {
      "epoch": 4.634533898305085,
      "grad_norm": 8.533923149108887,
      "learning_rate": 2.6827432366362455e-05,
      "loss": 0.4211,
      "step": 227500
    },
    {
      "epoch": 4.644719687092568,
      "grad_norm": 11.772581100463867,
      "learning_rate": 2.6776503422425036e-05,
      "loss": 0.4034,
      "step": 228000
    },
    {
      "epoch": 4.654905475880052,
      "grad_norm": 14.506388664245605,
      "learning_rate": 2.6725574478487614e-05,
      "loss": 0.4226,
      "step": 228500
    },
    {
      "epoch": 4.665091264667536,
      "grad_norm": 6.2832932472229,
      "learning_rate": 2.6674645534550198e-05,
      "loss": 0.4307,
      "step": 229000
    },
    {
      "epoch": 4.675277053455019,
      "grad_norm": 1.7358406782150269,
      "learning_rate": 2.662371659061278e-05,
      "loss": 0.4062,
      "step": 229500
    },
    {
      "epoch": 4.6854628422425035,
      "grad_norm": 11.869352340698242,
      "learning_rate": 2.6572787646675362e-05,
      "loss": 0.417,
      "step": 230000
    },
    {
      "epoch": 4.695648631029987,
      "grad_norm": 3.691202163696289,
      "learning_rate": 2.652185870273794e-05,
      "loss": 0.4047,
      "step": 230500
    },
    {
      "epoch": 4.705834419817471,
      "grad_norm": 7.3291730880737305,
      "learning_rate": 2.6470929758800524e-05,
      "loss": 0.4141,
      "step": 231000
    },
    {
      "epoch": 4.716020208604954,
      "grad_norm": 9.920878410339355,
      "learning_rate": 2.6420000814863105e-05,
      "loss": 0.4239,
      "step": 231500
    },
    {
      "epoch": 4.726205997392438,
      "grad_norm": 12.398565292358398,
      "learning_rate": 2.636907187092569e-05,
      "loss": 0.4065,
      "step": 232000
    },
    {
      "epoch": 4.736391786179921,
      "grad_norm": 1.230330228805542,
      "learning_rate": 2.6318142926988266e-05,
      "loss": 0.4186,
      "step": 232500
    },
    {
      "epoch": 4.7465775749674055,
      "grad_norm": 9.790966033935547,
      "learning_rate": 2.6267213983050847e-05,
      "loss": 0.4246,
      "step": 233000
    },
    {
      "epoch": 4.756763363754889,
      "grad_norm": 10.021185874938965,
      "learning_rate": 2.621628503911343e-05,
      "loss": 0.397,
      "step": 233500
    },
    {
      "epoch": 4.766949152542373,
      "grad_norm": 12.653619766235352,
      "learning_rate": 2.6165356095176015e-05,
      "loss": 0.4328,
      "step": 234000
    },
    {
      "epoch": 4.777134941329857,
      "grad_norm": 15.107874870300293,
      "learning_rate": 2.6114427151238592e-05,
      "loss": 0.4168,
      "step": 234500
    },
    {
      "epoch": 4.78732073011734,
      "grad_norm": 10.831413269042969,
      "learning_rate": 2.6063498207301173e-05,
      "loss": 0.417,
      "step": 235000
    },
    {
      "epoch": 4.797506518904824,
      "grad_norm": 14.682290077209473,
      "learning_rate": 2.6012569263363757e-05,
      "loss": 0.4063,
      "step": 235500
    },
    {
      "epoch": 4.8076923076923075,
      "grad_norm": 2.0098776817321777,
      "learning_rate": 2.5961640319426338e-05,
      "loss": 0.4405,
      "step": 236000
    },
    {
      "epoch": 4.817878096479792,
      "grad_norm": 2.7634661197662354,
      "learning_rate": 2.5910711375488915e-05,
      "loss": 0.3965,
      "step": 236500
    },
    {
      "epoch": 4.828063885267275,
      "grad_norm": 2.996753454208374,
      "learning_rate": 2.58597824315515e-05,
      "loss": 0.4083,
      "step": 237000
    },
    {
      "epoch": 4.838249674054759,
      "grad_norm": 12.085972785949707,
      "learning_rate": 2.5808853487614084e-05,
      "loss": 0.4144,
      "step": 237500
    },
    {
      "epoch": 4.848435462842242,
      "grad_norm": 7.362931251525879,
      "learning_rate": 2.5757924543676664e-05,
      "loss": 0.4127,
      "step": 238000
    },
    {
      "epoch": 4.858621251629726,
      "grad_norm": 3.4858033657073975,
      "learning_rate": 2.570699559973925e-05,
      "loss": 0.3978,
      "step": 238500
    },
    {
      "epoch": 4.8688070404172095,
      "grad_norm": 16.799678802490234,
      "learning_rate": 2.5656066655801826e-05,
      "loss": 0.4174,
      "step": 239000
    },
    {
      "epoch": 4.878992829204694,
      "grad_norm": 12.889969825744629,
      "learning_rate": 2.5605137711864406e-05,
      "loss": 0.4211,
      "step": 239500
    },
    {
      "epoch": 4.889178617992178,
      "grad_norm": 5.133121967315674,
      "learning_rate": 2.555420876792699e-05,
      "loss": 0.4086,
      "step": 240000
    },
    {
      "epoch": 4.899364406779661,
      "grad_norm": 15.00344181060791,
      "learning_rate": 2.5503279823989575e-05,
      "loss": 0.4001,
      "step": 240500
    },
    {
      "epoch": 4.909550195567145,
      "grad_norm": 17.959388732910156,
      "learning_rate": 2.5452350880052152e-05,
      "loss": 0.4201,
      "step": 241000
    },
    {
      "epoch": 4.919735984354628,
      "grad_norm": 10.663658142089844,
      "learning_rate": 2.5401421936114733e-05,
      "loss": 0.3987,
      "step": 241500
    },
    {
      "epoch": 4.929921773142112,
      "grad_norm": 18.774873733520508,
      "learning_rate": 2.5350492992177317e-05,
      "loss": 0.4202,
      "step": 242000
    },
    {
      "epoch": 4.9401075619295955,
      "grad_norm": 2.7611937522888184,
      "learning_rate": 2.5299564048239898e-05,
      "loss": 0.3954,
      "step": 242500
    },
    {
      "epoch": 4.95029335071708,
      "grad_norm": 1.9307293891906738,
      "learning_rate": 2.5248635104302475e-05,
      "loss": 0.4045,
      "step": 243000
    },
    {
      "epoch": 4.960479139504563,
      "grad_norm": 10.667132377624512,
      "learning_rate": 2.519770616036506e-05,
      "loss": 0.4055,
      "step": 243500
    },
    {
      "epoch": 4.970664928292047,
      "grad_norm": 5.221114635467529,
      "learning_rate": 2.5146777216427643e-05,
      "loss": 0.4193,
      "step": 244000
    },
    {
      "epoch": 4.98085071707953,
      "grad_norm": 9.956523895263672,
      "learning_rate": 2.5095848272490224e-05,
      "loss": 0.4189,
      "step": 244500
    },
    {
      "epoch": 4.991036505867014,
      "grad_norm": 10.477904319763184,
      "learning_rate": 2.50449193285528e-05,
      "loss": 0.4091,
      "step": 245000
    },
    {
      "epoch": 5.001222294654498,
      "grad_norm": 1.1475757360458374,
      "learning_rate": 2.4993990384615385e-05,
      "loss": 0.4072,
      "step": 245500
    },
    {
      "epoch": 5.011408083441982,
      "grad_norm": 14.476669311523438,
      "learning_rate": 2.4943061440677966e-05,
      "loss": 0.4023,
      "step": 246000
    },
    {
      "epoch": 5.021593872229466,
      "grad_norm": 4.801254749298096,
      "learning_rate": 2.489213249674055e-05,
      "loss": 0.3793,
      "step": 246500
    },
    {
      "epoch": 5.031779661016949,
      "grad_norm": 13.748568534851074,
      "learning_rate": 2.484120355280313e-05,
      "loss": 0.4068,
      "step": 247000
    },
    {
      "epoch": 5.041965449804433,
      "grad_norm": 6.8617658615112305,
      "learning_rate": 2.479027460886571e-05,
      "loss": 0.3994,
      "step": 247500
    },
    {
      "epoch": 5.052151238591916,
      "grad_norm": 9.658553123474121,
      "learning_rate": 2.4739345664928292e-05,
      "loss": 0.4184,
      "step": 248000
    },
    {
      "epoch": 5.0623370273794,
      "grad_norm": 2.732362747192383,
      "learning_rate": 2.4688416720990876e-05,
      "loss": 0.4017,
      "step": 248500
    },
    {
      "epoch": 5.072522816166884,
      "grad_norm": 5.886202812194824,
      "learning_rate": 2.4637487777053457e-05,
      "loss": 0.4101,
      "step": 249000
    },
    {
      "epoch": 5.082708604954368,
      "grad_norm": 6.467099666595459,
      "learning_rate": 2.4586558833116038e-05,
      "loss": 0.3736,
      "step": 249500
    },
    {
      "epoch": 5.092894393741851,
      "grad_norm": 1.4715474843978882,
      "learning_rate": 2.453562988917862e-05,
      "loss": 0.3748,
      "step": 250000
    },
    {
      "epoch": 5.103080182529335,
      "grad_norm": 8.008827209472656,
      "learning_rate": 2.4484700945241203e-05,
      "loss": 0.3877,
      "step": 250500
    },
    {
      "epoch": 5.113265971316819,
      "grad_norm": 13.440597534179688,
      "learning_rate": 2.443377200130378e-05,
      "loss": 0.3898,
      "step": 251000
    },
    {
      "epoch": 5.123451760104302,
      "grad_norm": 0.7765190601348877,
      "learning_rate": 2.4382843057366364e-05,
      "loss": 0.4163,
      "step": 251500
    },
    {
      "epoch": 5.1336375488917865,
      "grad_norm": 4.266671180725098,
      "learning_rate": 2.4331914113428945e-05,
      "loss": 0.4129,
      "step": 252000
    },
    {
      "epoch": 5.14382333767927,
      "grad_norm": 10.900856971740723,
      "learning_rate": 2.4280985169491526e-05,
      "loss": 0.3982,
      "step": 252500
    },
    {
      "epoch": 5.154009126466754,
      "grad_norm": 7.541343688964844,
      "learning_rate": 2.4230056225554106e-05,
      "loss": 0.4095,
      "step": 253000
    },
    {
      "epoch": 5.164194915254237,
      "grad_norm": 4.064420700073242,
      "learning_rate": 2.417912728161669e-05,
      "loss": 0.4041,
      "step": 253500
    },
    {
      "epoch": 5.174380704041721,
      "grad_norm": 10.722648620605469,
      "learning_rate": 2.412819833767927e-05,
      "loss": 0.4048,
      "step": 254000
    },
    {
      "epoch": 5.184566492829204,
      "grad_norm": 9.257691383361816,
      "learning_rate": 2.4077269393741852e-05,
      "loss": 0.3754,
      "step": 254500
    },
    {
      "epoch": 5.1947522816166884,
      "grad_norm": 13.121299743652344,
      "learning_rate": 2.4026340449804436e-05,
      "loss": 0.4012,
      "step": 255000
    },
    {
      "epoch": 5.204938070404172,
      "grad_norm": 15.793761253356934,
      "learning_rate": 2.3975411505867017e-05,
      "loss": 0.3922,
      "step": 255500
    },
    {
      "epoch": 5.215123859191656,
      "grad_norm": 18.72856903076172,
      "learning_rate": 2.3924482561929597e-05,
      "loss": 0.3789,
      "step": 256000
    },
    {
      "epoch": 5.22530964797914,
      "grad_norm": 2.63328218460083,
      "learning_rate": 2.3873553617992178e-05,
      "loss": 0.4163,
      "step": 256500
    },
    {
      "epoch": 5.235495436766623,
      "grad_norm": 6.277614593505859,
      "learning_rate": 2.3822624674054762e-05,
      "loss": 0.3846,
      "step": 257000
    },
    {
      "epoch": 5.245681225554107,
      "grad_norm": 6.0654401779174805,
      "learning_rate": 2.377169573011734e-05,
      "loss": 0.3944,
      "step": 257500
    },
    {
      "epoch": 5.25586701434159,
      "grad_norm": 6.318691253662109,
      "learning_rate": 2.3720766786179924e-05,
      "loss": 0.4049,
      "step": 258000
    },
    {
      "epoch": 5.2660528031290745,
      "grad_norm": 3.5734429359436035,
      "learning_rate": 2.3669837842242504e-05,
      "loss": 0.4163,
      "step": 258500
    },
    {
      "epoch": 5.276238591916558,
      "grad_norm": 9.010658264160156,
      "learning_rate": 2.3618908898305085e-05,
      "loss": 0.4045,
      "step": 259000
    },
    {
      "epoch": 5.286424380704042,
      "grad_norm": 13.526620864868164,
      "learning_rate": 2.3567979954367666e-05,
      "loss": 0.3869,
      "step": 259500
    },
    {
      "epoch": 5.296610169491525,
      "grad_norm": 10.637702941894531,
      "learning_rate": 2.351705101043025e-05,
      "loss": 0.4136,
      "step": 260000
    },
    {
      "epoch": 5.306795958279009,
      "grad_norm": 5.456037998199463,
      "learning_rate": 2.346612206649283e-05,
      "loss": 0.3905,
      "step": 260500
    },
    {
      "epoch": 5.316981747066492,
      "grad_norm": 1.3530329465866089,
      "learning_rate": 2.341519312255541e-05,
      "loss": 0.3939,
      "step": 261000
    },
    {
      "epoch": 5.3271675358539765,
      "grad_norm": 15.11874008178711,
      "learning_rate": 2.3364264178617992e-05,
      "loss": 0.4075,
      "step": 261500
    },
    {
      "epoch": 5.337353324641461,
      "grad_norm": 6.5962066650390625,
      "learning_rate": 2.3313335234680576e-05,
      "loss": 0.4069,
      "step": 262000
    },
    {
      "epoch": 5.347539113428944,
      "grad_norm": 9.900547981262207,
      "learning_rate": 2.3262406290743154e-05,
      "loss": 0.3841,
      "step": 262500
    },
    {
      "epoch": 5.357724902216428,
      "grad_norm": 6.691714286804199,
      "learning_rate": 2.3211477346805738e-05,
      "loss": 0.392,
      "step": 263000
    },
    {
      "epoch": 5.367910691003911,
      "grad_norm": 4.980052471160889,
      "learning_rate": 2.3160548402868322e-05,
      "loss": 0.3967,
      "step": 263500
    },
    {
      "epoch": 5.378096479791395,
      "grad_norm": 1.854934573173523,
      "learning_rate": 2.31096194589309e-05,
      "loss": 0.3993,
      "step": 264000
    },
    {
      "epoch": 5.3882822685788785,
      "grad_norm": 12.968315124511719,
      "learning_rate": 2.3058690514993483e-05,
      "loss": 0.402,
      "step": 264500
    },
    {
      "epoch": 5.398468057366363,
      "grad_norm": 6.432143688201904,
      "learning_rate": 2.3007761571056064e-05,
      "loss": 0.4139,
      "step": 265000
    },
    {
      "epoch": 5.408653846153846,
      "grad_norm": 3.513786792755127,
      "learning_rate": 2.2956832627118645e-05,
      "loss": 0.3896,
      "step": 265500
    },
    {
      "epoch": 5.41883963494133,
      "grad_norm": 8.32616138458252,
      "learning_rate": 2.2905903683181226e-05,
      "loss": 0.3955,
      "step": 266000
    },
    {
      "epoch": 5.429025423728813,
      "grad_norm": 5.913420677185059,
      "learning_rate": 2.285497473924381e-05,
      "loss": 0.4311,
      "step": 266500
    },
    {
      "epoch": 5.439211212516297,
      "grad_norm": 6.1346049308776855,
      "learning_rate": 2.280404579530639e-05,
      "loss": 0.3922,
      "step": 267000
    },
    {
      "epoch": 5.449397001303781,
      "grad_norm": 3.7313578128814697,
      "learning_rate": 2.275311685136897e-05,
      "loss": 0.4099,
      "step": 267500
    },
    {
      "epoch": 5.459582790091265,
      "grad_norm": 13.001245498657227,
      "learning_rate": 2.2702187907431552e-05,
      "loss": 0.4019,
      "step": 268000
    },
    {
      "epoch": 5.469768578878749,
      "grad_norm": 13.707371711730957,
      "learning_rate": 2.2651258963494136e-05,
      "loss": 0.4159,
      "step": 268500
    },
    {
      "epoch": 5.479954367666232,
      "grad_norm": 0.9002312421798706,
      "learning_rate": 2.2600330019556713e-05,
      "loss": 0.3956,
      "step": 269000
    },
    {
      "epoch": 5.490140156453716,
      "grad_norm": 1.194571614265442,
      "learning_rate": 2.2549401075619297e-05,
      "loss": 0.4106,
      "step": 269500
    },
    {
      "epoch": 5.500325945241199,
      "grad_norm": 7.438167095184326,
      "learning_rate": 2.2498472131681878e-05,
      "loss": 0.4284,
      "step": 270000
    },
    {
      "epoch": 5.510511734028683,
      "grad_norm": 14.509721755981445,
      "learning_rate": 2.244754318774446e-05,
      "loss": 0.4082,
      "step": 270500
    },
    {
      "epoch": 5.5206975228161665,
      "grad_norm": 0.853225588798523,
      "learning_rate": 2.239661424380704e-05,
      "loss": 0.4,
      "step": 271000
    },
    {
      "epoch": 5.530883311603651,
      "grad_norm": 5.04412841796875,
      "learning_rate": 2.2345685299869624e-05,
      "loss": 0.3969,
      "step": 271500
    },
    {
      "epoch": 5.541069100391134,
      "grad_norm": 8.865262031555176,
      "learning_rate": 2.2294756355932204e-05,
      "loss": 0.4209,
      "step": 272000
    },
    {
      "epoch": 5.551254889178618,
      "grad_norm": 6.622890949249268,
      "learning_rate": 2.2243827411994785e-05,
      "loss": 0.411,
      "step": 272500
    },
    {
      "epoch": 5.561440677966102,
      "grad_norm": 7.127081871032715,
      "learning_rate": 2.219289846805737e-05,
      "loss": 0.397,
      "step": 273000
    },
    {
      "epoch": 5.571626466753585,
      "grad_norm": 8.527459144592285,
      "learning_rate": 2.214196952411995e-05,
      "loss": 0.3988,
      "step": 273500
    },
    {
      "epoch": 5.581812255541069,
      "grad_norm": 2.5247209072113037,
      "learning_rate": 2.209104058018253e-05,
      "loss": 0.4011,
      "step": 274000
    },
    {
      "epoch": 5.591998044328553,
      "grad_norm": 4.481346607208252,
      "learning_rate": 2.204011163624511e-05,
      "loss": 0.4018,
      "step": 274500
    },
    {
      "epoch": 5.602183833116037,
      "grad_norm": 8.930427551269531,
      "learning_rate": 2.1989182692307696e-05,
      "loss": 0.4271,
      "step": 275000
    },
    {
      "epoch": 5.61236962190352,
      "grad_norm": 6.088761806488037,
      "learning_rate": 2.1938253748370273e-05,
      "loss": 0.3905,
      "step": 275500
    },
    {
      "epoch": 5.622555410691004,
      "grad_norm": 14.152480125427246,
      "learning_rate": 2.1887324804432857e-05,
      "loss": 0.4035,
      "step": 276000
    },
    {
      "epoch": 5.632741199478487,
      "grad_norm": 8.097442626953125,
      "learning_rate": 2.1836395860495438e-05,
      "loss": 0.385,
      "step": 276500
    },
    {
      "epoch": 5.642926988265971,
      "grad_norm": 9.339125633239746,
      "learning_rate": 2.178546691655802e-05,
      "loss": 0.3936,
      "step": 277000
    },
    {
      "epoch": 5.653112777053455,
      "grad_norm": 7.191258430480957,
      "learning_rate": 2.17345379726206e-05,
      "loss": 0.3949,
      "step": 277500
    },
    {
      "epoch": 5.663298565840939,
      "grad_norm": 12.26313591003418,
      "learning_rate": 2.1683609028683183e-05,
      "loss": 0.3977,
      "step": 278000
    },
    {
      "epoch": 5.673484354628423,
      "grad_norm": 12.069415092468262,
      "learning_rate": 2.1632680084745764e-05,
      "loss": 0.4,
      "step": 278500
    },
    {
      "epoch": 5.683670143415906,
      "grad_norm": 11.113691329956055,
      "learning_rate": 2.1581751140808345e-05,
      "loss": 0.392,
      "step": 279000
    },
    {
      "epoch": 5.69385593220339,
      "grad_norm": 10.419079780578613,
      "learning_rate": 2.1530822196870925e-05,
      "loss": 0.427,
      "step": 279500
    },
    {
      "epoch": 5.704041720990873,
      "grad_norm": 14.365821838378906,
      "learning_rate": 2.147989325293351e-05,
      "loss": 0.3943,
      "step": 280000
    },
    {
      "epoch": 5.7142275097783575,
      "grad_norm": 8.588862419128418,
      "learning_rate": 2.1428964308996087e-05,
      "loss": 0.4172,
      "step": 280500
    },
    {
      "epoch": 5.724413298565841,
      "grad_norm": 14.434028625488281,
      "learning_rate": 2.137803536505867e-05,
      "loss": 0.3982,
      "step": 281000
    },
    {
      "epoch": 5.734599087353325,
      "grad_norm": 8.038942337036133,
      "learning_rate": 2.1327106421121255e-05,
      "loss": 0.4195,
      "step": 281500
    },
    {
      "epoch": 5.744784876140808,
      "grad_norm": 0.342111200094223,
      "learning_rate": 2.1276177477183832e-05,
      "loss": 0.3978,
      "step": 282000
    },
    {
      "epoch": 5.754970664928292,
      "grad_norm": 0.7258845567703247,
      "learning_rate": 2.1225248533246417e-05,
      "loss": 0.3958,
      "step": 282500
    },
    {
      "epoch": 5.765156453715775,
      "grad_norm": 11.131864547729492,
      "learning_rate": 2.1174319589308997e-05,
      "loss": 0.4316,
      "step": 283000
    },
    {
      "epoch": 5.7753422425032594,
      "grad_norm": 13.149697303771973,
      "learning_rate": 2.1123390645371578e-05,
      "loss": 0.3979,
      "step": 283500
    },
    {
      "epoch": 5.7855280312907436,
      "grad_norm": 7.813989162445068,
      "learning_rate": 2.107246170143416e-05,
      "loss": 0.4065,
      "step": 284000
    },
    {
      "epoch": 5.795713820078227,
      "grad_norm": 4.0925397872924805,
      "learning_rate": 2.1021532757496743e-05,
      "loss": 0.4139,
      "step": 284500
    },
    {
      "epoch": 5.805899608865711,
      "grad_norm": 7.183837413787842,
      "learning_rate": 2.0970603813559324e-05,
      "loss": 0.4029,
      "step": 285000
    },
    {
      "epoch": 5.816085397653194,
      "grad_norm": 9.013066291809082,
      "learning_rate": 2.0919674869621904e-05,
      "loss": 0.4155,
      "step": 285500
    },
    {
      "epoch": 5.826271186440678,
      "grad_norm": 6.9930195808410645,
      "learning_rate": 2.0868745925684485e-05,
      "loss": 0.3966,
      "step": 286000
    },
    {
      "epoch": 5.836456975228161,
      "grad_norm": 10.895421981811523,
      "learning_rate": 2.081781698174707e-05,
      "loss": 0.3939,
      "step": 286500
    },
    {
      "epoch": 5.8466427640156455,
      "grad_norm": 1.4726698398590088,
      "learning_rate": 2.0766888037809646e-05,
      "loss": 0.3996,
      "step": 287000
    },
    {
      "epoch": 5.856828552803129,
      "grad_norm": 7.837386608123779,
      "learning_rate": 2.071595909387223e-05,
      "loss": 0.4013,
      "step": 287500
    },
    {
      "epoch": 5.867014341590613,
      "grad_norm": 12.157759666442871,
      "learning_rate": 2.066503014993481e-05,
      "loss": 0.3869,
      "step": 288000
    },
    {
      "epoch": 5.877200130378096,
      "grad_norm": 5.89497184753418,
      "learning_rate": 2.0614101205997392e-05,
      "loss": 0.385,
      "step": 288500
    },
    {
      "epoch": 5.88738591916558,
      "grad_norm": 12.256114959716797,
      "learning_rate": 2.0563172262059973e-05,
      "loss": 0.4048,
      "step": 289000
    },
    {
      "epoch": 5.897571707953064,
      "grad_norm": 7.690714359283447,
      "learning_rate": 2.0512243318122557e-05,
      "loss": 0.4052,
      "step": 289500
    },
    {
      "epoch": 5.9077574967405475,
      "grad_norm": 9.003664016723633,
      "learning_rate": 2.0461314374185138e-05,
      "loss": 0.4036,
      "step": 290000
    },
    {
      "epoch": 5.917943285528032,
      "grad_norm": 14.563791275024414,
      "learning_rate": 2.041038543024772e-05,
      "loss": 0.3985,
      "step": 290500
    },
    {
      "epoch": 5.928129074315515,
      "grad_norm": 1.0185116529464722,
      "learning_rate": 2.0359456486310302e-05,
      "loss": 0.391,
      "step": 291000
    },
    {
      "epoch": 5.938314863102999,
      "grad_norm": 9.246405601501465,
      "learning_rate": 2.0308527542372883e-05,
      "loss": 0.4194,
      "step": 291500
    },
    {
      "epoch": 5.948500651890482,
      "grad_norm": 7.7561469078063965,
      "learning_rate": 2.0257598598435464e-05,
      "loss": 0.3987,
      "step": 292000
    },
    {
      "epoch": 5.958686440677966,
      "grad_norm": 8.924543380737305,
      "learning_rate": 2.0206669654498045e-05,
      "loss": 0.3927,
      "step": 292500
    },
    {
      "epoch": 5.9688722294654495,
      "grad_norm": 4.015930652618408,
      "learning_rate": 2.015574071056063e-05,
      "loss": 0.391,
      "step": 293000
    },
    {
      "epoch": 5.979058018252934,
      "grad_norm": 2.291994333267212,
      "learning_rate": 2.0104811766623206e-05,
      "loss": 0.3967,
      "step": 293500
    },
    {
      "epoch": 5.989243807040417,
      "grad_norm": 10.432082176208496,
      "learning_rate": 2.005388282268579e-05,
      "loss": 0.4074,
      "step": 294000
    },
    {
      "epoch": 5.999429595827901,
      "grad_norm": 4.230676651000977,
      "learning_rate": 2.000295387874837e-05,
      "loss": 0.4232,
      "step": 294500
    },
    {
      "epoch": 6.009615384615385,
      "grad_norm": 8.02504825592041,
      "learning_rate": 1.9952024934810955e-05,
      "loss": 0.3778,
      "step": 295000
    },
    {
      "epoch": 6.019801173402868,
      "grad_norm": 2.3453547954559326,
      "learning_rate": 1.9901095990873532e-05,
      "loss": 0.3922,
      "step": 295500
    },
    {
      "epoch": 6.029986962190352,
      "grad_norm": 18.640056610107422,
      "learning_rate": 1.9850167046936116e-05,
      "loss": 0.3811,
      "step": 296000
    },
    {
      "epoch": 6.040172750977836,
      "grad_norm": 1.8246467113494873,
      "learning_rate": 1.9799238102998697e-05,
      "loss": 0.3871,
      "step": 296500
    },
    {
      "epoch": 6.05035853976532,
      "grad_norm": 0.7235602140426636,
      "learning_rate": 1.9748309159061278e-05,
      "loss": 0.4191,
      "step": 297000
    },
    {
      "epoch": 6.060544328552803,
      "grad_norm": 13.81978988647461,
      "learning_rate": 1.969738021512386e-05,
      "loss": 0.3829,
      "step": 297500
    },
    {
      "epoch": 6.070730117340287,
      "grad_norm": 8.44339370727539,
      "learning_rate": 1.9646451271186443e-05,
      "loss": 0.3745,
      "step": 298000
    },
    {
      "epoch": 6.08091590612777,
      "grad_norm": 8.726566314697266,
      "learning_rate": 1.9595522327249023e-05,
      "loss": 0.4053,
      "step": 298500
    },
    {
      "epoch": 6.091101694915254,
      "grad_norm": 10.525443077087402,
      "learning_rate": 1.9544593383311604e-05,
      "loss": 0.4115,
      "step": 299000
    },
    {
      "epoch": 6.1012874837027375,
      "grad_norm": 5.4387946128845215,
      "learning_rate": 1.949366443937419e-05,
      "loss": 0.3692,
      "step": 299500
    },
    {
      "epoch": 6.111473272490222,
      "grad_norm": 12.270941734313965,
      "learning_rate": 1.944273549543677e-05,
      "loss": 0.3866,
      "step": 300000
    },
    {
      "epoch": 6.121659061277706,
      "grad_norm": 11.306867599487305,
      "learning_rate": 1.939180655149935e-05,
      "loss": 0.3841,
      "step": 300500
    },
    {
      "epoch": 6.131844850065189,
      "grad_norm": 9.164039611816406,
      "learning_rate": 1.934087760756193e-05,
      "loss": 0.4017,
      "step": 301000
    },
    {
      "epoch": 6.142030638852673,
      "grad_norm": 8.083893775939941,
      "learning_rate": 1.9289948663624515e-05,
      "loss": 0.389,
      "step": 301500
    },
    {
      "epoch": 6.152216427640156,
      "grad_norm": 19.843761444091797,
      "learning_rate": 1.9239019719687092e-05,
      "loss": 0.388,
      "step": 302000
    },
    {
      "epoch": 6.16240221642764,
      "grad_norm": 12.080591201782227,
      "learning_rate": 1.9188090775749676e-05,
      "loss": 0.3859,
      "step": 302500
    },
    {
      "epoch": 6.172588005215124,
      "grad_norm": 16.980411529541016,
      "learning_rate": 1.9137161831812257e-05,
      "loss": 0.3954,
      "step": 303000
    },
    {
      "epoch": 6.182773794002608,
      "grad_norm": 9.591301918029785,
      "learning_rate": 1.9086232887874838e-05,
      "loss": 0.3984,
      "step": 303500
    },
    {
      "epoch": 6.192959582790091,
      "grad_norm": 10.15821361541748,
      "learning_rate": 1.9035303943937418e-05,
      "loss": 0.361,
      "step": 304000
    },
    {
      "epoch": 6.203145371577575,
      "grad_norm": 0.9231049418449402,
      "learning_rate": 1.8984375000000002e-05,
      "loss": 0.3908,
      "step": 304500
    },
    {
      "epoch": 6.213331160365058,
      "grad_norm": 10.707991600036621,
      "learning_rate": 1.8933446056062583e-05,
      "loss": 0.384,
      "step": 305000
    },
    {
      "epoch": 6.223516949152542,
      "grad_norm": 10.887263298034668,
      "learning_rate": 1.8882517112125164e-05,
      "loss": 0.3894,
      "step": 305500
    },
    {
      "epoch": 6.2337027379400265,
      "grad_norm": 9.465716361999512,
      "learning_rate": 1.8831588168187745e-05,
      "loss": 0.4088,
      "step": 306000
    },
    {
      "epoch": 6.24388852672751,
      "grad_norm": 3.1382203102111816,
      "learning_rate": 1.878065922425033e-05,
      "loss": 0.4157,
      "step": 306500
    },
    {
      "epoch": 6.254074315514994,
      "grad_norm": 6.219305038452148,
      "learning_rate": 1.8729730280312906e-05,
      "loss": 0.4001,
      "step": 307000
    },
    {
      "epoch": 6.264260104302477,
      "grad_norm": 11.079781532287598,
      "learning_rate": 1.867880133637549e-05,
      "loss": 0.3922,
      "step": 307500
    },
    {
      "epoch": 6.274445893089961,
      "grad_norm": 1.5795811414718628,
      "learning_rate": 1.862787239243807e-05,
      "loss": 0.3981,
      "step": 308000
    },
    {
      "epoch": 6.284631681877444,
      "grad_norm": 3.337442398071289,
      "learning_rate": 1.857694344850065e-05,
      "loss": 0.3817,
      "step": 308500
    },
    {
      "epoch": 6.2948174706649285,
      "grad_norm": 14.749442100524902,
      "learning_rate": 1.8526014504563236e-05,
      "loss": 0.3976,
      "step": 309000
    },
    {
      "epoch": 6.305003259452412,
      "grad_norm": 5.9799065589904785,
      "learning_rate": 1.8475085560625816e-05,
      "loss": 0.4085,
      "step": 309500
    },
    {
      "epoch": 6.315189048239896,
      "grad_norm": 25.240657806396484,
      "learning_rate": 1.8424156616688397e-05,
      "loss": 0.3802,
      "step": 310000
    },
    {
      "epoch": 6.325374837027379,
      "grad_norm": 22.325408935546875,
      "learning_rate": 1.8373227672750978e-05,
      "loss": 0.3792,
      "step": 310500
    },
    {
      "epoch": 6.335560625814863,
      "grad_norm": 18.854419708251953,
      "learning_rate": 1.8322298728813562e-05,
      "loss": 0.3991,
      "step": 311000
    },
    {
      "epoch": 6.345746414602347,
      "grad_norm": 16.481416702270508,
      "learning_rate": 1.8271369784876143e-05,
      "loss": 0.4135,
      "step": 311500
    },
    {
      "epoch": 6.3559322033898304,
      "grad_norm": 10.991999626159668,
      "learning_rate": 1.8220440840938723e-05,
      "loss": 0.3936,
      "step": 312000
    },
    {
      "epoch": 6.3661179921773146,
      "grad_norm": 1.0396839380264282,
      "learning_rate": 1.8169511897001304e-05,
      "loss": 0.395,
      "step": 312500
    },
    {
      "epoch": 6.376303780964798,
      "grad_norm": 2.990079641342163,
      "learning_rate": 1.8118582953063888e-05,
      "loss": 0.4135,
      "step": 313000
    },
    {
      "epoch": 6.386489569752282,
      "grad_norm": 0.8005949258804321,
      "learning_rate": 1.8067654009126466e-05,
      "loss": 0.3962,
      "step": 313500
    },
    {
      "epoch": 6.396675358539765,
      "grad_norm": 10.462079048156738,
      "learning_rate": 1.801672506518905e-05,
      "loss": 0.4076,
      "step": 314000
    },
    {
      "epoch": 6.406861147327249,
      "grad_norm": 2.0033061504364014,
      "learning_rate": 1.796579612125163e-05,
      "loss": 0.398,
      "step": 314500
    },
    {
      "epoch": 6.417046936114732,
      "grad_norm": 6.356797695159912,
      "learning_rate": 1.791486717731421e-05,
      "loss": 0.3871,
      "step": 315000
    },
    {
      "epoch": 6.4272327249022165,
      "grad_norm": 14.393778800964355,
      "learning_rate": 1.7863938233376792e-05,
      "loss": 0.3966,
      "step": 315500
    },
    {
      "epoch": 6.4374185136897,
      "grad_norm": 1.701906681060791,
      "learning_rate": 1.7813009289439376e-05,
      "loss": 0.3927,
      "step": 316000
    },
    {
      "epoch": 6.447604302477184,
      "grad_norm": 13.301788330078125,
      "learning_rate": 1.7762080345501957e-05,
      "loss": 0.4044,
      "step": 316500
    },
    {
      "epoch": 6.457790091264668,
      "grad_norm": 6.17011833190918,
      "learning_rate": 1.7711151401564537e-05,
      "loss": 0.3962,
      "step": 317000
    },
    {
      "epoch": 6.467975880052151,
      "grad_norm": 7.231762409210205,
      "learning_rate": 1.766022245762712e-05,
      "loss": 0.3974,
      "step": 317500
    },
    {
      "epoch": 6.478161668839635,
      "grad_norm": 6.195199966430664,
      "learning_rate": 1.7609293513689702e-05,
      "loss": 0.4077,
      "step": 318000
    },
    {
      "epoch": 6.4883474576271185,
      "grad_norm": 11.506616592407227,
      "learning_rate": 1.7558364569752283e-05,
      "loss": 0.3943,
      "step": 318500
    },
    {
      "epoch": 6.498533246414603,
      "grad_norm": 3.344878911972046,
      "learning_rate": 1.7507435625814864e-05,
      "loss": 0.3768,
      "step": 319000
    },
    {
      "epoch": 6.508719035202086,
      "grad_norm": 10.869100570678711,
      "learning_rate": 1.7456506681877448e-05,
      "loss": 0.3935,
      "step": 319500
    },
    {
      "epoch": 6.51890482398957,
      "grad_norm": 0.7028932571411133,
      "learning_rate": 1.7405577737940025e-05,
      "loss": 0.3794,
      "step": 320000
    },
    {
      "epoch": 6.529090612777053,
      "grad_norm": 10.812300682067871,
      "learning_rate": 1.735464879400261e-05,
      "loss": 0.4029,
      "step": 320500
    },
    {
      "epoch": 6.539276401564537,
      "grad_norm": 1.7606737613677979,
      "learning_rate": 1.730371985006519e-05,
      "loss": 0.3903,
      "step": 321000
    },
    {
      "epoch": 6.5494621903520205,
      "grad_norm": 6.972214221954346,
      "learning_rate": 1.725279090612777e-05,
      "loss": 0.3882,
      "step": 321500
    },
    {
      "epoch": 6.559647979139505,
      "grad_norm": 10.028178215026855,
      "learning_rate": 1.720186196219035e-05,
      "loss": 0.3892,
      "step": 322000
    },
    {
      "epoch": 6.569833767926989,
      "grad_norm": 13.1016206741333,
      "learning_rate": 1.7150933018252936e-05,
      "loss": 0.4014,
      "step": 322500
    },
    {
      "epoch": 6.580019556714472,
      "grad_norm": 14.253933906555176,
      "learning_rate": 1.7100004074315516e-05,
      "loss": 0.382,
      "step": 323000
    },
    {
      "epoch": 6.590205345501956,
      "grad_norm": 10.041610717773438,
      "learning_rate": 1.7049075130378097e-05,
      "loss": 0.3861,
      "step": 323500
    },
    {
      "epoch": 6.600391134289439,
      "grad_norm": 18.525419235229492,
      "learning_rate": 1.6998146186440678e-05,
      "loss": 0.4068,
      "step": 324000
    },
    {
      "epoch": 6.610576923076923,
      "grad_norm": 11.241277694702148,
      "learning_rate": 1.6947217242503262e-05,
      "loss": 0.392,
      "step": 324500
    },
    {
      "epoch": 6.620762711864407,
      "grad_norm": 7.804269313812256,
      "learning_rate": 1.689628829856584e-05,
      "loss": 0.4009,
      "step": 325000
    },
    {
      "epoch": 6.630948500651891,
      "grad_norm": 1.5835509300231934,
      "learning_rate": 1.6845359354628423e-05,
      "loss": 0.3896,
      "step": 325500
    },
    {
      "epoch": 6.641134289439374,
      "grad_norm": 4.198485851287842,
      "learning_rate": 1.6794430410691007e-05,
      "loss": 0.411,
      "step": 326000
    },
    {
      "epoch": 6.651320078226858,
      "grad_norm": 9.614863395690918,
      "learning_rate": 1.6743501466753585e-05,
      "loss": 0.4002,
      "step": 326500
    },
    {
      "epoch": 6.661505867014341,
      "grad_norm": 11.445052146911621,
      "learning_rate": 1.669257252281617e-05,
      "loss": 0.4047,
      "step": 327000
    },
    {
      "epoch": 6.671691655801825,
      "grad_norm": 13.951682090759277,
      "learning_rate": 1.664164357887875e-05,
      "loss": 0.4037,
      "step": 327500
    },
    {
      "epoch": 6.681877444589309,
      "grad_norm": 1.7027463912963867,
      "learning_rate": 1.659071463494133e-05,
      "loss": 0.3965,
      "step": 328000
    },
    {
      "epoch": 6.692063233376793,
      "grad_norm": 16.26432991027832,
      "learning_rate": 1.653978569100391e-05,
      "loss": 0.3938,
      "step": 328500
    },
    {
      "epoch": 6.702249022164277,
      "grad_norm": 7.5579094886779785,
      "learning_rate": 1.6488856747066495e-05,
      "loss": 0.3964,
      "step": 329000
    },
    {
      "epoch": 6.71243481095176,
      "grad_norm": 9.531218528747559,
      "learning_rate": 1.6437927803129076e-05,
      "loss": 0.391,
      "step": 329500
    },
    {
      "epoch": 6.722620599739244,
      "grad_norm": 2.45548939704895,
      "learning_rate": 1.6386998859191657e-05,
      "loss": 0.3927,
      "step": 330000
    },
    {
      "epoch": 6.732806388526727,
      "grad_norm": 7.185773849487305,
      "learning_rate": 1.6336069915254237e-05,
      "loss": 0.3888,
      "step": 330500
    },
    {
      "epoch": 6.742992177314211,
      "grad_norm": 1.0512409210205078,
      "learning_rate": 1.628514097131682e-05,
      "loss": 0.3945,
      "step": 331000
    },
    {
      "epoch": 6.753177966101695,
      "grad_norm": 7.269218444824219,
      "learning_rate": 1.62342120273794e-05,
      "loss": 0.4003,
      "step": 331500
    },
    {
      "epoch": 6.763363754889179,
      "grad_norm": 19.85027503967285,
      "learning_rate": 1.6183283083441983e-05,
      "loss": 0.3757,
      "step": 332000
    },
    {
      "epoch": 6.773549543676662,
      "grad_norm": 1.4856669902801514,
      "learning_rate": 1.6132354139504564e-05,
      "loss": 0.4156,
      "step": 332500
    },
    {
      "epoch": 6.783735332464146,
      "grad_norm": 4.146453857421875,
      "learning_rate": 1.6081425195567144e-05,
      "loss": 0.3923,
      "step": 333000
    },
    {
      "epoch": 6.79392112125163,
      "grad_norm": 8.222931861877441,
      "learning_rate": 1.6030496251629725e-05,
      "loss": 0.4073,
      "step": 333500
    },
    {
      "epoch": 6.804106910039113,
      "grad_norm": 5.114683151245117,
      "learning_rate": 1.597956730769231e-05,
      "loss": 0.4001,
      "step": 334000
    },
    {
      "epoch": 6.8142926988265975,
      "grad_norm": 10.151970863342285,
      "learning_rate": 1.592863836375489e-05,
      "loss": 0.3651,
      "step": 334500
    },
    {
      "epoch": 6.824478487614081,
      "grad_norm": 6.31268835067749,
      "learning_rate": 1.587770941981747e-05,
      "loss": 0.3749,
      "step": 335000
    },
    {
      "epoch": 6.834664276401565,
      "grad_norm": 0.5614781975746155,
      "learning_rate": 1.5826780475880055e-05,
      "loss": 0.3927,
      "step": 335500
    },
    {
      "epoch": 6.844850065189048,
      "grad_norm": 19.513290405273438,
      "learning_rate": 1.5775851531942635e-05,
      "loss": 0.3914,
      "step": 336000
    },
    {
      "epoch": 6.855035853976532,
      "grad_norm": 14.175106048583984,
      "learning_rate": 1.5724922588005216e-05,
      "loss": 0.4127,
      "step": 336500
    },
    {
      "epoch": 6.865221642764015,
      "grad_norm": 11.391510963439941,
      "learning_rate": 1.5673993644067797e-05,
      "loss": 0.3707,
      "step": 337000
    },
    {
      "epoch": 6.8754074315514995,
      "grad_norm": 1.1595475673675537,
      "learning_rate": 1.562306470013038e-05,
      "loss": 0.3979,
      "step": 337500
    },
    {
      "epoch": 6.885593220338983,
      "grad_norm": 6.398232936859131,
      "learning_rate": 1.557213575619296e-05,
      "loss": 0.376,
      "step": 338000
    },
    {
      "epoch": 6.895779009126467,
      "grad_norm": 3.1294448375701904,
      "learning_rate": 1.5521206812255543e-05,
      "loss": 0.3801,
      "step": 338500
    },
    {
      "epoch": 6.905964797913951,
      "grad_norm": 20.095306396484375,
      "learning_rate": 1.5470277868318123e-05,
      "loss": 0.3655,
      "step": 339000
    },
    {
      "epoch": 6.916150586701434,
      "grad_norm": 7.464609146118164,
      "learning_rate": 1.5419348924380704e-05,
      "loss": 0.3948,
      "step": 339500
    },
    {
      "epoch": 6.926336375488918,
      "grad_norm": 5.702201843261719,
      "learning_rate": 1.5368419980443285e-05,
      "loss": 0.3961,
      "step": 340000
    },
    {
      "epoch": 6.9365221642764014,
      "grad_norm": 12.89113712310791,
      "learning_rate": 1.531749103650587e-05,
      "loss": 0.3956,
      "step": 340500
    },
    {
      "epoch": 6.9467079530638856,
      "grad_norm": 8.19210433959961,
      "learning_rate": 1.526656209256845e-05,
      "loss": 0.3698,
      "step": 341000
    },
    {
      "epoch": 6.956893741851369,
      "grad_norm": 2.5992090702056885,
      "learning_rate": 1.521563314863103e-05,
      "loss": 0.3812,
      "step": 341500
    },
    {
      "epoch": 6.967079530638853,
      "grad_norm": 11.310589790344238,
      "learning_rate": 1.5164704204693611e-05,
      "loss": 0.4153,
      "step": 342000
    },
    {
      "epoch": 6.977265319426336,
      "grad_norm": 13.891740798950195,
      "learning_rate": 1.5113775260756193e-05,
      "loss": 0.386,
      "step": 342500
    },
    {
      "epoch": 6.98745110821382,
      "grad_norm": 8.108268737792969,
      "learning_rate": 1.5062846316818774e-05,
      "loss": 0.3968,
      "step": 343000
    },
    {
      "epoch": 6.997636897001303,
      "grad_norm": 12.606928825378418,
      "learning_rate": 1.5011917372881357e-05,
      "loss": 0.3849,
      "step": 343500
    },
    {
      "epoch": 7.0078226857887875,
      "grad_norm": 3.785325050354004,
      "learning_rate": 1.4960988428943939e-05,
      "loss": 0.3832,
      "step": 344000
    },
    {
      "epoch": 7.018008474576271,
      "grad_norm": 0.5239374041557312,
      "learning_rate": 1.491005948500652e-05,
      "loss": 0.3725,
      "step": 344500
    },
    {
      "epoch": 7.028194263363755,
      "grad_norm": 21.70319366455078,
      "learning_rate": 1.4859130541069102e-05,
      "loss": 0.3842,
      "step": 345000
    },
    {
      "epoch": 7.038380052151239,
      "grad_norm": 6.73881196975708,
      "learning_rate": 1.4808201597131683e-05,
      "loss": 0.379,
      "step": 345500
    },
    {
      "epoch": 7.048565840938722,
      "grad_norm": 7.576119899749756,
      "learning_rate": 1.4757272653194265e-05,
      "loss": 0.371,
      "step": 346000
    },
    {
      "epoch": 7.058751629726206,
      "grad_norm": 7.206815242767334,
      "learning_rate": 1.4706343709256844e-05,
      "loss": 0.4042,
      "step": 346500
    },
    {
      "epoch": 7.0689374185136895,
      "grad_norm": 0.8810093402862549,
      "learning_rate": 1.4655414765319428e-05,
      "loss": 0.3809,
      "step": 347000
    },
    {
      "epoch": 7.079123207301174,
      "grad_norm": 15.232030868530273,
      "learning_rate": 1.4604485821382007e-05,
      "loss": 0.3987,
      "step": 347500
    },
    {
      "epoch": 7.089308996088657,
      "grad_norm": 12.619032859802246,
      "learning_rate": 1.4553556877444592e-05,
      "loss": 0.3786,
      "step": 348000
    },
    {
      "epoch": 7.099494784876141,
      "grad_norm": 5.663772106170654,
      "learning_rate": 1.450262793350717e-05,
      "loss": 0.3953,
      "step": 348500
    },
    {
      "epoch": 7.109680573663624,
      "grad_norm": 0.736747145652771,
      "learning_rate": 1.4451698989569753e-05,
      "loss": 0.3678,
      "step": 349000
    },
    {
      "epoch": 7.119866362451108,
      "grad_norm": 12.055509567260742,
      "learning_rate": 1.4400770045632334e-05,
      "loss": 0.3706,
      "step": 349500
    },
    {
      "epoch": 7.130052151238592,
      "grad_norm": 16.132423400878906,
      "learning_rate": 1.4349841101694916e-05,
      "loss": 0.3893,
      "step": 350000
    },
    {
      "epoch": 7.140237940026076,
      "grad_norm": 12.807146072387695,
      "learning_rate": 1.4298912157757497e-05,
      "loss": 0.3929,
      "step": 350500
    },
    {
      "epoch": 7.15042372881356,
      "grad_norm": 11.841039657592773,
      "learning_rate": 1.424798321382008e-05,
      "loss": 0.351,
      "step": 351000
    },
    {
      "epoch": 7.160609517601043,
      "grad_norm": 10.265645027160645,
      "learning_rate": 1.419705426988266e-05,
      "loss": 0.3963,
      "step": 351500
    },
    {
      "epoch": 7.170795306388527,
      "grad_norm": 15.257122039794922,
      "learning_rate": 1.4146125325945242e-05,
      "loss": 0.3824,
      "step": 352000
    },
    {
      "epoch": 7.18098109517601,
      "grad_norm": 4.69447135925293,
      "learning_rate": 1.4095196382007821e-05,
      "loss": 0.3634,
      "step": 352500
    },
    {
      "epoch": 7.191166883963494,
      "grad_norm": 7.032171726226807,
      "learning_rate": 1.4044267438070406e-05,
      "loss": 0.3843,
      "step": 353000
    },
    {
      "epoch": 7.201352672750978,
      "grad_norm": 13.626188278198242,
      "learning_rate": 1.3993338494132988e-05,
      "loss": 0.4093,
      "step": 353500
    },
    {
      "epoch": 7.211538461538462,
      "grad_norm": 2.6458523273468018,
      "learning_rate": 1.3942409550195567e-05,
      "loss": 0.3826,
      "step": 354000
    },
    {
      "epoch": 7.221724250325945,
      "grad_norm": 15.797914505004883,
      "learning_rate": 1.3891480606258151e-05,
      "loss": 0.3889,
      "step": 354500
    },
    {
      "epoch": 7.231910039113429,
      "grad_norm": 4.3275909423828125,
      "learning_rate": 1.384055166232073e-05,
      "loss": 0.3637,
      "step": 355000
    },
    {
      "epoch": 7.242095827900912,
      "grad_norm": 17.405542373657227,
      "learning_rate": 1.3789622718383313e-05,
      "loss": 0.394,
      "step": 355500
    },
    {
      "epoch": 7.252281616688396,
      "grad_norm": 8.59645938873291,
      "learning_rate": 1.3738693774445893e-05,
      "loss": 0.3788,
      "step": 356000
    },
    {
      "epoch": 7.26246740547588,
      "grad_norm": 3.4818031787872314,
      "learning_rate": 1.3687764830508476e-05,
      "loss": 0.3802,
      "step": 356500
    },
    {
      "epoch": 7.272653194263364,
      "grad_norm": 7.633153438568115,
      "learning_rate": 1.3636835886571056e-05,
      "loss": 0.3996,
      "step": 357000
    },
    {
      "epoch": 7.282838983050848,
      "grad_norm": 7.016598701477051,
      "learning_rate": 1.3585906942633639e-05,
      "loss": 0.3796,
      "step": 357500
    },
    {
      "epoch": 7.293024771838331,
      "grad_norm": 6.171638011932373,
      "learning_rate": 1.353497799869622e-05,
      "loss": 0.4036,
      "step": 358000
    },
    {
      "epoch": 7.303210560625815,
      "grad_norm": 8.095043182373047,
      "learning_rate": 1.3484049054758802e-05,
      "loss": 0.3943,
      "step": 358500
    },
    {
      "epoch": 7.313396349413298,
      "grad_norm": 4.642739295959473,
      "learning_rate": 1.3433120110821381e-05,
      "loss": 0.3697,
      "step": 359000
    },
    {
      "epoch": 7.323582138200782,
      "grad_norm": 6.839117050170898,
      "learning_rate": 1.3382191166883965e-05,
      "loss": 0.3993,
      "step": 359500
    },
    {
      "epoch": 7.333767926988266,
      "grad_norm": 10.910243034362793,
      "learning_rate": 1.3331262222946544e-05,
      "loss": 0.3876,
      "step": 360000
    },
    {
      "epoch": 7.34395371577575,
      "grad_norm": 12.238420486450195,
      "learning_rate": 1.3280333279009127e-05,
      "loss": 0.4073,
      "step": 360500
    },
    {
      "epoch": 7.354139504563234,
      "grad_norm": 8.929980278015137,
      "learning_rate": 1.3229404335071707e-05,
      "loss": 0.3919,
      "step": 361000
    },
    {
      "epoch": 7.364325293350717,
      "grad_norm": 10.533299446105957,
      "learning_rate": 1.317847539113429e-05,
      "loss": 0.371,
      "step": 361500
    },
    {
      "epoch": 7.374511082138201,
      "grad_norm": 11.860602378845215,
      "learning_rate": 1.3127546447196872e-05,
      "loss": 0.381,
      "step": 362000
    },
    {
      "epoch": 7.384696870925684,
      "grad_norm": 7.346557140350342,
      "learning_rate": 1.3076617503259453e-05,
      "loss": 0.3886,
      "step": 362500
    },
    {
      "epoch": 7.3948826597131685,
      "grad_norm": 11.520604133605957,
      "learning_rate": 1.3025688559322035e-05,
      "loss": 0.3969,
      "step": 363000
    },
    {
      "epoch": 7.405068448500652,
      "grad_norm": 1.6688581705093384,
      "learning_rate": 1.2974759615384616e-05,
      "loss": 0.3701,
      "step": 363500
    },
    {
      "epoch": 7.415254237288136,
      "grad_norm": 11.67809772491455,
      "learning_rate": 1.2923830671447198e-05,
      "loss": 0.3664,
      "step": 364000
    },
    {
      "epoch": 7.425440026075619,
      "grad_norm": 1.7986429929733276,
      "learning_rate": 1.287290172750978e-05,
      "loss": 0.3507,
      "step": 364500
    },
    {
      "epoch": 7.435625814863103,
      "grad_norm": 1.1676450967788696,
      "learning_rate": 1.2821972783572362e-05,
      "loss": 0.3887,
      "step": 365000
    },
    {
      "epoch": 7.445811603650586,
      "grad_norm": 1.8038612604141235,
      "learning_rate": 1.277104383963494e-05,
      "loss": 0.3986,
      "step": 365500
    },
    {
      "epoch": 7.4559973924380705,
      "grad_norm": 14.575918197631836,
      "learning_rate": 1.2720114895697525e-05,
      "loss": 0.4055,
      "step": 366000
    },
    {
      "epoch": 7.466183181225555,
      "grad_norm": 5.649534702301025,
      "learning_rate": 1.2669185951760104e-05,
      "loss": 0.3606,
      "step": 366500
    },
    {
      "epoch": 7.476368970013038,
      "grad_norm": 0.4281367361545563,
      "learning_rate": 1.2618257007822686e-05,
      "loss": 0.3784,
      "step": 367000
    },
    {
      "epoch": 7.486554758800522,
      "grad_norm": 13.579446792602539,
      "learning_rate": 1.2567328063885267e-05,
      "loss": 0.3911,
      "step": 367500
    },
    {
      "epoch": 7.496740547588005,
      "grad_norm": 13.572755813598633,
      "learning_rate": 1.251639911994785e-05,
      "loss": 0.4154,
      "step": 368000
    },
    {
      "epoch": 7.506926336375489,
      "grad_norm": 2.2749135494232178,
      "learning_rate": 1.2465470176010432e-05,
      "loss": 0.3675,
      "step": 368500
    },
    {
      "epoch": 7.5171121251629724,
      "grad_norm": 7.6747517585754395,
      "learning_rate": 1.2414541232073013e-05,
      "loss": 0.3731,
      "step": 369000
    },
    {
      "epoch": 7.5272979139504566,
      "grad_norm": 8.20589828491211,
      "learning_rate": 1.2363612288135593e-05,
      "loss": 0.392,
      "step": 369500
    },
    {
      "epoch": 7.53748370273794,
      "grad_norm": 17.209163665771484,
      "learning_rate": 1.2312683344198176e-05,
      "loss": 0.3875,
      "step": 370000
    },
    {
      "epoch": 7.547669491525424,
      "grad_norm": 9.969813346862793,
      "learning_rate": 1.2261754400260756e-05,
      "loss": 0.4005,
      "step": 370500
    },
    {
      "epoch": 7.557855280312907,
      "grad_norm": 4.334341049194336,
      "learning_rate": 1.2210825456323339e-05,
      "loss": 0.3789,
      "step": 371000
    },
    {
      "epoch": 7.568041069100391,
      "grad_norm": 8.437796592712402,
      "learning_rate": 1.215989651238592e-05,
      "loss": 0.3965,
      "step": 371500
    },
    {
      "epoch": 7.578226857887875,
      "grad_norm": 3.6534981727600098,
      "learning_rate": 1.21089675684485e-05,
      "loss": 0.3948,
      "step": 372000
    },
    {
      "epoch": 7.5884126466753585,
      "grad_norm": 6.727264404296875,
      "learning_rate": 1.2058038624511083e-05,
      "loss": 0.3864,
      "step": 372500
    },
    {
      "epoch": 7.598598435462842,
      "grad_norm": 4.819587707519531,
      "learning_rate": 1.2007109680573663e-05,
      "loss": 0.3849,
      "step": 373000
    },
    {
      "epoch": 7.608784224250326,
      "grad_norm": 8.153926849365234,
      "learning_rate": 1.1956180736636246e-05,
      "loss": 0.3835,
      "step": 373500
    },
    {
      "epoch": 7.61897001303781,
      "grad_norm": 10.230034828186035,
      "learning_rate": 1.1905251792698827e-05,
      "loss": 0.4054,
      "step": 374000
    },
    {
      "epoch": 7.629155801825293,
      "grad_norm": 12.18301773071289,
      "learning_rate": 1.1854322848761407e-05,
      "loss": 0.364,
      "step": 374500
    },
    {
      "epoch": 7.639341590612777,
      "grad_norm": 11.936240196228027,
      "learning_rate": 1.1803393904823991e-05,
      "loss": 0.3833,
      "step": 375000
    },
    {
      "epoch": 7.6495273794002605,
      "grad_norm": 12.141427040100098,
      "learning_rate": 1.1752464960886572e-05,
      "loss": 0.3864,
      "step": 375500
    },
    {
      "epoch": 7.659713168187745,
      "grad_norm": 7.529736042022705,
      "learning_rate": 1.1701536016949153e-05,
      "loss": 0.3634,
      "step": 376000
    },
    {
      "epoch": 7.669898956975228,
      "grad_norm": 4.2634382247924805,
      "learning_rate": 1.1650607073011735e-05,
      "loss": 0.4069,
      "step": 376500
    },
    {
      "epoch": 7.680084745762712,
      "grad_norm": 1.252838134765625,
      "learning_rate": 1.1599678129074316e-05,
      "loss": 0.3941,
      "step": 377000
    },
    {
      "epoch": 7.690270534550196,
      "grad_norm": 10.392666816711426,
      "learning_rate": 1.1548749185136898e-05,
      "loss": 0.3753,
      "step": 377500
    },
    {
      "epoch": 7.700456323337679,
      "grad_norm": 9.235345840454102,
      "learning_rate": 1.1497820241199479e-05,
      "loss": 0.3593,
      "step": 378000
    },
    {
      "epoch": 7.7106421121251625,
      "grad_norm": 10.796313285827637,
      "learning_rate": 1.144689129726206e-05,
      "loss": 0.3987,
      "step": 378500
    },
    {
      "epoch": 7.720827900912647,
      "grad_norm": 9.671149253845215,
      "learning_rate": 1.1395962353324642e-05,
      "loss": 0.3902,
      "step": 379000
    },
    {
      "epoch": 7.731013689700131,
      "grad_norm": 8.162662506103516,
      "learning_rate": 1.1345033409387223e-05,
      "loss": 0.3902,
      "step": 379500
    },
    {
      "epoch": 7.741199478487614,
      "grad_norm": 25.356504440307617,
      "learning_rate": 1.1294104465449805e-05,
      "loss": 0.38,
      "step": 380000
    },
    {
      "epoch": 7.751385267275098,
      "grad_norm": 0.7822400331497192,
      "learning_rate": 1.1243175521512386e-05,
      "loss": 0.3736,
      "step": 380500
    },
    {
      "epoch": 7.761571056062581,
      "grad_norm": 4.197801113128662,
      "learning_rate": 1.1192246577574967e-05,
      "loss": 0.3987,
      "step": 381000
    },
    {
      "epoch": 7.771756844850065,
      "grad_norm": 6.058553218841553,
      "learning_rate": 1.114131763363755e-05,
      "loss": 0.3818,
      "step": 381500
    },
    {
      "epoch": 7.781942633637549,
      "grad_norm": 3.8850769996643066,
      "learning_rate": 1.109038868970013e-05,
      "loss": 0.3868,
      "step": 382000
    },
    {
      "epoch": 7.792128422425033,
      "grad_norm": 9.279074668884277,
      "learning_rate": 1.1039459745762712e-05,
      "loss": 0.382,
      "step": 382500
    },
    {
      "epoch": 7.802314211212517,
      "grad_norm": 15.728307723999023,
      "learning_rate": 1.0988530801825293e-05,
      "loss": 0.3913,
      "step": 383000
    },
    {
      "epoch": 7.8125,
      "grad_norm": 8.997519493103027,
      "learning_rate": 1.0937601857887876e-05,
      "loss": 0.3819,
      "step": 383500
    },
    {
      "epoch": 7.822685788787483,
      "grad_norm": 11.471165657043457,
      "learning_rate": 1.0886672913950458e-05,
      "loss": 0.3801,
      "step": 384000
    },
    {
      "epoch": 7.832871577574967,
      "grad_norm": 11.01817512512207,
      "learning_rate": 1.0835743970013039e-05,
      "loss": 0.3985,
      "step": 384500
    },
    {
      "epoch": 7.843057366362451,
      "grad_norm": 5.7523722648620605,
      "learning_rate": 1.0784815026075621e-05,
      "loss": 0.3747,
      "step": 385000
    },
    {
      "epoch": 7.853243155149935,
      "grad_norm": 0.6532629132270813,
      "learning_rate": 1.0733886082138202e-05,
      "loss": 0.3696,
      "step": 385500
    },
    {
      "epoch": 7.863428943937419,
      "grad_norm": 2.763427972793579,
      "learning_rate": 1.0682957138200783e-05,
      "loss": 0.4016,
      "step": 386000
    },
    {
      "epoch": 7.873614732724902,
      "grad_norm": 13.467397689819336,
      "learning_rate": 1.0632028194263365e-05,
      "loss": 0.3958,
      "step": 386500
    },
    {
      "epoch": 7.883800521512386,
      "grad_norm": 1.1211079359054565,
      "learning_rate": 1.0581099250325946e-05,
      "loss": 0.3832,
      "step": 387000
    },
    {
      "epoch": 7.893986310299869,
      "grad_norm": 8.16834545135498,
      "learning_rate": 1.0530170306388528e-05,
      "loss": 0.376,
      "step": 387500
    },
    {
      "epoch": 7.904172099087353,
      "grad_norm": 19.5809383392334,
      "learning_rate": 1.0479241362451109e-05,
      "loss": 0.3974,
      "step": 388000
    },
    {
      "epoch": 7.9143578878748375,
      "grad_norm": 1.4666714668273926,
      "learning_rate": 1.042831241851369e-05,
      "loss": 0.38,
      "step": 388500
    },
    {
      "epoch": 7.924543676662321,
      "grad_norm": 0.4475567936897278,
      "learning_rate": 1.0377383474576272e-05,
      "loss": 0.376,
      "step": 389000
    },
    {
      "epoch": 7.934729465449804,
      "grad_norm": 21.479578018188477,
      "learning_rate": 1.0326454530638853e-05,
      "loss": 0.4099,
      "step": 389500
    },
    {
      "epoch": 7.944915254237288,
      "grad_norm": 7.458521366119385,
      "learning_rate": 1.0275525586701435e-05,
      "loss": 0.3901,
      "step": 390000
    },
    {
      "epoch": 7.955101043024772,
      "grad_norm": 7.526137351989746,
      "learning_rate": 1.0224596642764016e-05,
      "loss": 0.3999,
      "step": 390500
    },
    {
      "epoch": 7.965286831812255,
      "grad_norm": 7.039983749389648,
      "learning_rate": 1.0173667698826597e-05,
      "loss": 0.3849,
      "step": 391000
    },
    {
      "epoch": 7.9754726205997395,
      "grad_norm": 14.079670906066895,
      "learning_rate": 1.0122738754889179e-05,
      "loss": 0.3879,
      "step": 391500
    },
    {
      "epoch": 7.985658409387223,
      "grad_norm": 0.7053142189979553,
      "learning_rate": 1.007180981095176e-05,
      "loss": 0.3726,
      "step": 392000
    },
    {
      "epoch": 7.995844198174707,
      "grad_norm": 0.7482399940490723,
      "learning_rate": 1.0020880867014342e-05,
      "loss": 0.3867,
      "step": 392500
    },
    {
      "epoch": 8.00602998696219,
      "grad_norm": 2.3860394954681396,
      "learning_rate": 9.969951923076925e-06,
      "loss": 0.3763,
      "step": 393000
    },
    {
      "epoch": 8.016215775749673,
      "grad_norm": 8.872492790222168,
      "learning_rate": 9.919022979139505e-06,
      "loss": 0.3778,
      "step": 393500
    },
    {
      "epoch": 8.026401564537158,
      "grad_norm": 15.148859024047852,
      "learning_rate": 9.868094035202088e-06,
      "loss": 0.3615,
      "step": 394000
    },
    {
      "epoch": 8.036587353324641,
      "grad_norm": 10.090505599975586,
      "learning_rate": 9.817165091264668e-06,
      "loss": 0.3578,
      "step": 394500
    },
    {
      "epoch": 8.046773142112125,
      "grad_norm": 8.039018630981445,
      "learning_rate": 9.76623614732725e-06,
      "loss": 0.3894,
      "step": 395000
    },
    {
      "epoch": 8.05695893089961,
      "grad_norm": 16.242891311645508,
      "learning_rate": 9.715307203389832e-06,
      "loss": 0.3733,
      "step": 395500
    },
    {
      "epoch": 8.067144719687093,
      "grad_norm": 9.830756187438965,
      "learning_rate": 9.664378259452412e-06,
      "loss": 0.3696,
      "step": 396000
    },
    {
      "epoch": 8.077330508474576,
      "grad_norm": 9.530723571777344,
      "learning_rate": 9.613449315514995e-06,
      "loss": 0.3925,
      "step": 396500
    },
    {
      "epoch": 8.08751629726206,
      "grad_norm": 12.716038703918457,
      "learning_rate": 9.562520371577575e-06,
      "loss": 0.3684,
      "step": 397000
    },
    {
      "epoch": 8.097702086049544,
      "grad_norm": 29.437999725341797,
      "learning_rate": 9.511591427640156e-06,
      "loss": 0.4089,
      "step": 397500
    },
    {
      "epoch": 8.107887874837028,
      "grad_norm": 1.735821008682251,
      "learning_rate": 9.460662483702739e-06,
      "loss": 0.3599,
      "step": 398000
    },
    {
      "epoch": 8.11807366362451,
      "grad_norm": 2.609419345855713,
      "learning_rate": 9.40973353976532e-06,
      "loss": 0.3695,
      "step": 398500
    },
    {
      "epoch": 8.128259452411994,
      "grad_norm": 15.281429290771484,
      "learning_rate": 9.358804595827902e-06,
      "loss": 0.3729,
      "step": 399000
    },
    {
      "epoch": 8.138445241199479,
      "grad_norm": 9.120739936828613,
      "learning_rate": 9.307875651890482e-06,
      "loss": 0.3821,
      "step": 399500
    },
    {
      "epoch": 8.148631029986962,
      "grad_norm": 3.6994266510009766,
      "learning_rate": 9.256946707953063e-06,
      "loss": 0.3982,
      "step": 400000
    },
    {
      "epoch": 8.158816818774445,
      "grad_norm": 8.103132247924805,
      "learning_rate": 9.206017764015646e-06,
      "loss": 0.3649,
      "step": 400500
    },
    {
      "epoch": 8.16900260756193,
      "grad_norm": 1.5178135633468628,
      "learning_rate": 9.155088820078226e-06,
      "loss": 0.3891,
      "step": 401000
    },
    {
      "epoch": 8.179188396349414,
      "grad_norm": 6.969143390655518,
      "learning_rate": 9.104159876140809e-06,
      "loss": 0.3562,
      "step": 401500
    },
    {
      "epoch": 8.189374185136897,
      "grad_norm": 12.921985626220703,
      "learning_rate": 9.053230932203391e-06,
      "loss": 0.3692,
      "step": 402000
    },
    {
      "epoch": 8.19955997392438,
      "grad_norm": 1.3632529973983765,
      "learning_rate": 9.002301988265972e-06,
      "loss": 0.3941,
      "step": 402500
    },
    {
      "epoch": 8.209745762711865,
      "grad_norm": 9.800527572631836,
      "learning_rate": 8.951373044328554e-06,
      "loss": 0.3637,
      "step": 403000
    },
    {
      "epoch": 8.219931551499348,
      "grad_norm": 11.7229642868042,
      "learning_rate": 8.900444100391135e-06,
      "loss": 0.3576,
      "step": 403500
    },
    {
      "epoch": 8.230117340286832,
      "grad_norm": 10.19090461730957,
      "learning_rate": 8.849515156453716e-06,
      "loss": 0.3735,
      "step": 404000
    },
    {
      "epoch": 8.240303129074315,
      "grad_norm": 8.01159381866455,
      "learning_rate": 8.798586212516298e-06,
      "loss": 0.3798,
      "step": 404500
    },
    {
      "epoch": 8.2504889178618,
      "grad_norm": 14.529433250427246,
      "learning_rate": 8.747657268578879e-06,
      "loss": 0.4001,
      "step": 405000
    },
    {
      "epoch": 8.260674706649283,
      "grad_norm": 14.341994285583496,
      "learning_rate": 8.696728324641461e-06,
      "loss": 0.3907,
      "step": 405500
    },
    {
      "epoch": 8.270860495436766,
      "grad_norm": 2.791405200958252,
      "learning_rate": 8.645799380704042e-06,
      "loss": 0.3659,
      "step": 406000
    },
    {
      "epoch": 8.281046284224251,
      "grad_norm": 7.59390926361084,
      "learning_rate": 8.594870436766623e-06,
      "loss": 0.383,
      "step": 406500
    },
    {
      "epoch": 8.291232073011734,
      "grad_norm": 3.850207805633545,
      "learning_rate": 8.543941492829205e-06,
      "loss": 0.3591,
      "step": 407000
    },
    {
      "epoch": 8.301417861799218,
      "grad_norm": 11.437399864196777,
      "learning_rate": 8.493012548891786e-06,
      "loss": 0.3861,
      "step": 407500
    },
    {
      "epoch": 8.3116036505867,
      "grad_norm": 1.398573637008667,
      "learning_rate": 8.442083604954368e-06,
      "loss": 0.3967,
      "step": 408000
    },
    {
      "epoch": 8.321789439374186,
      "grad_norm": 9.060970306396484,
      "learning_rate": 8.391154661016949e-06,
      "loss": 0.3713,
      "step": 408500
    },
    {
      "epoch": 8.331975228161669,
      "grad_norm": 2.209052085876465,
      "learning_rate": 8.34022571707953e-06,
      "loss": 0.3793,
      "step": 409000
    },
    {
      "epoch": 8.342161016949152,
      "grad_norm": 8.056268692016602,
      "learning_rate": 8.289296773142112e-06,
      "loss": 0.3879,
      "step": 409500
    },
    {
      "epoch": 8.352346805736635,
      "grad_norm": 0.8918192982673645,
      "learning_rate": 8.238367829204693e-06,
      "loss": 0.3741,
      "step": 410000
    },
    {
      "epoch": 8.36253259452412,
      "grad_norm": 16.194704055786133,
      "learning_rate": 8.187438885267275e-06,
      "loss": 0.3748,
      "step": 410500
    },
    {
      "epoch": 8.372718383311604,
      "grad_norm": 11.312004089355469,
      "learning_rate": 8.136509941329858e-06,
      "loss": 0.3892,
      "step": 411000
    },
    {
      "epoch": 8.382904172099087,
      "grad_norm": 1.4468865394592285,
      "learning_rate": 8.085580997392439e-06,
      "loss": 0.3981,
      "step": 411500
    },
    {
      "epoch": 8.393089960886572,
      "grad_norm": 0.5952428579330444,
      "learning_rate": 8.034652053455021e-06,
      "loss": 0.3815,
      "step": 412000
    },
    {
      "epoch": 8.403275749674055,
      "grad_norm": 6.829463005065918,
      "learning_rate": 7.983723109517602e-06,
      "loss": 0.3783,
      "step": 412500
    },
    {
      "epoch": 8.413461538461538,
      "grad_norm": 0.6776830554008484,
      "learning_rate": 7.932794165580182e-06,
      "loss": 0.3816,
      "step": 413000
    },
    {
      "epoch": 8.423647327249022,
      "grad_norm": 2.264005422592163,
      "learning_rate": 7.881865221642765e-06,
      "loss": 0.3789,
      "step": 413500
    },
    {
      "epoch": 8.433833116036507,
      "grad_norm": 12.783854484558105,
      "learning_rate": 7.830936277705346e-06,
      "loss": 0.4019,
      "step": 414000
    },
    {
      "epoch": 8.44401890482399,
      "grad_norm": 7.022912502288818,
      "learning_rate": 7.780007333767928e-06,
      "loss": 0.3573,
      "step": 414500
    },
    {
      "epoch": 8.454204693611473,
      "grad_norm": 6.407833576202393,
      "learning_rate": 7.729078389830509e-06,
      "loss": 0.3724,
      "step": 415000
    },
    {
      "epoch": 8.464390482398956,
      "grad_norm": 8.57397174835205,
      "learning_rate": 7.67814944589309e-06,
      "loss": 0.3812,
      "step": 415500
    },
    {
      "epoch": 8.474576271186441,
      "grad_norm": 15.997581481933594,
      "learning_rate": 7.627220501955672e-06,
      "loss": 0.365,
      "step": 416000
    },
    {
      "epoch": 8.484762059973924,
      "grad_norm": 3.4063217639923096,
      "learning_rate": 7.576291558018253e-06,
      "loss": 0.3702,
      "step": 416500
    },
    {
      "epoch": 8.494947848761408,
      "grad_norm": 14.704655647277832,
      "learning_rate": 7.525362614080834e-06,
      "loss": 0.3991,
      "step": 417000
    },
    {
      "epoch": 8.50513363754889,
      "grad_norm": 4.923027038574219,
      "learning_rate": 7.474433670143416e-06,
      "loss": 0.3823,
      "step": 417500
    },
    {
      "epoch": 8.515319426336376,
      "grad_norm": 12.015297889709473,
      "learning_rate": 7.423504726205997e-06,
      "loss": 0.3775,
      "step": 418000
    },
    {
      "epoch": 8.525505215123859,
      "grad_norm": 0.6990856528282166,
      "learning_rate": 7.372575782268579e-06,
      "loss": 0.3825,
      "step": 418500
    },
    {
      "epoch": 8.535691003911342,
      "grad_norm": 18.891712188720703,
      "learning_rate": 7.3216468383311604e-06,
      "loss": 0.3954,
      "step": 419000
    },
    {
      "epoch": 8.545876792698827,
      "grad_norm": 9.286857604980469,
      "learning_rate": 7.270717894393741e-06,
      "loss": 0.3775,
      "step": 419500
    },
    {
      "epoch": 8.55606258148631,
      "grad_norm": 12.024374961853027,
      "learning_rate": 7.2197889504563244e-06,
      "loss": 0.3739,
      "step": 420000
    },
    {
      "epoch": 8.566248370273794,
      "grad_norm": 11.569973945617676,
      "learning_rate": 7.168860006518906e-06,
      "loss": 0.3776,
      "step": 420500
    },
    {
      "epoch": 8.576434159061277,
      "grad_norm": 11.273216247558594,
      "learning_rate": 7.117931062581487e-06,
      "loss": 0.3514,
      "step": 421000
    },
    {
      "epoch": 8.586619947848762,
      "grad_norm": 19.77876853942871,
      "learning_rate": 7.067002118644068e-06,
      "loss": 0.3782,
      "step": 421500
    },
    {
      "epoch": 8.596805736636245,
      "grad_norm": 19.149526596069336,
      "learning_rate": 7.01607317470665e-06,
      "loss": 0.3986,
      "step": 422000
    },
    {
      "epoch": 8.606991525423728,
      "grad_norm": 12.570600509643555,
      "learning_rate": 6.9651442307692314e-06,
      "loss": 0.369,
      "step": 422500
    },
    {
      "epoch": 8.617177314211212,
      "grad_norm": 8.821846008300781,
      "learning_rate": 6.914215286831813e-06,
      "loss": 0.376,
      "step": 423000
    },
    {
      "epoch": 8.627363102998697,
      "grad_norm": 1.6607110500335693,
      "learning_rate": 6.863286342894394e-06,
      "loss": 0.3946,
      "step": 423500
    },
    {
      "epoch": 8.63754889178618,
      "grad_norm": 12.340936660766602,
      "learning_rate": 6.812357398956975e-06,
      "loss": 0.3677,
      "step": 424000
    },
    {
      "epoch": 8.647734680573663,
      "grad_norm": 9.723381042480469,
      "learning_rate": 6.761428455019557e-06,
      "loss": 0.3952,
      "step": 424500
    },
    {
      "epoch": 8.657920469361148,
      "grad_norm": 20.63104820251465,
      "learning_rate": 6.7104995110821385e-06,
      "loss": 0.3843,
      "step": 425000
    },
    {
      "epoch": 8.668106258148631,
      "grad_norm": 2.8440520763397217,
      "learning_rate": 6.65957056714472e-06,
      "loss": 0.3877,
      "step": 425500
    },
    {
      "epoch": 8.678292046936114,
      "grad_norm": 8.592601776123047,
      "learning_rate": 6.608641623207301e-06,
      "loss": 0.3882,
      "step": 426000
    },
    {
      "epoch": 8.688477835723598,
      "grad_norm": 2.313809394836426,
      "learning_rate": 6.557712679269882e-06,
      "loss": 0.3746,
      "step": 426500
    },
    {
      "epoch": 8.698663624511083,
      "grad_norm": 2.700324296951294,
      "learning_rate": 6.506783735332464e-06,
      "loss": 0.3646,
      "step": 427000
    },
    {
      "epoch": 8.708849413298566,
      "grad_norm": 17.993993759155273,
      "learning_rate": 6.4558547913950455e-06,
      "loss": 0.3852,
      "step": 427500
    },
    {
      "epoch": 8.719035202086049,
      "grad_norm": 8.776219367980957,
      "learning_rate": 6.404925847457627e-06,
      "loss": 0.3826,
      "step": 428000
    },
    {
      "epoch": 8.729220990873532,
      "grad_norm": 13.274560928344727,
      "learning_rate": 6.3539969035202095e-06,
      "loss": 0.3899,
      "step": 428500
    },
    {
      "epoch": 8.739406779661017,
      "grad_norm": 10.768257141113281,
      "learning_rate": 6.303067959582791e-06,
      "loss": 0.3635,
      "step": 429000
    },
    {
      "epoch": 8.7495925684485,
      "grad_norm": 13.631868362426758,
      "learning_rate": 6.252139015645373e-06,
      "loss": 0.367,
      "step": 429500
    },
    {
      "epoch": 8.759778357235984,
      "grad_norm": 7.484901428222656,
      "learning_rate": 6.201210071707953e-06,
      "loss": 0.3792,
      "step": 430000
    },
    {
      "epoch": 8.769964146023469,
      "grad_norm": 10.996405601501465,
      "learning_rate": 6.150281127770535e-06,
      "loss": 0.3598,
      "step": 430500
    },
    {
      "epoch": 8.780149934810952,
      "grad_norm": 3.0370452404022217,
      "learning_rate": 6.0993521838331165e-06,
      "loss": 0.3677,
      "step": 431000
    },
    {
      "epoch": 8.790335723598435,
      "grad_norm": 17.8145751953125,
      "learning_rate": 6.048423239895698e-06,
      "loss": 0.3964,
      "step": 431500
    },
    {
      "epoch": 8.800521512385918,
      "grad_norm": 0.6044010519981384,
      "learning_rate": 5.99749429595828e-06,
      "loss": 0.3819,
      "step": 432000
    },
    {
      "epoch": 8.810707301173403,
      "grad_norm": 10.036141395568848,
      "learning_rate": 5.946565352020861e-06,
      "loss": 0.3612,
      "step": 432500
    },
    {
      "epoch": 8.820893089960887,
      "grad_norm": 8.41771125793457,
      "learning_rate": 5.895636408083442e-06,
      "loss": 0.3771,
      "step": 433000
    },
    {
      "epoch": 8.83107887874837,
      "grad_norm": 6.7518792152404785,
      "learning_rate": 5.8447074641460235e-06,
      "loss": 0.3889,
      "step": 433500
    },
    {
      "epoch": 8.841264667535853,
      "grad_norm": 3.1178934574127197,
      "learning_rate": 5.793778520208605e-06,
      "loss": 0.3681,
      "step": 434000
    },
    {
      "epoch": 8.851450456323338,
      "grad_norm": 13.712972640991211,
      "learning_rate": 5.742849576271187e-06,
      "loss": 0.3966,
      "step": 434500
    },
    {
      "epoch": 8.861636245110821,
      "grad_norm": 1.9305857419967651,
      "learning_rate": 5.691920632333768e-06,
      "loss": 0.39,
      "step": 435000
    },
    {
      "epoch": 8.871822033898304,
      "grad_norm": 6.789309024810791,
      "learning_rate": 5.64099168839635e-06,
      "loss": 0.361,
      "step": 435500
    },
    {
      "epoch": 8.88200782268579,
      "grad_norm": 10.666377067565918,
      "learning_rate": 5.590062744458931e-06,
      "loss": 0.3725,
      "step": 436000
    },
    {
      "epoch": 8.892193611473273,
      "grad_norm": 6.6772990226745605,
      "learning_rate": 5.539133800521513e-06,
      "loss": 0.3765,
      "step": 436500
    },
    {
      "epoch": 8.902379400260756,
      "grad_norm": 5.1145100593566895,
      "learning_rate": 5.4882048565840945e-06,
      "loss": 0.3649,
      "step": 437000
    },
    {
      "epoch": 8.91256518904824,
      "grad_norm": 0.9134565591812134,
      "learning_rate": 5.437275912646675e-06,
      "loss": 0.3825,
      "step": 437500
    },
    {
      "epoch": 8.922750977835724,
      "grad_norm": 3.586257219314575,
      "learning_rate": 5.386346968709257e-06,
      "loss": 0.386,
      "step": 438000
    },
    {
      "epoch": 8.932936766623207,
      "grad_norm": 7.300569534301758,
      "learning_rate": 5.335418024771838e-06,
      "loss": 0.3789,
      "step": 438500
    },
    {
      "epoch": 8.94312255541069,
      "grad_norm": 5.866146564483643,
      "learning_rate": 5.28448908083442e-06,
      "loss": 0.3838,
      "step": 439000
    },
    {
      "epoch": 8.953308344198174,
      "grad_norm": 11.171117782592773,
      "learning_rate": 5.2335601368970015e-06,
      "loss": 0.3782,
      "step": 439500
    },
    {
      "epoch": 8.963494132985659,
      "grad_norm": 11.862093925476074,
      "learning_rate": 5.182631192959583e-06,
      "loss": 0.3728,
      "step": 440000
    },
    {
      "epoch": 8.973679921773142,
      "grad_norm": 14.071413040161133,
      "learning_rate": 5.131702249022165e-06,
      "loss": 0.3736,
      "step": 440500
    },
    {
      "epoch": 8.983865710560625,
      "grad_norm": 19.723928451538086,
      "learning_rate": 5.080773305084746e-06,
      "loss": 0.3616,
      "step": 441000
    },
    {
      "epoch": 8.99405149934811,
      "grad_norm": 0.8851810097694397,
      "learning_rate": 5.029844361147328e-06,
      "loss": 0.3573,
      "step": 441500
    },
    {
      "epoch": 9.004237288135593,
      "grad_norm": 12.502819061279297,
      "learning_rate": 4.9789154172099085e-06,
      "loss": 0.3753,
      "step": 442000
    },
    {
      "epoch": 9.014423076923077,
      "grad_norm": 0.22742924094200134,
      "learning_rate": 4.92798647327249e-06,
      "loss": 0.3677,
      "step": 442500
    },
    {
      "epoch": 9.02460886571056,
      "grad_norm": 2.2969346046447754,
      "learning_rate": 4.877057529335072e-06,
      "loss": 0.3555,
      "step": 443000
    },
    {
      "epoch": 9.034794654498045,
      "grad_norm": 7.525805473327637,
      "learning_rate": 4.826128585397653e-06,
      "loss": 0.3734,
      "step": 443500
    },
    {
      "epoch": 9.044980443285528,
      "grad_norm": 1.734435796737671,
      "learning_rate": 4.775199641460235e-06,
      "loss": 0.3893,
      "step": 444000
    },
    {
      "epoch": 9.055166232073011,
      "grad_norm": 13.832061767578125,
      "learning_rate": 4.724270697522816e-06,
      "loss": 0.3668,
      "step": 444500
    },
    {
      "epoch": 9.065352020860496,
      "grad_norm": 10.286893844604492,
      "learning_rate": 4.673341753585398e-06,
      "loss": 0.3487,
      "step": 445000
    },
    {
      "epoch": 9.07553780964798,
      "grad_norm": 8.211054801940918,
      "learning_rate": 4.6224128096479795e-06,
      "loss": 0.3643,
      "step": 445500
    },
    {
      "epoch": 9.085723598435463,
      "grad_norm": 12.049545288085938,
      "learning_rate": 4.571483865710561e-06,
      "loss": 0.3833,
      "step": 446000
    },
    {
      "epoch": 9.095909387222946,
      "grad_norm": 17.807931900024414,
      "learning_rate": 4.520554921773143e-06,
      "loss": 0.408,
      "step": 446500
    },
    {
      "epoch": 9.106095176010431,
      "grad_norm": 11.287349700927734,
      "learning_rate": 4.469625977835723e-06,
      "loss": 0.4059,
      "step": 447000
    },
    {
      "epoch": 9.116280964797914,
      "grad_norm": 3.9559407234191895,
      "learning_rate": 4.418697033898305e-06,
      "loss": 0.3757,
      "step": 447500
    },
    {
      "epoch": 9.126466753585397,
      "grad_norm": 6.624411106109619,
      "learning_rate": 4.3677680899608866e-06,
      "loss": 0.3573,
      "step": 448000
    },
    {
      "epoch": 9.13665254237288,
      "grad_norm": 18.72463035583496,
      "learning_rate": 4.316839146023468e-06,
      "loss": 0.3925,
      "step": 448500
    },
    {
      "epoch": 9.146838331160366,
      "grad_norm": 0.2742168605327606,
      "learning_rate": 4.26591020208605e-06,
      "loss": 0.389,
      "step": 449000
    },
    {
      "epoch": 9.157024119947849,
      "grad_norm": 11.384761810302734,
      "learning_rate": 4.214981258148631e-06,
      "loss": 0.3784,
      "step": 449500
    },
    {
      "epoch": 9.167209908735332,
      "grad_norm": 5.662493705749512,
      "learning_rate": 4.164052314211213e-06,
      "loss": 0.3796,
      "step": 450000
    },
    {
      "epoch": 9.177395697522817,
      "grad_norm": 13.778042793273926,
      "learning_rate": 4.113123370273794e-06,
      "loss": 0.3616,
      "step": 450500
    },
    {
      "epoch": 9.1875814863103,
      "grad_norm": 7.551876544952393,
      "learning_rate": 4.062194426336376e-06,
      "loss": 0.3476,
      "step": 451000
    },
    {
      "epoch": 9.197767275097783,
      "grad_norm": 38.43654251098633,
      "learning_rate": 4.011265482398957e-06,
      "loss": 0.3736,
      "step": 451500
    },
    {
      "epoch": 9.207953063885267,
      "grad_norm": 3.2689735889434814,
      "learning_rate": 3.960336538461538e-06,
      "loss": 0.4068,
      "step": 452000
    },
    {
      "epoch": 9.218138852672752,
      "grad_norm": 11.468966484069824,
      "learning_rate": 3.90940759452412e-06,
      "loss": 0.3835,
      "step": 452500
    },
    {
      "epoch": 9.228324641460235,
      "grad_norm": 12.87534236907959,
      "learning_rate": 3.858478650586702e-06,
      "loss": 0.3651,
      "step": 453000
    },
    {
      "epoch": 9.238510430247718,
      "grad_norm": 9.195168495178223,
      "learning_rate": 3.8075497066492834e-06,
      "loss": 0.3618,
      "step": 453500
    },
    {
      "epoch": 9.248696219035201,
      "grad_norm": 17.3817195892334,
      "learning_rate": 3.7566207627118646e-06,
      "loss": 0.3787,
      "step": 454000
    },
    {
      "epoch": 9.258882007822686,
      "grad_norm": 1.7122384309768677,
      "learning_rate": 3.705691818774446e-06,
      "loss": 0.3721,
      "step": 454500
    },
    {
      "epoch": 9.26906779661017,
      "grad_norm": 10.170698165893555,
      "learning_rate": 3.6547628748370277e-06,
      "loss": 0.3735,
      "step": 455000
    },
    {
      "epoch": 9.279253585397653,
      "grad_norm": 10.658530235290527,
      "learning_rate": 3.603833930899609e-06,
      "loss": 0.3822,
      "step": 455500
    },
    {
      "epoch": 9.289439374185136,
      "grad_norm": 21.91527557373047,
      "learning_rate": 3.5529049869621905e-06,
      "loss": 0.397,
      "step": 456000
    },
    {
      "epoch": 9.299625162972621,
      "grad_norm": 12.654435157775879,
      "learning_rate": 3.5019760430247716e-06,
      "loss": 0.3668,
      "step": 456500
    },
    {
      "epoch": 9.309810951760104,
      "grad_norm": 18.22873878479004,
      "learning_rate": 3.451047099087353e-06,
      "loss": 0.3783,
      "step": 457000
    },
    {
      "epoch": 9.319996740547587,
      "grad_norm": 2.6607556343078613,
      "learning_rate": 3.400118155149935e-06,
      "loss": 0.3902,
      "step": 457500
    },
    {
      "epoch": 9.330182529335072,
      "grad_norm": 0.834313154220581,
      "learning_rate": 3.3491892112125167e-06,
      "loss": 0.3981,
      "step": 458000
    },
    {
      "epoch": 9.340368318122556,
      "grad_norm": 8.62730884552002,
      "learning_rate": 3.2982602672750983e-06,
      "loss": 0.3771,
      "step": 458500
    },
    {
      "epoch": 9.350554106910039,
      "grad_norm": 4.378833293914795,
      "learning_rate": 3.2473313233376795e-06,
      "loss": 0.3661,
      "step": 459000
    },
    {
      "epoch": 9.360739895697522,
      "grad_norm": 17.47856330871582,
      "learning_rate": 3.196402379400261e-06,
      "loss": 0.3937,
      "step": 459500
    },
    {
      "epoch": 9.370925684485007,
      "grad_norm": 6.261695384979248,
      "learning_rate": 3.145473435462842e-06,
      "loss": 0.3794,
      "step": 460000
    },
    {
      "epoch": 9.38111147327249,
      "grad_norm": 12.112814903259277,
      "learning_rate": 3.0945444915254238e-06,
      "loss": 0.3796,
      "step": 460500
    },
    {
      "epoch": 9.391297262059974,
      "grad_norm": 11.01955795288086,
      "learning_rate": 3.0436155475880053e-06,
      "loss": 0.3751,
      "step": 461000
    },
    {
      "epoch": 9.401483050847457,
      "grad_norm": 5.091585636138916,
      "learning_rate": 2.992686603650587e-06,
      "loss": 0.3801,
      "step": 461500
    },
    {
      "epoch": 9.411668839634942,
      "grad_norm": 9.787166595458984,
      "learning_rate": 2.9417576597131685e-06,
      "loss": 0.3607,
      "step": 462000
    },
    {
      "epoch": 9.421854628422425,
      "grad_norm": 5.3006062507629395,
      "learning_rate": 2.8908287157757496e-06,
      "loss": 0.3858,
      "step": 462500
    },
    {
      "epoch": 9.432040417209908,
      "grad_norm": 21.651268005371094,
      "learning_rate": 2.839899771838331e-06,
      "loss": 0.3788,
      "step": 463000
    },
    {
      "epoch": 9.442226205997393,
      "grad_norm": 10.771814346313477,
      "learning_rate": 2.7889708279009128e-06,
      "loss": 0.3688,
      "step": 463500
    },
    {
      "epoch": 9.452411994784876,
      "grad_norm": 6.6535964012146,
      "learning_rate": 2.7380418839634943e-06,
      "loss": 0.3732,
      "step": 464000
    },
    {
      "epoch": 9.46259778357236,
      "grad_norm": 29.89267921447754,
      "learning_rate": 2.6871129400260755e-06,
      "loss": 0.359,
      "step": 464500
    },
    {
      "epoch": 9.472783572359843,
      "grad_norm": 11.078018188476562,
      "learning_rate": 2.636183996088657e-06,
      "loss": 0.3963,
      "step": 465000
    },
    {
      "epoch": 9.482969361147328,
      "grad_norm": 23.505146026611328,
      "learning_rate": 2.585255052151239e-06,
      "loss": 0.3892,
      "step": 465500
    },
    {
      "epoch": 9.493155149934811,
      "grad_norm": 18.20115089416504,
      "learning_rate": 2.53432610821382e-06,
      "loss": 0.3819,
      "step": 466000
    },
    {
      "epoch": 9.503340938722294,
      "grad_norm": 6.695158004760742,
      "learning_rate": 2.4833971642764018e-06,
      "loss": 0.3707,
      "step": 466500
    },
    {
      "epoch": 9.513526727509777,
      "grad_norm": 17.957435607910156,
      "learning_rate": 2.432468220338983e-06,
      "loss": 0.3653,
      "step": 467000
    },
    {
      "epoch": 9.523712516297262,
      "grad_norm": 14.551542282104492,
      "learning_rate": 2.381539276401565e-06,
      "loss": 0.3818,
      "step": 467500
    },
    {
      "epoch": 9.533898305084746,
      "grad_norm": 7.713936805725098,
      "learning_rate": 2.330610332464146e-06,
      "loss": 0.3904,
      "step": 468000
    },
    {
      "epoch": 9.544084093872229,
      "grad_norm": 22.707605361938477,
      "learning_rate": 2.2796813885267276e-06,
      "loss": 0.3619,
      "step": 468500
    },
    {
      "epoch": 9.554269882659714,
      "grad_norm": 13.690971374511719,
      "learning_rate": 2.2287524445893092e-06,
      "loss": 0.3605,
      "step": 469000
    },
    {
      "epoch": 9.564455671447197,
      "grad_norm": 7.193748474121094,
      "learning_rate": 2.1778235006518904e-06,
      "loss": 0.3651,
      "step": 469500
    },
    {
      "epoch": 9.57464146023468,
      "grad_norm": 8.926946640014648,
      "learning_rate": 2.1268945567144724e-06,
      "loss": 0.3754,
      "step": 470000
    },
    {
      "epoch": 9.584827249022164,
      "grad_norm": 10.292336463928223,
      "learning_rate": 2.0759656127770535e-06,
      "loss": 0.3792,
      "step": 470500
    },
    {
      "epoch": 9.595013037809649,
      "grad_norm": 11.59450912475586,
      "learning_rate": 2.025036668839635e-06,
      "loss": 0.3691,
      "step": 471000
    },
    {
      "epoch": 9.605198826597132,
      "grad_norm": 0.649363100528717,
      "learning_rate": 1.9741077249022162e-06,
      "loss": 0.3642,
      "step": 471500
    },
    {
      "epoch": 9.615384615384615,
      "grad_norm": 5.829932689666748,
      "learning_rate": 1.9231787809647982e-06,
      "loss": 0.3637,
      "step": 472000
    },
    {
      "epoch": 9.625570404172098,
      "grad_norm": 10.9273681640625,
      "learning_rate": 1.8722498370273796e-06,
      "loss": 0.3755,
      "step": 472500
    },
    {
      "epoch": 9.635756192959583,
      "grad_norm": 5.034908294677734,
      "learning_rate": 1.821320893089961e-06,
      "loss": 0.3816,
      "step": 473000
    },
    {
      "epoch": 9.645941981747066,
      "grad_norm": 6.8455705642700195,
      "learning_rate": 1.7703919491525423e-06,
      "loss": 0.3443,
      "step": 473500
    },
    {
      "epoch": 9.65612777053455,
      "grad_norm": 12.958783149719238,
      "learning_rate": 1.7194630052151239e-06,
      "loss": 0.3681,
      "step": 474000
    },
    {
      "epoch": 9.666313559322035,
      "grad_norm": 8.226170539855957,
      "learning_rate": 1.6685340612777055e-06,
      "loss": 0.38,
      "step": 474500
    },
    {
      "epoch": 9.676499348109518,
      "grad_norm": 0.7670891284942627,
      "learning_rate": 1.617605117340287e-06,
      "loss": 0.3754,
      "step": 475000
    },
    {
      "epoch": 9.686685136897001,
      "grad_norm": 10.160887718200684,
      "learning_rate": 1.5666761734028684e-06,
      "loss": 0.3803,
      "step": 475500
    },
    {
      "epoch": 9.696870925684484,
      "grad_norm": 12.025693893432617,
      "learning_rate": 1.51574722946545e-06,
      "loss": 0.3655,
      "step": 476000
    },
    {
      "epoch": 9.70705671447197,
      "grad_norm": 9.850225448608398,
      "learning_rate": 1.4648182855280313e-06,
      "loss": 0.3875,
      "step": 476500
    },
    {
      "epoch": 9.717242503259452,
      "grad_norm": 16.108551025390625,
      "learning_rate": 1.413889341590613e-06,
      "loss": 0.3786,
      "step": 477000
    },
    {
      "epoch": 9.727428292046936,
      "grad_norm": 17.54432487487793,
      "learning_rate": 1.3629603976531943e-06,
      "loss": 0.3418,
      "step": 477500
    },
    {
      "epoch": 9.737614080834419,
      "grad_norm": 10.961540222167969,
      "learning_rate": 1.3120314537157758e-06,
      "loss": 0.3713,
      "step": 478000
    },
    {
      "epoch": 9.747799869621904,
      "grad_norm": 1.565496563911438,
      "learning_rate": 1.2611025097783574e-06,
      "loss": 0.3719,
      "step": 478500
    },
    {
      "epoch": 9.757985658409387,
      "grad_norm": 12.539321899414062,
      "learning_rate": 1.2101735658409388e-06,
      "loss": 0.3685,
      "step": 479000
    },
    {
      "epoch": 9.76817144719687,
      "grad_norm": 6.349011421203613,
      "learning_rate": 1.1592446219035203e-06,
      "loss": 0.3855,
      "step": 479500
    },
    {
      "epoch": 9.778357235984355,
      "grad_norm": 13.744929313659668,
      "learning_rate": 1.1083156779661017e-06,
      "loss": 0.362,
      "step": 480000
    },
    {
      "epoch": 9.788543024771839,
      "grad_norm": 1.6391286849975586,
      "learning_rate": 1.0573867340286833e-06,
      "loss": 0.3744,
      "step": 480500
    },
    {
      "epoch": 9.798728813559322,
      "grad_norm": 8.637030601501465,
      "learning_rate": 1.0064577900912646e-06,
      "loss": 0.3821,
      "step": 481000
    },
    {
      "epoch": 9.808914602346805,
      "grad_norm": 14.151241302490234,
      "learning_rate": 9.555288461538462e-07,
      "loss": 0.361,
      "step": 481500
    },
    {
      "epoch": 9.81910039113429,
      "grad_norm": 27.73950958251953,
      "learning_rate": 9.045999022164277e-07,
      "loss": 0.3657,
      "step": 482000
    },
    {
      "epoch": 9.829286179921773,
      "grad_norm": 5.784178256988525,
      "learning_rate": 8.536709582790091e-07,
      "loss": 0.3864,
      "step": 482500
    },
    {
      "epoch": 9.839471968709256,
      "grad_norm": 8.27527141571045,
      "learning_rate": 8.027420143415907e-07,
      "loss": 0.3745,
      "step": 483000
    },
    {
      "epoch": 9.84965775749674,
      "grad_norm": 1.2614431381225586,
      "learning_rate": 7.518130704041722e-07,
      "loss": 0.3652,
      "step": 483500
    },
    {
      "epoch": 9.859843546284225,
      "grad_norm": 9.985779762268066,
      "learning_rate": 7.008841264667536e-07,
      "loss": 0.3933,
      "step": 484000
    },
    {
      "epoch": 9.870029335071708,
      "grad_norm": 7.3633131980896,
      "learning_rate": 6.499551825293351e-07,
      "loss": 0.3721,
      "step": 484500
    },
    {
      "epoch": 9.880215123859191,
      "grad_norm": 2.2133212089538574,
      "learning_rate": 5.990262385919166e-07,
      "loss": 0.3786,
      "step": 485000
    },
    {
      "epoch": 9.890400912646676,
      "grad_norm": 0.45200005173683167,
      "learning_rate": 5.48097294654498e-07,
      "loss": 0.3581,
      "step": 485500
    },
    {
      "epoch": 9.90058670143416,
      "grad_norm": 5.954257965087891,
      "learning_rate": 4.971683507170795e-07,
      "loss": 0.3954,
      "step": 486000
    },
    {
      "epoch": 9.910772490221643,
      "grad_norm": 8.919881820678711,
      "learning_rate": 4.46239406779661e-07,
      "loss": 0.3519,
      "step": 486500
    },
    {
      "epoch": 9.920958279009126,
      "grad_norm": 9.871923446655273,
      "learning_rate": 3.9531046284224254e-07,
      "loss": 0.3774,
      "step": 487000
    },
    {
      "epoch": 9.93114406779661,
      "grad_norm": 1.7874246835708618,
      "learning_rate": 3.44381518904824e-07,
      "loss": 0.3569,
      "step": 487500
    },
    {
      "epoch": 9.941329856584094,
      "grad_norm": 1.2572555541992188,
      "learning_rate": 2.9345257496740553e-07,
      "loss": 0.3899,
      "step": 488000
    },
    {
      "epoch": 9.951515645371577,
      "grad_norm": 0.9439372420310974,
      "learning_rate": 2.4252363102998694e-07,
      "loss": 0.3601,
      "step": 488500
    },
    {
      "epoch": 9.96170143415906,
      "grad_norm": 4.418605804443359,
      "learning_rate": 1.9159468709256844e-07,
      "loss": 0.3631,
      "step": 489000
    },
    {
      "epoch": 9.971887222946545,
      "grad_norm": 7.392570495605469,
      "learning_rate": 1.4066574315514993e-07,
      "loss": 0.349,
      "step": 489500
    },
    {
      "epoch": 9.982073011734029,
      "grad_norm": 13.580124855041504,
      "learning_rate": 8.973679921773142e-08,
      "loss": 0.3708,
      "step": 490000
    },
    {
      "epoch": 9.992258800521512,
      "grad_norm": 20.936054229736328,
      "learning_rate": 3.880785528031291e-08,
      "loss": 0.3744,
      "step": 490500
    },
    {
      "epoch": 10.0,
      "step": 490880,
      "total_flos": 9.167195214996173e+17,
      "train_loss": 0.42591692366220807,
      "train_runtime": 75288.4323,
      "train_samples_per_second": 52.16,
      "train_steps_per_second": 6.52
    }
  ],
  "logging_steps": 500,
  "max_steps": 490880,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9.167195214996173e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
