{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 84190,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.05938947618482005,
      "grad_norm": 8.431836128234863,
      "learning_rate": 4.970364651383775e-05,
      "loss": 0.6931,
      "step": 500
    },
    {
      "epoch": 0.1187789523696401,
      "grad_norm": 6.692655563354492,
      "learning_rate": 4.9406699132913645e-05,
      "loss": 0.6724,
      "step": 1000
    },
    {
      "epoch": 0.17816842855446016,
      "grad_norm": 6.595463275909424,
      "learning_rate": 4.910975175198955e-05,
      "loss": 0.5076,
      "step": 1500
    },
    {
      "epoch": 0.2375579047392802,
      "grad_norm": 4.204756259918213,
      "learning_rate": 4.8812804371065455e-05,
      "loss": 0.3804,
      "step": 2000
    },
    {
      "epoch": 0.2969473809241002,
      "grad_norm": 7.070993900299072,
      "learning_rate": 4.851585699014135e-05,
      "loss": 0.3487,
      "step": 2500
    },
    {
      "epoch": 0.3563368571089203,
      "grad_norm": 1.0734732151031494,
      "learning_rate": 4.821890960921725e-05,
      "loss": 0.342,
      "step": 3000
    },
    {
      "epoch": 0.41572633329374037,
      "grad_norm": 4.687109470367432,
      "learning_rate": 4.792196222829315e-05,
      "loss": 0.3324,
      "step": 3500
    },
    {
      "epoch": 0.4751158094785604,
      "grad_norm": 7.952758312225342,
      "learning_rate": 4.762501484736905e-05,
      "loss": 0.3325,
      "step": 4000
    },
    {
      "epoch": 0.5345052856633804,
      "grad_norm": 0.4618130326271057,
      "learning_rate": 4.7328067466444945e-05,
      "loss": 0.3258,
      "step": 4500
    },
    {
      "epoch": 0.5938947618482004,
      "grad_norm": 15.289559364318848,
      "learning_rate": 4.703112008552085e-05,
      "loss": 0.3078,
      "step": 5000
    },
    {
      "epoch": 0.6532842380330206,
      "grad_norm": 1.2398138046264648,
      "learning_rate": 4.673417270459675e-05,
      "loss": 0.3205,
      "step": 5500
    },
    {
      "epoch": 0.7126737142178406,
      "grad_norm": 0.5983824729919434,
      "learning_rate": 4.6437225323672646e-05,
      "loss": 0.3091,
      "step": 6000
    },
    {
      "epoch": 0.7720631904026607,
      "grad_norm": 4.497046947479248,
      "learning_rate": 4.614027794274855e-05,
      "loss": 0.2999,
      "step": 6500
    },
    {
      "epoch": 0.8314526665874807,
      "grad_norm": 2.950563430786133,
      "learning_rate": 4.584333056182445e-05,
      "loss": 0.2982,
      "step": 7000
    },
    {
      "epoch": 0.8908421427723008,
      "grad_norm": 8.703462600708008,
      "learning_rate": 4.554638318090035e-05,
      "loss": 0.2946,
      "step": 7500
    },
    {
      "epoch": 0.9502316189571208,
      "grad_norm": 5.020432472229004,
      "learning_rate": 4.5249435799976245e-05,
      "loss": 0.2681,
      "step": 8000
    },
    {
      "epoch": 1.0096210951419409,
      "grad_norm": 4.667519569396973,
      "learning_rate": 4.495248841905215e-05,
      "loss": 0.2854,
      "step": 8500
    },
    {
      "epoch": 1.0690105713267608,
      "grad_norm": 4.799442768096924,
      "learning_rate": 4.465554103812805e-05,
      "loss": 0.2903,
      "step": 9000
    },
    {
      "epoch": 1.128400047511581,
      "grad_norm": 0.7537134289741516,
      "learning_rate": 4.4358593657203946e-05,
      "loss": 0.2739,
      "step": 9500
    },
    {
      "epoch": 1.187789523696401,
      "grad_norm": 6.119437217712402,
      "learning_rate": 4.4061646276279844e-05,
      "loss": 0.2818,
      "step": 10000
    },
    {
      "epoch": 1.247178999881221,
      "grad_norm": 1.809651494026184,
      "learning_rate": 4.376469889535574e-05,
      "loss": 0.2807,
      "step": 10500
    },
    {
      "epoch": 1.3065684760660412,
      "grad_norm": 3.7088100910186768,
      "learning_rate": 4.346775151443164e-05,
      "loss": 0.2662,
      "step": 11000
    },
    {
      "epoch": 1.3659579522508611,
      "grad_norm": 1.6584469079971313,
      "learning_rate": 4.3170804133507545e-05,
      "loss": 0.257,
      "step": 11500
    },
    {
      "epoch": 1.425347428435681,
      "grad_norm": 0.4869789183139801,
      "learning_rate": 4.287385675258344e-05,
      "loss": 0.2593,
      "step": 12000
    },
    {
      "epoch": 1.4847369046205012,
      "grad_norm": 3.9636709690093994,
      "learning_rate": 4.257690937165935e-05,
      "loss": 0.2638,
      "step": 12500
    },
    {
      "epoch": 1.5441263808053214,
      "grad_norm": 8.97789478302002,
      "learning_rate": 4.2279961990735246e-05,
      "loss": 0.2718,
      "step": 13000
    },
    {
      "epoch": 1.6035158569901413,
      "grad_norm": 19.021760940551758,
      "learning_rate": 4.1983014609811144e-05,
      "loss": 0.2541,
      "step": 13500
    },
    {
      "epoch": 1.6629053331749613,
      "grad_norm": 3.1492514610290527,
      "learning_rate": 4.168606722888704e-05,
      "loss": 0.2622,
      "step": 14000
    },
    {
      "epoch": 1.7222948093597814,
      "grad_norm": 1.7482960224151611,
      "learning_rate": 4.138911984796294e-05,
      "loss": 0.2604,
      "step": 14500
    },
    {
      "epoch": 1.7816842855446016,
      "grad_norm": 0.4560535252094269,
      "learning_rate": 4.1092172467038845e-05,
      "loss": 0.2452,
      "step": 15000
    },
    {
      "epoch": 1.8410737617294215,
      "grad_norm": 14.02933120727539,
      "learning_rate": 4.079522508611474e-05,
      "loss": 0.2578,
      "step": 15500
    },
    {
      "epoch": 1.9004632379142414,
      "grad_norm": 6.77968168258667,
      "learning_rate": 4.049827770519064e-05,
      "loss": 0.2542,
      "step": 16000
    },
    {
      "epoch": 1.9598527140990618,
      "grad_norm": 12.843097686767578,
      "learning_rate": 4.020133032426654e-05,
      "loss": 0.2405,
      "step": 16500
    },
    {
      "epoch": 2.0192421902838817,
      "grad_norm": 5.041632652282715,
      "learning_rate": 3.9904382943342444e-05,
      "loss": 0.2624,
      "step": 17000
    },
    {
      "epoch": 2.0786316664687017,
      "grad_norm": 0.7623839974403381,
      "learning_rate": 3.960743556241834e-05,
      "loss": 0.2612,
      "step": 17500
    },
    {
      "epoch": 2.1380211426535216,
      "grad_norm": 17.28373908996582,
      "learning_rate": 3.931048818149424e-05,
      "loss": 0.2467,
      "step": 18000
    },
    {
      "epoch": 2.197410618838342,
      "grad_norm": 3.3179826736450195,
      "learning_rate": 3.901354080057014e-05,
      "loss": 0.2469,
      "step": 18500
    },
    {
      "epoch": 2.256800095023162,
      "grad_norm": 1.3143314123153687,
      "learning_rate": 3.8716593419646043e-05,
      "loss": 0.2394,
      "step": 19000
    },
    {
      "epoch": 2.316189571207982,
      "grad_norm": 1.2728191614151,
      "learning_rate": 3.841964603872194e-05,
      "loss": 0.2378,
      "step": 19500
    },
    {
      "epoch": 2.375579047392802,
      "grad_norm": 12.487811088562012,
      "learning_rate": 3.812269865779784e-05,
      "loss": 0.2579,
      "step": 20000
    },
    {
      "epoch": 2.434968523577622,
      "grad_norm": 0.3522071838378906,
      "learning_rate": 3.782575127687374e-05,
      "loss": 0.2646,
      "step": 20500
    },
    {
      "epoch": 2.494357999762442,
      "grad_norm": 12.184176445007324,
      "learning_rate": 3.7528803895949636e-05,
      "loss": 0.2459,
      "step": 21000
    },
    {
      "epoch": 2.553747475947262,
      "grad_norm": 14.963072776794434,
      "learning_rate": 3.7231856515025534e-05,
      "loss": 0.2332,
      "step": 21500
    },
    {
      "epoch": 2.6131369521320824,
      "grad_norm": 0.27151817083358765,
      "learning_rate": 3.693490913410144e-05,
      "loss": 0.2366,
      "step": 22000
    },
    {
      "epoch": 2.6725264283169023,
      "grad_norm": 0.2191184014081955,
      "learning_rate": 3.6637961753177343e-05,
      "loss": 0.2465,
      "step": 22500
    },
    {
      "epoch": 2.7319159045017223,
      "grad_norm": 6.518644332885742,
      "learning_rate": 3.634101437225324e-05,
      "loss": 0.2287,
      "step": 23000
    },
    {
      "epoch": 2.791305380686542,
      "grad_norm": 6.4370436668396,
      "learning_rate": 3.604406699132914e-05,
      "loss": 0.2316,
      "step": 23500
    },
    {
      "epoch": 2.850694856871362,
      "grad_norm": 5.2251739501953125,
      "learning_rate": 3.574711961040504e-05,
      "loss": 0.2382,
      "step": 24000
    },
    {
      "epoch": 2.9100843330561825,
      "grad_norm": 3.5766730308532715,
      "learning_rate": 3.5450172229480936e-05,
      "loss": 0.234,
      "step": 24500
    },
    {
      "epoch": 2.9694738092410025,
      "grad_norm": 0.640747606754303,
      "learning_rate": 3.5153224848556834e-05,
      "loss": 0.2317,
      "step": 25000
    },
    {
      "epoch": 3.0288632854258224,
      "grad_norm": 1.145448088645935,
      "learning_rate": 3.485627746763274e-05,
      "loss": 0.2334,
      "step": 25500
    },
    {
      "epoch": 3.0882527616106428,
      "grad_norm": 4.383065700531006,
      "learning_rate": 3.455933008670864e-05,
      "loss": 0.2172,
      "step": 26000
    },
    {
      "epoch": 3.1476422377954627,
      "grad_norm": 3.5083441734313965,
      "learning_rate": 3.4262382705784535e-05,
      "loss": 0.2126,
      "step": 26500
    },
    {
      "epoch": 3.2070317139802826,
      "grad_norm": 0.44270455837249756,
      "learning_rate": 3.396543532486043e-05,
      "loss": 0.2502,
      "step": 27000
    },
    {
      "epoch": 3.2664211901651026,
      "grad_norm": 3.7744762897491455,
      "learning_rate": 3.366848794393634e-05,
      "loss": 0.2364,
      "step": 27500
    },
    {
      "epoch": 3.325810666349923,
      "grad_norm": 1.5705558061599731,
      "learning_rate": 3.3371540563012236e-05,
      "loss": 0.2329,
      "step": 28000
    },
    {
      "epoch": 3.385200142534743,
      "grad_norm": 0.258644163608551,
      "learning_rate": 3.3074593182088134e-05,
      "loss": 0.2421,
      "step": 28500
    },
    {
      "epoch": 3.444589618719563,
      "grad_norm": 0.22065190970897675,
      "learning_rate": 3.277764580116404e-05,
      "loss": 0.2314,
      "step": 29000
    },
    {
      "epoch": 3.503979094904383,
      "grad_norm": 12.326627731323242,
      "learning_rate": 3.248069842023994e-05,
      "loss": 0.221,
      "step": 29500
    },
    {
      "epoch": 3.563368571089203,
      "grad_norm": 0.5406470894813538,
      "learning_rate": 3.2183751039315835e-05,
      "loss": 0.2523,
      "step": 30000
    },
    {
      "epoch": 3.622758047274023,
      "grad_norm": 7.9312520027160645,
      "learning_rate": 3.188680365839173e-05,
      "loss": 0.2126,
      "step": 30500
    },
    {
      "epoch": 3.682147523458843,
      "grad_norm": 7.907069206237793,
      "learning_rate": 3.158985627746763e-05,
      "loss": 0.2396,
      "step": 31000
    },
    {
      "epoch": 3.741536999643663,
      "grad_norm": 7.656393051147461,
      "learning_rate": 3.129290889654353e-05,
      "loss": 0.2431,
      "step": 31500
    },
    {
      "epoch": 3.8009264758284833,
      "grad_norm": 1.8363325595855713,
      "learning_rate": 3.0995961515619434e-05,
      "loss": 0.2253,
      "step": 32000
    },
    {
      "epoch": 3.8603159520133032,
      "grad_norm": 15.211860656738281,
      "learning_rate": 3.069901413469533e-05,
      "loss": 0.2215,
      "step": 32500
    },
    {
      "epoch": 3.919705428198123,
      "grad_norm": 0.5832353234291077,
      "learning_rate": 3.0402066753771237e-05,
      "loss": 0.236,
      "step": 33000
    },
    {
      "epoch": 3.9790949043829436,
      "grad_norm": 0.07257506251335144,
      "learning_rate": 3.0105119372847135e-05,
      "loss": 0.2267,
      "step": 33500
    },
    {
      "epoch": 4.0384843805677635,
      "grad_norm": 6.327512264251709,
      "learning_rate": 2.9808171991923033e-05,
      "loss": 0.2284,
      "step": 34000
    },
    {
      "epoch": 4.097873856752583,
      "grad_norm": 21.424331665039062,
      "learning_rate": 2.9511224610998934e-05,
      "loss": 0.2163,
      "step": 34500
    },
    {
      "epoch": 4.157263332937403,
      "grad_norm": 8.946359634399414,
      "learning_rate": 2.9214277230074833e-05,
      "loss": 0.2319,
      "step": 35000
    },
    {
      "epoch": 4.216652809122223,
      "grad_norm": 0.22036148607730865,
      "learning_rate": 2.891732984915073e-05,
      "loss": 0.2213,
      "step": 35500
    },
    {
      "epoch": 4.276042285307043,
      "grad_norm": 0.23708999156951904,
      "learning_rate": 2.8620382468226632e-05,
      "loss": 0.2331,
      "step": 36000
    },
    {
      "epoch": 4.335431761491864,
      "grad_norm": 3.754673480987549,
      "learning_rate": 2.832343508730253e-05,
      "loss": 0.2292,
      "step": 36500
    },
    {
      "epoch": 4.394821237676684,
      "grad_norm": 8.387911796569824,
      "learning_rate": 2.8026487706378428e-05,
      "loss": 0.209,
      "step": 37000
    },
    {
      "epoch": 4.454210713861504,
      "grad_norm": 1.4307023286819458,
      "learning_rate": 2.772954032545433e-05,
      "loss": 0.2245,
      "step": 37500
    },
    {
      "epoch": 4.513600190046324,
      "grad_norm": 0.17648956179618835,
      "learning_rate": 2.7432592944530235e-05,
      "loss": 0.2189,
      "step": 38000
    },
    {
      "epoch": 4.572989666231144,
      "grad_norm": 0.18828721344470978,
      "learning_rate": 2.7135645563606133e-05,
      "loss": 0.2174,
      "step": 38500
    },
    {
      "epoch": 4.632379142415964,
      "grad_norm": 11.336623191833496,
      "learning_rate": 2.683869818268203e-05,
      "loss": 0.231,
      "step": 39000
    },
    {
      "epoch": 4.691768618600784,
      "grad_norm": 5.140317440032959,
      "learning_rate": 2.6541750801757932e-05,
      "loss": 0.2077,
      "step": 39500
    },
    {
      "epoch": 4.751158094785604,
      "grad_norm": 16.93331527709961,
      "learning_rate": 2.624480342083383e-05,
      "loss": 0.2226,
      "step": 40000
    },
    {
      "epoch": 4.810547570970424,
      "grad_norm": 7.922807693481445,
      "learning_rate": 2.5947856039909728e-05,
      "loss": 0.2179,
      "step": 40500
    },
    {
      "epoch": 4.869937047155244,
      "grad_norm": 6.726395130157471,
      "learning_rate": 2.565090865898563e-05,
      "loss": 0.224,
      "step": 41000
    },
    {
      "epoch": 4.929326523340064,
      "grad_norm": 3.589909076690674,
      "learning_rate": 2.5353961278061528e-05,
      "loss": 0.2265,
      "step": 41500
    },
    {
      "epoch": 4.988715999524884,
      "grad_norm": 6.789613246917725,
      "learning_rate": 2.5057013897137426e-05,
      "loss": 0.2157,
      "step": 42000
    },
    {
      "epoch": 5.048105475709704,
      "grad_norm": 0.24587783217430115,
      "learning_rate": 2.4760066516213327e-05,
      "loss": 0.2196,
      "step": 42500
    },
    {
      "epoch": 5.107494951894524,
      "grad_norm": 0.24842363595962524,
      "learning_rate": 2.446311913528923e-05,
      "loss": 0.2082,
      "step": 43000
    },
    {
      "epoch": 5.166884428079344,
      "grad_norm": 4.701938629150391,
      "learning_rate": 2.4166171754365127e-05,
      "loss": 0.2185,
      "step": 43500
    },
    {
      "epoch": 5.226273904264165,
      "grad_norm": 1.8965641260147095,
      "learning_rate": 2.3869224373441025e-05,
      "loss": 0.2169,
      "step": 44000
    },
    {
      "epoch": 5.285663380448985,
      "grad_norm": 0.26931607723236084,
      "learning_rate": 2.357227699251693e-05,
      "loss": 0.2116,
      "step": 44500
    },
    {
      "epoch": 5.345052856633805,
      "grad_norm": 1.5821081399917603,
      "learning_rate": 2.3275329611592828e-05,
      "loss": 0.217,
      "step": 45000
    },
    {
      "epoch": 5.404442332818625,
      "grad_norm": 6.2921295166015625,
      "learning_rate": 2.2978382230668726e-05,
      "loss": 0.2157,
      "step": 45500
    },
    {
      "epoch": 5.463831809003445,
      "grad_norm": 0.17894455790519714,
      "learning_rate": 2.2681434849744627e-05,
      "loss": 0.234,
      "step": 46000
    },
    {
      "epoch": 5.5232212851882645,
      "grad_norm": 1.4423621892929077,
      "learning_rate": 2.2384487468820525e-05,
      "loss": 0.2035,
      "step": 46500
    },
    {
      "epoch": 5.582610761373084,
      "grad_norm": 8.691725730895996,
      "learning_rate": 2.2087540087896424e-05,
      "loss": 0.2317,
      "step": 47000
    },
    {
      "epoch": 5.642000237557904,
      "grad_norm": 0.205758199095726,
      "learning_rate": 2.1790592706972325e-05,
      "loss": 0.2032,
      "step": 47500
    },
    {
      "epoch": 5.701389713742724,
      "grad_norm": 0.269040584564209,
      "learning_rate": 2.1493645326048226e-05,
      "loss": 0.2094,
      "step": 48000
    },
    {
      "epoch": 5.760779189927545,
      "grad_norm": 5.277259349822998,
      "learning_rate": 2.1196697945124125e-05,
      "loss": 0.2056,
      "step": 48500
    },
    {
      "epoch": 5.820168666112365,
      "grad_norm": 0.2958448529243469,
      "learning_rate": 2.0899750564200023e-05,
      "loss": 0.1966,
      "step": 49000
    },
    {
      "epoch": 5.879558142297185,
      "grad_norm": 8.95850658416748,
      "learning_rate": 2.0602803183275924e-05,
      "loss": 0.2279,
      "step": 49500
    },
    {
      "epoch": 5.938947618482005,
      "grad_norm": 15.944082260131836,
      "learning_rate": 2.0305855802351826e-05,
      "loss": 0.2235,
      "step": 50000
    },
    {
      "epoch": 5.998337094666825,
      "grad_norm": 21.001781463623047,
      "learning_rate": 2.0008908421427724e-05,
      "loss": 0.2188,
      "step": 50500
    },
    {
      "epoch": 6.057726570851645,
      "grad_norm": 0.27957049012184143,
      "learning_rate": 1.9711961040503625e-05,
      "loss": 0.2151,
      "step": 51000
    },
    {
      "epoch": 6.117116047036465,
      "grad_norm": 0.30258071422576904,
      "learning_rate": 1.9415013659579523e-05,
      "loss": 0.209,
      "step": 51500
    },
    {
      "epoch": 6.1765055232212855,
      "grad_norm": 2.1397945880889893,
      "learning_rate": 1.911806627865542e-05,
      "loss": 0.2198,
      "step": 52000
    },
    {
      "epoch": 6.2358949994061055,
      "grad_norm": 5.8194499015808105,
      "learning_rate": 1.8821118897731323e-05,
      "loss": 0.1993,
      "step": 52500
    },
    {
      "epoch": 6.295284475590925,
      "grad_norm": 3.4745280742645264,
      "learning_rate": 1.8524171516807224e-05,
      "loss": 0.2187,
      "step": 53000
    },
    {
      "epoch": 6.354673951775745,
      "grad_norm": 0.5450263023376465,
      "learning_rate": 1.8227224135883122e-05,
      "loss": 0.2119,
      "step": 53500
    },
    {
      "epoch": 6.414063427960565,
      "grad_norm": 8.032036781311035,
      "learning_rate": 1.793027675495902e-05,
      "loss": 0.2072,
      "step": 54000
    },
    {
      "epoch": 6.473452904145385,
      "grad_norm": 0.1633976846933365,
      "learning_rate": 1.7633329374034922e-05,
      "loss": 0.1961,
      "step": 54500
    },
    {
      "epoch": 6.532842380330205,
      "grad_norm": 34.03715133666992,
      "learning_rate": 1.7336381993110823e-05,
      "loss": 0.1992,
      "step": 55000
    },
    {
      "epoch": 6.592231856515026,
      "grad_norm": 6.554075717926025,
      "learning_rate": 1.703943461218672e-05,
      "loss": 0.2112,
      "step": 55500
    },
    {
      "epoch": 6.651621332699846,
      "grad_norm": 0.24813838303089142,
      "learning_rate": 1.6742487231262623e-05,
      "loss": 0.2136,
      "step": 56000
    },
    {
      "epoch": 6.711010808884666,
      "grad_norm": 14.688493728637695,
      "learning_rate": 1.644553985033852e-05,
      "loss": 0.1991,
      "step": 56500
    },
    {
      "epoch": 6.770400285069486,
      "grad_norm": 0.31031376123428345,
      "learning_rate": 1.614859246941442e-05,
      "loss": 0.2137,
      "step": 57000
    },
    {
      "epoch": 6.829789761254306,
      "grad_norm": 11.24359130859375,
      "learning_rate": 1.585164508849032e-05,
      "loss": 0.2147,
      "step": 57500
    },
    {
      "epoch": 6.889179237439126,
      "grad_norm": 0.6016626954078674,
      "learning_rate": 1.5554697707566222e-05,
      "loss": 0.2144,
      "step": 58000
    },
    {
      "epoch": 6.948568713623946,
      "grad_norm": 5.249744892120361,
      "learning_rate": 1.525775032664212e-05,
      "loss": 0.2116,
      "step": 58500
    },
    {
      "epoch": 7.0079581898087655,
      "grad_norm": 12.505036354064941,
      "learning_rate": 1.496080294571802e-05,
      "loss": 0.205,
      "step": 59000
    },
    {
      "epoch": 7.067347665993586,
      "grad_norm": 0.18492178618907928,
      "learning_rate": 1.4663855564793918e-05,
      "loss": 0.214,
      "step": 59500
    },
    {
      "epoch": 7.126737142178406,
      "grad_norm": 13.44475269317627,
      "learning_rate": 1.4366908183869817e-05,
      "loss": 0.2162,
      "step": 60000
    },
    {
      "epoch": 7.186126618363226,
      "grad_norm": 0.5735265016555786,
      "learning_rate": 1.4069960802945719e-05,
      "loss": 0.2081,
      "step": 60500
    },
    {
      "epoch": 7.245516094548046,
      "grad_norm": 0.4242115318775177,
      "learning_rate": 1.3773013422021619e-05,
      "loss": 0.2034,
      "step": 61000
    },
    {
      "epoch": 7.304905570732866,
      "grad_norm": 0.14468608796596527,
      "learning_rate": 1.3476066041097518e-05,
      "loss": 0.1995,
      "step": 61500
    },
    {
      "epoch": 7.364295046917686,
      "grad_norm": 5.98443078994751,
      "learning_rate": 1.3179118660173417e-05,
      "loss": 0.2009,
      "step": 62000
    },
    {
      "epoch": 7.423684523102506,
      "grad_norm": 5.891522407531738,
      "learning_rate": 1.2882171279249316e-05,
      "loss": 0.2102,
      "step": 62500
    },
    {
      "epoch": 7.483073999287326,
      "grad_norm": 0.33786237239837646,
      "learning_rate": 1.2585223898325218e-05,
      "loss": 0.1908,
      "step": 63000
    },
    {
      "epoch": 7.542463475472147,
      "grad_norm": 4.5611419677734375,
      "learning_rate": 1.2288276517401118e-05,
      "loss": 0.2119,
      "step": 63500
    },
    {
      "epoch": 7.601852951656967,
      "grad_norm": 9.362081527709961,
      "learning_rate": 1.1991329136477017e-05,
      "loss": 0.2171,
      "step": 64000
    },
    {
      "epoch": 7.6612424278417866,
      "grad_norm": 0.6049050688743591,
      "learning_rate": 1.1694381755552915e-05,
      "loss": 0.1929,
      "step": 64500
    },
    {
      "epoch": 7.7206319040266065,
      "grad_norm": 6.576931476593018,
      "learning_rate": 1.1397434374628817e-05,
      "loss": 0.2266,
      "step": 65000
    },
    {
      "epoch": 7.780021380211426,
      "grad_norm": 1.4825910329818726,
      "learning_rate": 1.1100486993704717e-05,
      "loss": 0.2228,
      "step": 65500
    },
    {
      "epoch": 7.839410856396246,
      "grad_norm": 0.12655669450759888,
      "learning_rate": 1.0803539612780615e-05,
      "loss": 0.1965,
      "step": 66000
    },
    {
      "epoch": 7.898800332581066,
      "grad_norm": 15.4908447265625,
      "learning_rate": 1.0506592231856516e-05,
      "loss": 0.2177,
      "step": 66500
    },
    {
      "epoch": 7.958189808765887,
      "grad_norm": 0.19818857312202454,
      "learning_rate": 1.0209644850932414e-05,
      "loss": 0.2102,
      "step": 67000
    },
    {
      "epoch": 8.017579284950706,
      "grad_norm": 0.21848945319652557,
      "learning_rate": 9.912697470008316e-06,
      "loss": 0.2065,
      "step": 67500
    },
    {
      "epoch": 8.076968761135527,
      "grad_norm": 0.6445671319961548,
      "learning_rate": 9.615750089084215e-06,
      "loss": 0.2008,
      "step": 68000
    },
    {
      "epoch": 8.136358237320346,
      "grad_norm": 0.280748575925827,
      "learning_rate": 9.318802708160113e-06,
      "loss": 0.2245,
      "step": 68500
    },
    {
      "epoch": 8.195747713505167,
      "grad_norm": 24.881778717041016,
      "learning_rate": 9.021855327236015e-06,
      "loss": 0.1997,
      "step": 69000
    },
    {
      "epoch": 8.255137189689988,
      "grad_norm": 0.9619548916816711,
      "learning_rate": 8.724907946311913e-06,
      "loss": 0.1985,
      "step": 69500
    },
    {
      "epoch": 8.314526665874807,
      "grad_norm": 1.114235281944275,
      "learning_rate": 8.427960565387814e-06,
      "loss": 0.2101,
      "step": 70000
    },
    {
      "epoch": 8.373916142059628,
      "grad_norm": 9.495949745178223,
      "learning_rate": 8.131013184463714e-06,
      "loss": 0.2138,
      "step": 70500
    },
    {
      "epoch": 8.433305618244447,
      "grad_norm": 0.16730286180973053,
      "learning_rate": 7.834065803539612e-06,
      "loss": 0.203,
      "step": 71000
    },
    {
      "epoch": 8.492695094429267,
      "grad_norm": 0.15652629733085632,
      "learning_rate": 7.537118422615514e-06,
      "loss": 0.2021,
      "step": 71500
    },
    {
      "epoch": 8.552084570614086,
      "grad_norm": 2.803934335708618,
      "learning_rate": 7.240171041691413e-06,
      "loss": 0.1855,
      "step": 72000
    },
    {
      "epoch": 8.611474046798907,
      "grad_norm": 12.26259994506836,
      "learning_rate": 6.943223660767312e-06,
      "loss": 0.2399,
      "step": 72500
    },
    {
      "epoch": 8.670863522983728,
      "grad_norm": 6.406132221221924,
      "learning_rate": 6.646276279843212e-06,
      "loss": 0.2032,
      "step": 73000
    },
    {
      "epoch": 8.730252999168547,
      "grad_norm": 1.5035994052886963,
      "learning_rate": 6.349328898919112e-06,
      "loss": 0.196,
      "step": 73500
    },
    {
      "epoch": 8.789642475353368,
      "grad_norm": 1.6283535957336426,
      "learning_rate": 6.052381517995012e-06,
      "loss": 0.193,
      "step": 74000
    },
    {
      "epoch": 8.849031951538187,
      "grad_norm": 8.486122131347656,
      "learning_rate": 5.7554341370709115e-06,
      "loss": 0.2006,
      "step": 74500
    },
    {
      "epoch": 8.908421427723008,
      "grad_norm": 11.398519515991211,
      "learning_rate": 5.458486756146811e-06,
      "loss": 0.1892,
      "step": 75000
    },
    {
      "epoch": 8.967810903907827,
      "grad_norm": 3.992992639541626,
      "learning_rate": 5.16153937522271e-06,
      "loss": 0.2162,
      "step": 75500
    },
    {
      "epoch": 9.027200380092648,
      "grad_norm": 6.00981330871582,
      "learning_rate": 4.86459199429861e-06,
      "loss": 0.2052,
      "step": 76000
    },
    {
      "epoch": 9.086589856277469,
      "grad_norm": 20.110034942626953,
      "learning_rate": 4.567644613374511e-06,
      "loss": 0.183,
      "step": 76500
    },
    {
      "epoch": 9.145979332462288,
      "grad_norm": 20.564823150634766,
      "learning_rate": 4.27069723245041e-06,
      "loss": 0.2123,
      "step": 77000
    },
    {
      "epoch": 9.205368808647108,
      "grad_norm": 0.6540493965148926,
      "learning_rate": 3.973749851526309e-06,
      "loss": 0.1934,
      "step": 77500
    },
    {
      "epoch": 9.264758284831927,
      "grad_norm": 21.726987838745117,
      "learning_rate": 3.6768024706022095e-06,
      "loss": 0.1993,
      "step": 78000
    },
    {
      "epoch": 9.324147761016748,
      "grad_norm": 5.815369606018066,
      "learning_rate": 3.3798550896781092e-06,
      "loss": 0.2069,
      "step": 78500
    },
    {
      "epoch": 9.383537237201567,
      "grad_norm": 10.726638793945312,
      "learning_rate": 3.082907708754009e-06,
      "loss": 0.2171,
      "step": 79000
    },
    {
      "epoch": 9.442926713386388,
      "grad_norm": 1.782410979270935,
      "learning_rate": 2.7859603278299088e-06,
      "loss": 0.2031,
      "step": 79500
    },
    {
      "epoch": 9.502316189571207,
      "grad_norm": 12.140434265136719,
      "learning_rate": 2.4890129469058085e-06,
      "loss": 0.2029,
      "step": 80000
    },
    {
      "epoch": 9.561705665756028,
      "grad_norm": 0.08415389806032181,
      "learning_rate": 2.1920655659817083e-06,
      "loss": 0.1927,
      "step": 80500
    },
    {
      "epoch": 9.621095141940849,
      "grad_norm": 0.05518447235226631,
      "learning_rate": 1.895118185057608e-06,
      "loss": 0.1954,
      "step": 81000
    },
    {
      "epoch": 9.680484618125668,
      "grad_norm": 15.507923126220703,
      "learning_rate": 1.5981708041335076e-06,
      "loss": 0.227,
      "step": 81500
    },
    {
      "epoch": 9.739874094310489,
      "grad_norm": 0.3272992968559265,
      "learning_rate": 1.3012234232094074e-06,
      "loss": 0.2136,
      "step": 82000
    },
    {
      "epoch": 9.799263570495308,
      "grad_norm": 4.771664142608643,
      "learning_rate": 1.004276042285307e-06,
      "loss": 0.2085,
      "step": 82500
    },
    {
      "epoch": 9.858653046680129,
      "grad_norm": 0.16227562725543976,
      "learning_rate": 7.073286613612068e-07,
      "loss": 0.1841,
      "step": 83000
    },
    {
      "epoch": 9.918042522864948,
      "grad_norm": 20.46376609802246,
      "learning_rate": 4.1038128043710657e-07,
      "loss": 0.1876,
      "step": 83500
    },
    {
      "epoch": 9.977431999049768,
      "grad_norm": 6.046584129333496,
      "learning_rate": 1.134338995130063e-07,
      "loss": 0.2087,
      "step": 84000
    },
    {
      "epoch": 10.0,
      "step": 84190,
      "total_flos": 1.5721775456910336e+17,
      "train_loss": 0.23733696874692614,
      "train_runtime": 13250.8461,
      "train_samples_per_second": 50.826,
      "train_steps_per_second": 6.354
    }
  ],
  "logging_steps": 500,
  "max_steps": 84190,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5721775456910336e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
