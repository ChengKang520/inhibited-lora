{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 84190,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.05938947618482005,
      "grad_norm": 7.287806034088135,
      "learning_rate": 4.970364651383775e-05,
      "loss": 0.6676,
      "step": 500
    },
    {
      "epoch": 0.1187789523696401,
      "grad_norm": 8.574426651000977,
      "learning_rate": 4.9406699132913645e-05,
      "loss": 0.3809,
      "step": 1000
    },
    {
      "epoch": 0.17816842855446016,
      "grad_norm": 1.6672430038452148,
      "learning_rate": 4.910975175198955e-05,
      "loss": 0.3314,
      "step": 1500
    },
    {
      "epoch": 0.2375579047392802,
      "grad_norm": 6.802521228790283,
      "learning_rate": 4.8812804371065455e-05,
      "loss": 0.3189,
      "step": 2000
    },
    {
      "epoch": 0.2969473809241002,
      "grad_norm": 9.568502426147461,
      "learning_rate": 4.851585699014135e-05,
      "loss": 0.3029,
      "step": 2500
    },
    {
      "epoch": 0.3563368571089203,
      "grad_norm": 0.932904064655304,
      "learning_rate": 4.821890960921725e-05,
      "loss": 0.2884,
      "step": 3000
    },
    {
      "epoch": 0.41572633329374037,
      "grad_norm": 3.2491161823272705,
      "learning_rate": 4.792196222829315e-05,
      "loss": 0.2805,
      "step": 3500
    },
    {
      "epoch": 0.4751158094785604,
      "grad_norm": 23.432281494140625,
      "learning_rate": 4.762501484736905e-05,
      "loss": 0.2691,
      "step": 4000
    },
    {
      "epoch": 0.5345052856633804,
      "grad_norm": 0.5077170729637146,
      "learning_rate": 4.7328067466444945e-05,
      "loss": 0.273,
      "step": 4500
    },
    {
      "epoch": 0.5938947618482004,
      "grad_norm": 9.429269790649414,
      "learning_rate": 4.703112008552085e-05,
      "loss": 0.2529,
      "step": 5000
    },
    {
      "epoch": 0.6532842380330206,
      "grad_norm": 0.5786527991294861,
      "learning_rate": 4.673417270459675e-05,
      "loss": 0.2746,
      "step": 5500
    },
    {
      "epoch": 0.7126737142178406,
      "grad_norm": 1.3329490423202515,
      "learning_rate": 4.6437225323672646e-05,
      "loss": 0.2659,
      "step": 6000
    },
    {
      "epoch": 0.7720631904026607,
      "grad_norm": 5.648776054382324,
      "learning_rate": 4.614027794274855e-05,
      "loss": 0.2532,
      "step": 6500
    },
    {
      "epoch": 0.8314526665874807,
      "grad_norm": 3.606933116912842,
      "learning_rate": 4.584333056182445e-05,
      "loss": 0.266,
      "step": 7000
    },
    {
      "epoch": 0.8908421427723008,
      "grad_norm": 15.413904190063477,
      "learning_rate": 4.554638318090035e-05,
      "loss": 0.2568,
      "step": 7500
    },
    {
      "epoch": 0.9502316189571208,
      "grad_norm": 17.996423721313477,
      "learning_rate": 4.5249435799976245e-05,
      "loss": 0.2285,
      "step": 8000
    },
    {
      "epoch": 1.0096210951419409,
      "grad_norm": 12.034502983093262,
      "learning_rate": 4.495248841905215e-05,
      "loss": 0.2459,
      "step": 8500
    },
    {
      "epoch": 1.0690105713267608,
      "grad_norm": 6.109315395355225,
      "learning_rate": 4.465554103812805e-05,
      "loss": 0.2493,
      "step": 9000
    },
    {
      "epoch": 1.128400047511581,
      "grad_norm": 0.3470696806907654,
      "learning_rate": 4.4358593657203946e-05,
      "loss": 0.2332,
      "step": 9500
    },
    {
      "epoch": 1.187789523696401,
      "grad_norm": 6.139891147613525,
      "learning_rate": 4.4061646276279844e-05,
      "loss": 0.2323,
      "step": 10000
    },
    {
      "epoch": 1.247178999881221,
      "grad_norm": 3.2600810527801514,
      "learning_rate": 4.376469889535574e-05,
      "loss": 0.2488,
      "step": 10500
    },
    {
      "epoch": 1.3065684760660412,
      "grad_norm": 1.313178300857544,
      "learning_rate": 4.346775151443164e-05,
      "loss": 0.2309,
      "step": 11000
    },
    {
      "epoch": 1.3659579522508611,
      "grad_norm": 2.178807258605957,
      "learning_rate": 4.3170804133507545e-05,
      "loss": 0.221,
      "step": 11500
    },
    {
      "epoch": 1.425347428435681,
      "grad_norm": 0.2539654076099396,
      "learning_rate": 4.287385675258344e-05,
      "loss": 0.2229,
      "step": 12000
    },
    {
      "epoch": 1.4847369046205012,
      "grad_norm": 0.8768194913864136,
      "learning_rate": 4.257690937165935e-05,
      "loss": 0.2266,
      "step": 12500
    },
    {
      "epoch": 1.5441263808053214,
      "grad_norm": 9.578116416931152,
      "learning_rate": 4.2279961990735246e-05,
      "loss": 0.2426,
      "step": 13000
    },
    {
      "epoch": 1.6035158569901413,
      "grad_norm": 19.4862003326416,
      "learning_rate": 4.1983014609811144e-05,
      "loss": 0.2217,
      "step": 13500
    },
    {
      "epoch": 1.6629053331749613,
      "grad_norm": 0.4957877993583679,
      "learning_rate": 4.168606722888704e-05,
      "loss": 0.2334,
      "step": 14000
    },
    {
      "epoch": 1.7222948093597814,
      "grad_norm": 1.0118826627731323,
      "learning_rate": 4.138911984796294e-05,
      "loss": 0.2239,
      "step": 14500
    },
    {
      "epoch": 1.7816842855446016,
      "grad_norm": 0.26292684674263,
      "learning_rate": 4.1092172467038845e-05,
      "loss": 0.2245,
      "step": 15000
    },
    {
      "epoch": 1.8410737617294215,
      "grad_norm": 17.600360870361328,
      "learning_rate": 4.079522508611474e-05,
      "loss": 0.2371,
      "step": 15500
    },
    {
      "epoch": 1.9004632379142414,
      "grad_norm": 8.73851490020752,
      "learning_rate": 4.049827770519064e-05,
      "loss": 0.2342,
      "step": 16000
    },
    {
      "epoch": 1.9598527140990618,
      "grad_norm": 16.444244384765625,
      "learning_rate": 4.020133032426654e-05,
      "loss": 0.2165,
      "step": 16500
    },
    {
      "epoch": 2.0192421902838817,
      "grad_norm": 0.20335277915000916,
      "learning_rate": 3.9904382943342444e-05,
      "loss": 0.2256,
      "step": 17000
    },
    {
      "epoch": 2.0786316664687017,
      "grad_norm": 0.1509954333305359,
      "learning_rate": 3.960743556241834e-05,
      "loss": 0.2254,
      "step": 17500
    },
    {
      "epoch": 2.1380211426535216,
      "grad_norm": 4.289148330688477,
      "learning_rate": 3.931048818149424e-05,
      "loss": 0.2101,
      "step": 18000
    },
    {
      "epoch": 2.197410618838342,
      "grad_norm": 0.29085052013397217,
      "learning_rate": 3.901354080057014e-05,
      "loss": 0.2157,
      "step": 18500
    },
    {
      "epoch": 2.256800095023162,
      "grad_norm": 0.4177958071231842,
      "learning_rate": 3.8716593419646043e-05,
      "loss": 0.2019,
      "step": 19000
    },
    {
      "epoch": 2.316189571207982,
      "grad_norm": 11.262699127197266,
      "learning_rate": 3.841964603872194e-05,
      "loss": 0.1941,
      "step": 19500
    },
    {
      "epoch": 2.375579047392802,
      "grad_norm": 14.530861854553223,
      "learning_rate": 3.812269865779784e-05,
      "loss": 0.2089,
      "step": 20000
    },
    {
      "epoch": 2.434968523577622,
      "grad_norm": 0.2841944992542267,
      "learning_rate": 3.782575127687374e-05,
      "loss": 0.2225,
      "step": 20500
    },
    {
      "epoch": 2.494357999762442,
      "grad_norm": 10.00597095489502,
      "learning_rate": 3.7528803895949636e-05,
      "loss": 0.2103,
      "step": 21000
    },
    {
      "epoch": 2.553747475947262,
      "grad_norm": 0.3020334243774414,
      "learning_rate": 3.7231856515025534e-05,
      "loss": 0.2028,
      "step": 21500
    },
    {
      "epoch": 2.6131369521320824,
      "grad_norm": 0.08289043605327606,
      "learning_rate": 3.693490913410144e-05,
      "loss": 0.196,
      "step": 22000
    },
    {
      "epoch": 2.6725264283169023,
      "grad_norm": 0.2559968829154968,
      "learning_rate": 3.6637961753177343e-05,
      "loss": 0.22,
      "step": 22500
    },
    {
      "epoch": 2.7319159045017223,
      "grad_norm": 15.132568359375,
      "learning_rate": 3.634101437225324e-05,
      "loss": 0.1873,
      "step": 23000
    },
    {
      "epoch": 2.791305380686542,
      "grad_norm": 0.8474793434143066,
      "learning_rate": 3.604406699132914e-05,
      "loss": 0.1993,
      "step": 23500
    },
    {
      "epoch": 2.850694856871362,
      "grad_norm": 12.992345809936523,
      "learning_rate": 3.574711961040504e-05,
      "loss": 0.1935,
      "step": 24000
    },
    {
      "epoch": 2.9100843330561825,
      "grad_norm": 0.19529171288013458,
      "learning_rate": 3.5450172229480936e-05,
      "loss": 0.1975,
      "step": 24500
    },
    {
      "epoch": 2.9694738092410025,
      "grad_norm": 0.31805846095085144,
      "learning_rate": 3.5153224848556834e-05,
      "loss": 0.2003,
      "step": 25000
    },
    {
      "epoch": 3.0288632854258224,
      "grad_norm": 0.7422872185707092,
      "learning_rate": 3.485627746763274e-05,
      "loss": 0.207,
      "step": 25500
    },
    {
      "epoch": 3.0882527616106428,
      "grad_norm": 6.920568943023682,
      "learning_rate": 3.455933008670864e-05,
      "loss": 0.1702,
      "step": 26000
    },
    {
      "epoch": 3.1476422377954627,
      "grad_norm": 36.5086784362793,
      "learning_rate": 3.4262382705784535e-05,
      "loss": 0.1753,
      "step": 26500
    },
    {
      "epoch": 3.2070317139802826,
      "grad_norm": 1.777907371520996,
      "learning_rate": 3.396543532486043e-05,
      "loss": 0.1967,
      "step": 27000
    },
    {
      "epoch": 3.2664211901651026,
      "grad_norm": 7.347896575927734,
      "learning_rate": 3.366848794393634e-05,
      "loss": 0.1917,
      "step": 27500
    },
    {
      "epoch": 3.325810666349923,
      "grad_norm": 2.70468807220459,
      "learning_rate": 3.3371540563012236e-05,
      "loss": 0.1859,
      "step": 28000
    },
    {
      "epoch": 3.385200142534743,
      "grad_norm": 0.17847490310668945,
      "learning_rate": 3.3074593182088134e-05,
      "loss": 0.1935,
      "step": 28500
    },
    {
      "epoch": 3.444589618719563,
      "grad_norm": 0.06331944465637207,
      "learning_rate": 3.277764580116404e-05,
      "loss": 0.1875,
      "step": 29000
    },
    {
      "epoch": 3.503979094904383,
      "grad_norm": 0.4155776798725128,
      "learning_rate": 3.248069842023994e-05,
      "loss": 0.2012,
      "step": 29500
    },
    {
      "epoch": 3.563368571089203,
      "grad_norm": 0.2872049808502197,
      "learning_rate": 3.2183751039315835e-05,
      "loss": 0.2005,
      "step": 30000
    },
    {
      "epoch": 3.622758047274023,
      "grad_norm": 1.3229783773422241,
      "learning_rate": 3.188680365839173e-05,
      "loss": 0.1887,
      "step": 30500
    },
    {
      "epoch": 3.682147523458843,
      "grad_norm": 9.590188980102539,
      "learning_rate": 3.158985627746763e-05,
      "loss": 0.2004,
      "step": 31000
    },
    {
      "epoch": 3.741536999643663,
      "grad_norm": 0.46546074748039246,
      "learning_rate": 3.129290889654353e-05,
      "loss": 0.1963,
      "step": 31500
    },
    {
      "epoch": 3.8009264758284833,
      "grad_norm": 0.39821892976760864,
      "learning_rate": 3.0995961515619434e-05,
      "loss": 0.2013,
      "step": 32000
    },
    {
      "epoch": 3.8603159520133032,
      "grad_norm": 0.7200143337249756,
      "learning_rate": 3.069901413469533e-05,
      "loss": 0.1775,
      "step": 32500
    },
    {
      "epoch": 3.919705428198123,
      "grad_norm": 0.3615873157978058,
      "learning_rate": 3.0402066753771237e-05,
      "loss": 0.1878,
      "step": 33000
    },
    {
      "epoch": 3.9790949043829436,
      "grad_norm": 0.023251714184880257,
      "learning_rate": 3.0105119372847135e-05,
      "loss": 0.1862,
      "step": 33500
    },
    {
      "epoch": 4.0384843805677635,
      "grad_norm": 21.640024185180664,
      "learning_rate": 2.9808171991923033e-05,
      "loss": 0.1906,
      "step": 34000
    },
    {
      "epoch": 4.097873856752583,
      "grad_norm": 7.005900859832764,
      "learning_rate": 2.9511224610998934e-05,
      "loss": 0.1707,
      "step": 34500
    },
    {
      "epoch": 4.157263332937403,
      "grad_norm": 11.863363265991211,
      "learning_rate": 2.9214277230074833e-05,
      "loss": 0.1732,
      "step": 35000
    },
    {
      "epoch": 4.216652809122223,
      "grad_norm": 0.15678583085536957,
      "learning_rate": 2.891732984915073e-05,
      "loss": 0.1731,
      "step": 35500
    },
    {
      "epoch": 4.276042285307043,
      "grad_norm": 0.1746947169303894,
      "learning_rate": 2.8620382468226632e-05,
      "loss": 0.1791,
      "step": 36000
    },
    {
      "epoch": 4.335431761491864,
      "grad_norm": 5.523243427276611,
      "learning_rate": 2.832343508730253e-05,
      "loss": 0.1795,
      "step": 36500
    },
    {
      "epoch": 4.394821237676684,
      "grad_norm": 3.809544324874878,
      "learning_rate": 2.8026487706378428e-05,
      "loss": 0.1685,
      "step": 37000
    },
    {
      "epoch": 4.454210713861504,
      "grad_norm": 20.299434661865234,
      "learning_rate": 2.772954032545433e-05,
      "loss": 0.1851,
      "step": 37500
    },
    {
      "epoch": 4.513600190046324,
      "grad_norm": 0.05150548741221428,
      "learning_rate": 2.7432592944530235e-05,
      "loss": 0.1705,
      "step": 38000
    },
    {
      "epoch": 4.572989666231144,
      "grad_norm": 0.14746969938278198,
      "learning_rate": 2.7135645563606133e-05,
      "loss": 0.1562,
      "step": 38500
    },
    {
      "epoch": 4.632379142415964,
      "grad_norm": 10.920620918273926,
      "learning_rate": 2.683869818268203e-05,
      "loss": 0.1816,
      "step": 39000
    },
    {
      "epoch": 4.691768618600784,
      "grad_norm": 8.867208480834961,
      "learning_rate": 2.6541750801757932e-05,
      "loss": 0.1661,
      "step": 39500
    },
    {
      "epoch": 4.751158094785604,
      "grad_norm": 17.39511489868164,
      "learning_rate": 2.624480342083383e-05,
      "loss": 0.168,
      "step": 40000
    },
    {
      "epoch": 4.810547570970424,
      "grad_norm": 8.45566463470459,
      "learning_rate": 2.5947856039909728e-05,
      "loss": 0.1705,
      "step": 40500
    },
    {
      "epoch": 4.869937047155244,
      "grad_norm": 19.699909210205078,
      "learning_rate": 2.565090865898563e-05,
      "loss": 0.1754,
      "step": 41000
    },
    {
      "epoch": 4.929326523340064,
      "grad_norm": 0.3658403754234314,
      "learning_rate": 2.5353961278061528e-05,
      "loss": 0.1769,
      "step": 41500
    },
    {
      "epoch": 4.988715999524884,
      "grad_norm": 7.371465682983398,
      "learning_rate": 2.5057013897137426e-05,
      "loss": 0.174,
      "step": 42000
    },
    {
      "epoch": 5.048105475709704,
      "grad_norm": 0.14874222874641418,
      "learning_rate": 2.4760066516213327e-05,
      "loss": 0.167,
      "step": 42500
    },
    {
      "epoch": 5.107494951894524,
      "grad_norm": 0.2752915024757385,
      "learning_rate": 2.446311913528923e-05,
      "loss": 0.1639,
      "step": 43000
    },
    {
      "epoch": 5.166884428079344,
      "grad_norm": 6.349766254425049,
      "learning_rate": 2.4166171754365127e-05,
      "loss": 0.1759,
      "step": 43500
    },
    {
      "epoch": 5.226273904264165,
      "grad_norm": 0.06188015267252922,
      "learning_rate": 2.3869224373441025e-05,
      "loss": 0.1735,
      "step": 44000
    },
    {
      "epoch": 5.285663380448985,
      "grad_norm": 0.35817545652389526,
      "learning_rate": 2.357227699251693e-05,
      "loss": 0.164,
      "step": 44500
    },
    {
      "epoch": 5.345052856633805,
      "grad_norm": 0.15398411452770233,
      "learning_rate": 2.3275329611592828e-05,
      "loss": 0.1774,
      "step": 45000
    },
    {
      "epoch": 5.404442332818625,
      "grad_norm": 8.791378021240234,
      "learning_rate": 2.2978382230668726e-05,
      "loss": 0.1642,
      "step": 45500
    },
    {
      "epoch": 5.463831809003445,
      "grad_norm": 0.2572952210903168,
      "learning_rate": 2.2681434849744627e-05,
      "loss": 0.1747,
      "step": 46000
    },
    {
      "epoch": 5.5232212851882645,
      "grad_norm": 1.76666259765625,
      "learning_rate": 2.2384487468820525e-05,
      "loss": 0.1566,
      "step": 46500
    },
    {
      "epoch": 5.582610761373084,
      "grad_norm": 0.48929697275161743,
      "learning_rate": 2.2087540087896424e-05,
      "loss": 0.1735,
      "step": 47000
    },
    {
      "epoch": 5.642000237557904,
      "grad_norm": 0.19854962825775146,
      "learning_rate": 2.1790592706972325e-05,
      "loss": 0.1629,
      "step": 47500
    },
    {
      "epoch": 5.701389713742724,
      "grad_norm": 0.06879042834043503,
      "learning_rate": 2.1493645326048226e-05,
      "loss": 0.1611,
      "step": 48000
    },
    {
      "epoch": 5.760779189927545,
      "grad_norm": 5.205249786376953,
      "learning_rate": 2.1196697945124125e-05,
      "loss": 0.1598,
      "step": 48500
    },
    {
      "epoch": 5.820168666112365,
      "grad_norm": 0.9200173616409302,
      "learning_rate": 2.0899750564200023e-05,
      "loss": 0.1624,
      "step": 49000
    },
    {
      "epoch": 5.879558142297185,
      "grad_norm": 9.5564603805542,
      "learning_rate": 2.0602803183275924e-05,
      "loss": 0.1616,
      "step": 49500
    },
    {
      "epoch": 5.938947618482005,
      "grad_norm": 2.060730218887329,
      "learning_rate": 2.0305855802351826e-05,
      "loss": 0.1803,
      "step": 50000
    },
    {
      "epoch": 5.998337094666825,
      "grad_norm": 24.03533935546875,
      "learning_rate": 2.0008908421427724e-05,
      "loss": 0.1621,
      "step": 50500
    },
    {
      "epoch": 6.057726570851645,
      "grad_norm": 0.1425561159849167,
      "learning_rate": 1.9711961040503625e-05,
      "loss": 0.1765,
      "step": 51000
    },
    {
      "epoch": 6.117116047036465,
      "grad_norm": 0.14246664941310883,
      "learning_rate": 1.9415013659579523e-05,
      "loss": 0.1568,
      "step": 51500
    },
    {
      "epoch": 6.1765055232212855,
      "grad_norm": 3.3458850383758545,
      "learning_rate": 1.911806627865542e-05,
      "loss": 0.1521,
      "step": 52000
    },
    {
      "epoch": 6.2358949994061055,
      "grad_norm": 0.65163254737854,
      "learning_rate": 1.8821118897731323e-05,
      "loss": 0.144,
      "step": 52500
    },
    {
      "epoch": 6.295284475590925,
      "grad_norm": 7.578251361846924,
      "learning_rate": 1.8524171516807224e-05,
      "loss": 0.1671,
      "step": 53000
    },
    {
      "epoch": 6.354673951775745,
      "grad_norm": 0.39779746532440186,
      "learning_rate": 1.8227224135883122e-05,
      "loss": 0.1406,
      "step": 53500
    },
    {
      "epoch": 6.414063427960565,
      "grad_norm": 6.933846950531006,
      "learning_rate": 1.793027675495902e-05,
      "loss": 0.1603,
      "step": 54000
    },
    {
      "epoch": 6.473452904145385,
      "grad_norm": 0.06135296821594238,
      "learning_rate": 1.7633329374034922e-05,
      "loss": 0.1392,
      "step": 54500
    },
    {
      "epoch": 6.532842380330205,
      "grad_norm": 2.8789148330688477,
      "learning_rate": 1.7336381993110823e-05,
      "loss": 0.1431,
      "step": 55000
    },
    {
      "epoch": 6.592231856515026,
      "grad_norm": 5.881856918334961,
      "learning_rate": 1.703943461218672e-05,
      "loss": 0.1644,
      "step": 55500
    },
    {
      "epoch": 6.651621332699846,
      "grad_norm": 0.14209546148777008,
      "learning_rate": 1.6742487231262623e-05,
      "loss": 0.16,
      "step": 56000
    },
    {
      "epoch": 6.711010808884666,
      "grad_norm": 0.3156222403049469,
      "learning_rate": 1.644553985033852e-05,
      "loss": 0.1585,
      "step": 56500
    },
    {
      "epoch": 6.770400285069486,
      "grad_norm": 0.0966719388961792,
      "learning_rate": 1.614859246941442e-05,
      "loss": 0.1519,
      "step": 57000
    },
    {
      "epoch": 6.829789761254306,
      "grad_norm": 0.19453974068164825,
      "learning_rate": 1.585164508849032e-05,
      "loss": 0.1631,
      "step": 57500
    },
    {
      "epoch": 6.889179237439126,
      "grad_norm": 0.26236557960510254,
      "learning_rate": 1.5554697707566222e-05,
      "loss": 0.1596,
      "step": 58000
    },
    {
      "epoch": 6.948568713623946,
      "grad_norm": 8.191447257995605,
      "learning_rate": 1.525775032664212e-05,
      "loss": 0.1614,
      "step": 58500
    },
    {
      "epoch": 7.0079581898087655,
      "grad_norm": 0.7675635814666748,
      "learning_rate": 1.496080294571802e-05,
      "loss": 0.1555,
      "step": 59000
    },
    {
      "epoch": 7.067347665993586,
      "grad_norm": 0.11633681505918503,
      "learning_rate": 1.4663855564793918e-05,
      "loss": 0.1515,
      "step": 59500
    },
    {
      "epoch": 7.126737142178406,
      "grad_norm": 9.136467933654785,
      "learning_rate": 1.4366908183869817e-05,
      "loss": 0.1604,
      "step": 60000
    },
    {
      "epoch": 7.186126618363226,
      "grad_norm": 0.2117263525724411,
      "learning_rate": 1.4069960802945719e-05,
      "loss": 0.1411,
      "step": 60500
    },
    {
      "epoch": 7.245516094548046,
      "grad_norm": 0.08998356759548187,
      "learning_rate": 1.3773013422021619e-05,
      "loss": 0.1571,
      "step": 61000
    },
    {
      "epoch": 7.304905570732866,
      "grad_norm": 0.15407980978488922,
      "learning_rate": 1.3476066041097518e-05,
      "loss": 0.1485,
      "step": 61500
    },
    {
      "epoch": 7.364295046917686,
      "grad_norm": 0.15005910396575928,
      "learning_rate": 1.3179118660173417e-05,
      "loss": 0.1342,
      "step": 62000
    },
    {
      "epoch": 7.423684523102506,
      "grad_norm": 31.839906692504883,
      "learning_rate": 1.2882171279249316e-05,
      "loss": 0.148,
      "step": 62500
    },
    {
      "epoch": 7.483073999287326,
      "grad_norm": 0.0800689235329628,
      "learning_rate": 1.2585223898325218e-05,
      "loss": 0.1364,
      "step": 63000
    },
    {
      "epoch": 7.542463475472147,
      "grad_norm": 0.5021679401397705,
      "learning_rate": 1.2288276517401118e-05,
      "loss": 0.1498,
      "step": 63500
    },
    {
      "epoch": 7.601852951656967,
      "grad_norm": 0.6583542227745056,
      "learning_rate": 1.1991329136477017e-05,
      "loss": 0.156,
      "step": 64000
    },
    {
      "epoch": 7.6612424278417866,
      "grad_norm": 0.14778991043567657,
      "learning_rate": 1.1694381755552915e-05,
      "loss": 0.1309,
      "step": 64500
    },
    {
      "epoch": 7.7206319040266065,
      "grad_norm": 8.751687049865723,
      "learning_rate": 1.1397434374628817e-05,
      "loss": 0.1652,
      "step": 65000
    },
    {
      "epoch": 7.780021380211426,
      "grad_norm": 0.3377176821231842,
      "learning_rate": 1.1100486993704717e-05,
      "loss": 0.163,
      "step": 65500
    },
    {
      "epoch": 7.839410856396246,
      "grad_norm": 0.0935937687754631,
      "learning_rate": 1.0803539612780615e-05,
      "loss": 0.1423,
      "step": 66000
    },
    {
      "epoch": 7.898800332581066,
      "grad_norm": 0.3736500144004822,
      "learning_rate": 1.0506592231856516e-05,
      "loss": 0.1516,
      "step": 66500
    },
    {
      "epoch": 7.958189808765887,
      "grad_norm": 0.07015950232744217,
      "learning_rate": 1.0209644850932414e-05,
      "loss": 0.1574,
      "step": 67000
    },
    {
      "epoch": 8.017579284950706,
      "grad_norm": 0.07617408782243729,
      "learning_rate": 9.912697470008316e-06,
      "loss": 0.148,
      "step": 67500
    },
    {
      "epoch": 8.076968761135527,
      "grad_norm": 0.1838974803686142,
      "learning_rate": 9.615750089084215e-06,
      "loss": 0.1606,
      "step": 68000
    },
    {
      "epoch": 8.136358237320346,
      "grad_norm": 0.15102285146713257,
      "learning_rate": 9.318802708160113e-06,
      "loss": 0.1573,
      "step": 68500
    },
    {
      "epoch": 8.195747713505167,
      "grad_norm": 3.79799222946167,
      "learning_rate": 9.021855327236015e-06,
      "loss": 0.1408,
      "step": 69000
    },
    {
      "epoch": 8.255137189689988,
      "grad_norm": 0.3489106297492981,
      "learning_rate": 8.724907946311913e-06,
      "loss": 0.1259,
      "step": 69500
    },
    {
      "epoch": 8.314526665874807,
      "grad_norm": 0.24987433850765228,
      "learning_rate": 8.427960565387814e-06,
      "loss": 0.1516,
      "step": 70000
    },
    {
      "epoch": 8.373916142059628,
      "grad_norm": 2.5470986366271973,
      "learning_rate": 8.131013184463714e-06,
      "loss": 0.1571,
      "step": 70500
    },
    {
      "epoch": 8.433305618244447,
      "grad_norm": 0.11550384759902954,
      "learning_rate": 7.834065803539612e-06,
      "loss": 0.1446,
      "step": 71000
    },
    {
      "epoch": 8.492695094429267,
      "grad_norm": 0.0631740465760231,
      "learning_rate": 7.537118422615514e-06,
      "loss": 0.1355,
      "step": 71500
    },
    {
      "epoch": 8.552084570614086,
      "grad_norm": 24.123151779174805,
      "learning_rate": 7.240171041691413e-06,
      "loss": 0.1283,
      "step": 72000
    },
    {
      "epoch": 8.611474046798907,
      "grad_norm": 7.646299362182617,
      "learning_rate": 6.943223660767312e-06,
      "loss": 0.1686,
      "step": 72500
    },
    {
      "epoch": 8.670863522983728,
      "grad_norm": 0.2185475379228592,
      "learning_rate": 6.646276279843212e-06,
      "loss": 0.157,
      "step": 73000
    },
    {
      "epoch": 8.730252999168547,
      "grad_norm": 0.22499719262123108,
      "learning_rate": 6.349328898919112e-06,
      "loss": 0.1258,
      "step": 73500
    },
    {
      "epoch": 8.789642475353368,
      "grad_norm": 0.2029040902853012,
      "learning_rate": 6.052381517995012e-06,
      "loss": 0.1366,
      "step": 74000
    },
    {
      "epoch": 8.849031951538187,
      "grad_norm": 13.430375099182129,
      "learning_rate": 5.7554341370709115e-06,
      "loss": 0.1456,
      "step": 74500
    },
    {
      "epoch": 8.908421427723008,
      "grad_norm": 0.8442843556404114,
      "learning_rate": 5.458486756146811e-06,
      "loss": 0.1254,
      "step": 75000
    },
    {
      "epoch": 8.967810903907827,
      "grad_norm": 35.155921936035156,
      "learning_rate": 5.16153937522271e-06,
      "loss": 0.148,
      "step": 75500
    },
    {
      "epoch": 9.027200380092648,
      "grad_norm": 0.17909201979637146,
      "learning_rate": 4.86459199429861e-06,
      "loss": 0.1405,
      "step": 76000
    },
    {
      "epoch": 9.086589856277469,
      "grad_norm": 1.693151593208313,
      "learning_rate": 4.567644613374511e-06,
      "loss": 0.1188,
      "step": 76500
    },
    {
      "epoch": 9.145979332462288,
      "grad_norm": 7.934651851654053,
      "learning_rate": 4.27069723245041e-06,
      "loss": 0.1415,
      "step": 77000
    },
    {
      "epoch": 9.205368808647108,
      "grad_norm": 0.07342024147510529,
      "learning_rate": 3.973749851526309e-06,
      "loss": 0.1238,
      "step": 77500
    },
    {
      "epoch": 9.264758284831927,
      "grad_norm": 1.149187684059143,
      "learning_rate": 3.6768024706022095e-06,
      "loss": 0.1342,
      "step": 78000
    },
    {
      "epoch": 9.324147761016748,
      "grad_norm": 12.540670394897461,
      "learning_rate": 3.3798550896781092e-06,
      "loss": 0.1352,
      "step": 78500
    },
    {
      "epoch": 9.383537237201567,
      "grad_norm": 0.27895280718803406,
      "learning_rate": 3.082907708754009e-06,
      "loss": 0.1554,
      "step": 79000
    },
    {
      "epoch": 9.442926713386388,
      "grad_norm": 0.09460175782442093,
      "learning_rate": 2.7859603278299088e-06,
      "loss": 0.1403,
      "step": 79500
    },
    {
      "epoch": 9.502316189571207,
      "grad_norm": 0.5037944912910461,
      "learning_rate": 2.4890129469058085e-06,
      "loss": 0.1322,
      "step": 80000
    },
    {
      "epoch": 9.561705665756028,
      "grad_norm": 0.03287679702043533,
      "learning_rate": 2.1920655659817083e-06,
      "loss": 0.1307,
      "step": 80500
    },
    {
      "epoch": 9.621095141940849,
      "grad_norm": 0.09635494649410248,
      "learning_rate": 1.895118185057608e-06,
      "loss": 0.1381,
      "step": 81000
    },
    {
      "epoch": 9.680484618125668,
      "grad_norm": 9.306863784790039,
      "learning_rate": 1.5981708041335076e-06,
      "loss": 0.1584,
      "step": 81500
    },
    {
      "epoch": 9.739874094310489,
      "grad_norm": 0.14398419857025146,
      "learning_rate": 1.3012234232094074e-06,
      "loss": 0.1615,
      "step": 82000
    },
    {
      "epoch": 9.799263570495308,
      "grad_norm": 0.2234906554222107,
      "learning_rate": 1.004276042285307e-06,
      "loss": 0.1366,
      "step": 82500
    },
    {
      "epoch": 9.858653046680129,
      "grad_norm": 5.227334976196289,
      "learning_rate": 7.073286613612068e-07,
      "loss": 0.114,
      "step": 83000
    },
    {
      "epoch": 9.918042522864948,
      "grad_norm": 0.14224953949451447,
      "learning_rate": 4.1038128043710657e-07,
      "loss": 0.1274,
      "step": 83500
    },
    {
      "epoch": 9.977431999049768,
      "grad_norm": 8.835307121276855,
      "learning_rate": 1.134338995130063e-07,
      "loss": 0.1441,
      "step": 84000
    },
    {
      "epoch": 10.0,
      "step": 84190,
      "total_flos": 1.5721775456910336e+17,
      "train_loss": 0.18597868824107075,
      "train_runtime": 13258.3704,
      "train_samples_per_second": 50.797,
      "train_steps_per_second": 6.35
    }
  ],
  "logging_steps": 500,
  "max_steps": 84190,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5721775456910336e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
