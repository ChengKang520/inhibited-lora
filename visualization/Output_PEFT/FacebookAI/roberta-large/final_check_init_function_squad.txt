Module                                                                                                 Parameters  dtype
---------------------------------------------------------------------------------------------------  ------------  -------------
base_model.model.roberta.embeddings.LayerNorm.modules_to_save.default.weight                                1_024  torch.float16
base_model.model.roberta.embeddings.LayerNorm.modules_to_save.default.bias                                  1_024  torch.float16
base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.0.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.0.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.0.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.0.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.1.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.1.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.1.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.1.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.2.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.2.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.2.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.2.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.3.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.3.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.3.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.3.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.4.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.4.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.4.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.4.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.5.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.5.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.5.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.5.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.6.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.6.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.6.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.6.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.7.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.7.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.7.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.7.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.8.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.8.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.8.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.8.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.9.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.9.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.9.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.9.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.10.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.10.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.10.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.10.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.11.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.11.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.11.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.11.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.12.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.12.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.12.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.12.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.12.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.12.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.12.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.12.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.12.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.12.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.13.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.13.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.13.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.13.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.13.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.13.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.13.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.13.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.13.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.13.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.14.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.14.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.14.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.14.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.14.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.14.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.14.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.14.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.14.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.14.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.15.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.15.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.15.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.15.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.15.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.15.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.15.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.15.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.15.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.15.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.16.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.16.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.16.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.16.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.16.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.16.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.16.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.16.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.16.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.16.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.17.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.17.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.17.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.17.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.17.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.17.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.17.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.17.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.17.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.17.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.18.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.18.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.18.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.18.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.18.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.18.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.18.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.18.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.18.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.18.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.19.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.19.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.19.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.19.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.19.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.19.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.19.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.19.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.19.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.19.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.20.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.20.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.20.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.20.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.20.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.20.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.20.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.20.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.20.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.20.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.21.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.21.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.21.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.21.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.21.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.21.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.21.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.21.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.21.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.21.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.22.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.22.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.22.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.22.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.22.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.22.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.22.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.22.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.22.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.22.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.23.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.23.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.23.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.23.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.23.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.23.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.23.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.23.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.23.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.23.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.qa_outputs.modules_to_save.default.weight                                                  2_048  torch.float16
base_model.model.qa_outputs.modules_to_save.default.bias                                                        2  torch.float16
---------------------------------------------------------------------------------------------------  ------------  -------------
TOTAL                                                                                                   1_282_050