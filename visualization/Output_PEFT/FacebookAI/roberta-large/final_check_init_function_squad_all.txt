Module                                                                                                 Parameters  dtype
---------------------------------------------------------------------------------------------------  ------------  -------------
base_model.model.roberta.embeddings.word_embeddings.weight                                             51_471_360  torch.float16
base_model.model.roberta.embeddings.position_embeddings.weight                                            526_336  torch.float16
base_model.model.roberta.embeddings.token_type_embeddings.weight                                            1_024  torch.float16
base_model.model.roberta.embeddings.LayerNorm.original_module.weight                                        1_024  torch.float16
base_model.model.roberta.embeddings.LayerNorm.original_module.bias                                          1_024  torch.float16
base_model.model.roberta.embeddings.LayerNorm.modules_to_save.default.weight                                1_024  torch.float16
base_model.model.roberta.embeddings.LayerNorm.modules_to_save.default.bias                                  1_024  torch.float16
base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.0.attention.self.key.base_layer.weight                             524_288  torch.uint8
base_model.model.roberta.encoder.layer.0.attention.self.key.base_layer.bias                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.0.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.0.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.0.attention.output.dense.weight                                    524_288  torch.uint8
base_model.model.roberta.encoder.layer.0.attention.output.dense.bias                                        1_024  torch.float16
base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.original_module.weight                  1_024  torch.float16
base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.original_module.bias                    1_024  torch.float16
base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.0.intermediate.dense.weight                                      2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.0.intermediate.dense.bias                                            4_096  torch.float16
base_model.model.roberta.encoder.layer.0.output.dense.weight                                            2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.0.output.dense.bias                                                  1_024  torch.float16
base_model.model.roberta.encoder.layer.0.output.LayerNorm.original_module.weight                            1_024  torch.float16
base_model.model.roberta.encoder.layer.0.output.LayerNorm.original_module.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.0.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.0.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.1.attention.self.key.base_layer.weight                             524_288  torch.uint8
base_model.model.roberta.encoder.layer.1.attention.self.key.base_layer.bias                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.1.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.1.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.1.attention.output.dense.weight                                    524_288  torch.uint8
base_model.model.roberta.encoder.layer.1.attention.output.dense.bias                                        1_024  torch.float16
base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.original_module.weight                  1_024  torch.float16
base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.original_module.bias                    1_024  torch.float16
base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.1.intermediate.dense.weight                                      2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.1.intermediate.dense.bias                                            4_096  torch.float16
base_model.model.roberta.encoder.layer.1.output.dense.weight                                            2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.1.output.dense.bias                                                  1_024  torch.float16
base_model.model.roberta.encoder.layer.1.output.LayerNorm.original_module.weight                            1_024  torch.float16
base_model.model.roberta.encoder.layer.1.output.LayerNorm.original_module.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.1.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.1.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.2.attention.self.key.base_layer.weight                             524_288  torch.uint8
base_model.model.roberta.encoder.layer.2.attention.self.key.base_layer.bias                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.2.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.2.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.2.attention.output.dense.weight                                    524_288  torch.uint8
base_model.model.roberta.encoder.layer.2.attention.output.dense.bias                                        1_024  torch.float16
base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.original_module.weight                  1_024  torch.float16
base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.original_module.bias                    1_024  torch.float16
base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.2.intermediate.dense.weight                                      2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.2.intermediate.dense.bias                                            4_096  torch.float16
base_model.model.roberta.encoder.layer.2.output.dense.weight                                            2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.2.output.dense.bias                                                  1_024  torch.float16
base_model.model.roberta.encoder.layer.2.output.LayerNorm.original_module.weight                            1_024  torch.float16
base_model.model.roberta.encoder.layer.2.output.LayerNorm.original_module.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.2.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.2.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.3.attention.self.key.base_layer.weight                             524_288  torch.uint8
base_model.model.roberta.encoder.layer.3.attention.self.key.base_layer.bias                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.3.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.3.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.3.attention.output.dense.weight                                    524_288  torch.uint8
base_model.model.roberta.encoder.layer.3.attention.output.dense.bias                                        1_024  torch.float16
base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.original_module.weight                  1_024  torch.float16
base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.original_module.bias                    1_024  torch.float16
base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.3.intermediate.dense.weight                                      2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.3.intermediate.dense.bias                                            4_096  torch.float16
base_model.model.roberta.encoder.layer.3.output.dense.weight                                            2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.3.output.dense.bias                                                  1_024  torch.float16
base_model.model.roberta.encoder.layer.3.output.LayerNorm.original_module.weight                            1_024  torch.float16
base_model.model.roberta.encoder.layer.3.output.LayerNorm.original_module.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.3.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.3.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.4.attention.self.key.base_layer.weight                             524_288  torch.uint8
base_model.model.roberta.encoder.layer.4.attention.self.key.base_layer.bias                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.4.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.4.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.4.attention.output.dense.weight                                    524_288  torch.uint8
base_model.model.roberta.encoder.layer.4.attention.output.dense.bias                                        1_024  torch.float16
base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.original_module.weight                  1_024  torch.float16
base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.original_module.bias                    1_024  torch.float16
base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.4.intermediate.dense.weight                                      2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.4.intermediate.dense.bias                                            4_096  torch.float16
base_model.model.roberta.encoder.layer.4.output.dense.weight                                            2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.4.output.dense.bias                                                  1_024  torch.float16
base_model.model.roberta.encoder.layer.4.output.LayerNorm.original_module.weight                            1_024  torch.float16
base_model.model.roberta.encoder.layer.4.output.LayerNorm.original_module.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.4.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.4.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.5.attention.self.key.base_layer.weight                             524_288  torch.uint8
base_model.model.roberta.encoder.layer.5.attention.self.key.base_layer.bias                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.5.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.5.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.5.attention.output.dense.weight                                    524_288  torch.uint8
base_model.model.roberta.encoder.layer.5.attention.output.dense.bias                                        1_024  torch.float16
base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.original_module.weight                  1_024  torch.float16
base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.original_module.bias                    1_024  torch.float16
base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.5.intermediate.dense.weight                                      2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.5.intermediate.dense.bias                                            4_096  torch.float16
base_model.model.roberta.encoder.layer.5.output.dense.weight                                            2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.5.output.dense.bias                                                  1_024  torch.float16
base_model.model.roberta.encoder.layer.5.output.LayerNorm.original_module.weight                            1_024  torch.float16
base_model.model.roberta.encoder.layer.5.output.LayerNorm.original_module.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.5.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.5.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.6.attention.self.key.base_layer.weight                             524_288  torch.uint8
base_model.model.roberta.encoder.layer.6.attention.self.key.base_layer.bias                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.6.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.6.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.6.attention.output.dense.weight                                    524_288  torch.uint8
base_model.model.roberta.encoder.layer.6.attention.output.dense.bias                                        1_024  torch.float16
base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.original_module.weight                  1_024  torch.float16
base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.original_module.bias                    1_024  torch.float16
base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.6.intermediate.dense.weight                                      2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.6.intermediate.dense.bias                                            4_096  torch.float16
base_model.model.roberta.encoder.layer.6.output.dense.weight                                            2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.6.output.dense.bias                                                  1_024  torch.float16
base_model.model.roberta.encoder.layer.6.output.LayerNorm.original_module.weight                            1_024  torch.float16
base_model.model.roberta.encoder.layer.6.output.LayerNorm.original_module.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.6.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.6.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.7.attention.self.key.base_layer.weight                             524_288  torch.uint8
base_model.model.roberta.encoder.layer.7.attention.self.key.base_layer.bias                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.7.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.7.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.7.attention.output.dense.weight                                    524_288  torch.uint8
base_model.model.roberta.encoder.layer.7.attention.output.dense.bias                                        1_024  torch.float16
base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.original_module.weight                  1_024  torch.float16
base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.original_module.bias                    1_024  torch.float16
base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.7.intermediate.dense.weight                                      2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.7.intermediate.dense.bias                                            4_096  torch.float16
base_model.model.roberta.encoder.layer.7.output.dense.weight                                            2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.7.output.dense.bias                                                  1_024  torch.float16
base_model.model.roberta.encoder.layer.7.output.LayerNorm.original_module.weight                            1_024  torch.float16
base_model.model.roberta.encoder.layer.7.output.LayerNorm.original_module.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.7.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.7.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.8.attention.self.key.base_layer.weight                             524_288  torch.uint8
base_model.model.roberta.encoder.layer.8.attention.self.key.base_layer.bias                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.8.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.8.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.8.attention.output.dense.weight                                    524_288  torch.uint8
base_model.model.roberta.encoder.layer.8.attention.output.dense.bias                                        1_024  torch.float16
base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.original_module.weight                  1_024  torch.float16
base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.original_module.bias                    1_024  torch.float16
base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.8.intermediate.dense.weight                                      2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.8.intermediate.dense.bias                                            4_096  torch.float16
base_model.model.roberta.encoder.layer.8.output.dense.weight                                            2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.8.output.dense.bias                                                  1_024  torch.float16
base_model.model.roberta.encoder.layer.8.output.LayerNorm.original_module.weight                            1_024  torch.float16
base_model.model.roberta.encoder.layer.8.output.LayerNorm.original_module.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.8.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.8.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.9.attention.self.key.base_layer.weight                             524_288  torch.uint8
base_model.model.roberta.encoder.layer.9.attention.self.key.base_layer.bias                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.9.attention.self.key.lora_A.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.9.attention.self.key.lora_B.default.weight                           8_192  torch.float32
base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer.weight                           524_288  torch.uint8
base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer.bias                               1_024  torch.float16
base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default.weight                         8_192  torch.float32
base_model.model.roberta.encoder.layer.9.attention.output.dense.weight                                    524_288  torch.uint8
base_model.model.roberta.encoder.layer.9.attention.output.dense.bias                                        1_024  torch.float16
base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.original_module.weight                  1_024  torch.float16
base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.original_module.bias                    1_024  torch.float16
base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.modules_to_save.default.weight          1_024  torch.float16
base_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.modules_to_save.default.bias            1_024  torch.float16
base_model.model.roberta.encoder.layer.9.intermediate.dense.weight                                      2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.9.intermediate.dense.bias                                            4_096  torch.float16
base_model.model.roberta.encoder.layer.9.output.dense.weight                                            2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.9.output.dense.bias                                                  1_024  torch.float16
base_model.model.roberta.encoder.layer.9.output.LayerNorm.original_module.weight                            1_024  torch.float16
base_model.model.roberta.encoder.layer.9.output.LayerNorm.original_module.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.9.output.LayerNorm.modules_to_save.default.weight                    1_024  torch.float16
base_model.model.roberta.encoder.layer.9.output.LayerNorm.modules_to_save.default.bias                      1_024  torch.float16
base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.10.attention.self.key.base_layer.weight                            524_288  torch.uint8
base_model.model.roberta.encoder.layer.10.attention.self.key.base_layer.bias                                1_024  torch.float16
base_model.model.roberta.encoder.layer.10.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.10.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.10.attention.output.dense.weight                                   524_288  torch.uint8
base_model.model.roberta.encoder.layer.10.attention.output.dense.bias                                       1_024  torch.float16
base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.original_module.weight                 1_024  torch.float16
base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.original_module.bias                   1_024  torch.float16
base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.10.intermediate.dense.weight                                     2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.10.intermediate.dense.bias                                           4_096  torch.float16
base_model.model.roberta.encoder.layer.10.output.dense.weight                                           2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.10.output.dense.bias                                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.10.output.LayerNorm.original_module.weight                           1_024  torch.float16
base_model.model.roberta.encoder.layer.10.output.LayerNorm.original_module.bias                             1_024  torch.float16
base_model.model.roberta.encoder.layer.10.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.10.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.11.attention.self.key.base_layer.weight                            524_288  torch.uint8
base_model.model.roberta.encoder.layer.11.attention.self.key.base_layer.bias                                1_024  torch.float16
base_model.model.roberta.encoder.layer.11.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.11.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.11.attention.output.dense.weight                                   524_288  torch.uint8
base_model.model.roberta.encoder.layer.11.attention.output.dense.bias                                       1_024  torch.float16
base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.original_module.weight                 1_024  torch.float16
base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.original_module.bias                   1_024  torch.float16
base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.11.intermediate.dense.weight                                     2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.11.intermediate.dense.bias                                           4_096  torch.float16
base_model.model.roberta.encoder.layer.11.output.dense.weight                                           2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.11.output.dense.bias                                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.11.output.LayerNorm.original_module.weight                           1_024  torch.float16
base_model.model.roberta.encoder.layer.11.output.LayerNorm.original_module.bias                             1_024  torch.float16
base_model.model.roberta.encoder.layer.11.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.11.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.12.attention.self.query.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.12.attention.self.query.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.12.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.12.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.12.attention.self.key.base_layer.weight                            524_288  torch.uint8
base_model.model.roberta.encoder.layer.12.attention.self.key.base_layer.bias                                1_024  torch.float16
base_model.model.roberta.encoder.layer.12.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.12.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.12.attention.self.value.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.12.attention.self.value.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.12.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.12.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.12.attention.output.dense.weight                                   524_288  torch.uint8
base_model.model.roberta.encoder.layer.12.attention.output.dense.bias                                       1_024  torch.float16
base_model.model.roberta.encoder.layer.12.attention.output.LayerNorm.original_module.weight                 1_024  torch.float16
base_model.model.roberta.encoder.layer.12.attention.output.LayerNorm.original_module.bias                   1_024  torch.float16
base_model.model.roberta.encoder.layer.12.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.12.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.12.intermediate.dense.weight                                     2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.12.intermediate.dense.bias                                           4_096  torch.float16
base_model.model.roberta.encoder.layer.12.output.dense.weight                                           2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.12.output.dense.bias                                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.12.output.LayerNorm.original_module.weight                           1_024  torch.float16
base_model.model.roberta.encoder.layer.12.output.LayerNorm.original_module.bias                             1_024  torch.float16
base_model.model.roberta.encoder.layer.12.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.12.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.13.attention.self.query.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.13.attention.self.query.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.13.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.13.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.13.attention.self.key.base_layer.weight                            524_288  torch.uint8
base_model.model.roberta.encoder.layer.13.attention.self.key.base_layer.bias                                1_024  torch.float16
base_model.model.roberta.encoder.layer.13.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.13.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.13.attention.self.value.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.13.attention.self.value.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.13.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.13.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.13.attention.output.dense.weight                                   524_288  torch.uint8
base_model.model.roberta.encoder.layer.13.attention.output.dense.bias                                       1_024  torch.float16
base_model.model.roberta.encoder.layer.13.attention.output.LayerNorm.original_module.weight                 1_024  torch.float16
base_model.model.roberta.encoder.layer.13.attention.output.LayerNorm.original_module.bias                   1_024  torch.float16
base_model.model.roberta.encoder.layer.13.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.13.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.13.intermediate.dense.weight                                     2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.13.intermediate.dense.bias                                           4_096  torch.float16
base_model.model.roberta.encoder.layer.13.output.dense.weight                                           2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.13.output.dense.bias                                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.13.output.LayerNorm.original_module.weight                           1_024  torch.float16
base_model.model.roberta.encoder.layer.13.output.LayerNorm.original_module.bias                             1_024  torch.float16
base_model.model.roberta.encoder.layer.13.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.13.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.14.attention.self.query.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.14.attention.self.query.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.14.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.14.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.14.attention.self.key.base_layer.weight                            524_288  torch.uint8
base_model.model.roberta.encoder.layer.14.attention.self.key.base_layer.bias                                1_024  torch.float16
base_model.model.roberta.encoder.layer.14.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.14.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.14.attention.self.value.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.14.attention.self.value.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.14.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.14.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.14.attention.output.dense.weight                                   524_288  torch.uint8
base_model.model.roberta.encoder.layer.14.attention.output.dense.bias                                       1_024  torch.float16
base_model.model.roberta.encoder.layer.14.attention.output.LayerNorm.original_module.weight                 1_024  torch.float16
base_model.model.roberta.encoder.layer.14.attention.output.LayerNorm.original_module.bias                   1_024  torch.float16
base_model.model.roberta.encoder.layer.14.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.14.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.14.intermediate.dense.weight                                     2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.14.intermediate.dense.bias                                           4_096  torch.float16
base_model.model.roberta.encoder.layer.14.output.dense.weight                                           2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.14.output.dense.bias                                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.14.output.LayerNorm.original_module.weight                           1_024  torch.float16
base_model.model.roberta.encoder.layer.14.output.LayerNorm.original_module.bias                             1_024  torch.float16
base_model.model.roberta.encoder.layer.14.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.14.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.15.attention.self.query.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.15.attention.self.query.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.15.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.15.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.15.attention.self.key.base_layer.weight                            524_288  torch.uint8
base_model.model.roberta.encoder.layer.15.attention.self.key.base_layer.bias                                1_024  torch.float16
base_model.model.roberta.encoder.layer.15.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.15.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.15.attention.self.value.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.15.attention.self.value.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.15.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.15.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.15.attention.output.dense.weight                                   524_288  torch.uint8
base_model.model.roberta.encoder.layer.15.attention.output.dense.bias                                       1_024  torch.float16
base_model.model.roberta.encoder.layer.15.attention.output.LayerNorm.original_module.weight                 1_024  torch.float16
base_model.model.roberta.encoder.layer.15.attention.output.LayerNorm.original_module.bias                   1_024  torch.float16
base_model.model.roberta.encoder.layer.15.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.15.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.15.intermediate.dense.weight                                     2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.15.intermediate.dense.bias                                           4_096  torch.float16
base_model.model.roberta.encoder.layer.15.output.dense.weight                                           2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.15.output.dense.bias                                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.15.output.LayerNorm.original_module.weight                           1_024  torch.float16
base_model.model.roberta.encoder.layer.15.output.LayerNorm.original_module.bias                             1_024  torch.float16
base_model.model.roberta.encoder.layer.15.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.15.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.16.attention.self.query.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.16.attention.self.query.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.16.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.16.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.16.attention.self.key.base_layer.weight                            524_288  torch.uint8
base_model.model.roberta.encoder.layer.16.attention.self.key.base_layer.bias                                1_024  torch.float16
base_model.model.roberta.encoder.layer.16.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.16.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.16.attention.self.value.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.16.attention.self.value.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.16.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.16.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.16.attention.output.dense.weight                                   524_288  torch.uint8
base_model.model.roberta.encoder.layer.16.attention.output.dense.bias                                       1_024  torch.float16
base_model.model.roberta.encoder.layer.16.attention.output.LayerNorm.original_module.weight                 1_024  torch.float16
base_model.model.roberta.encoder.layer.16.attention.output.LayerNorm.original_module.bias                   1_024  torch.float16
base_model.model.roberta.encoder.layer.16.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.16.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.16.intermediate.dense.weight                                     2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.16.intermediate.dense.bias                                           4_096  torch.float16
base_model.model.roberta.encoder.layer.16.output.dense.weight                                           2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.16.output.dense.bias                                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.16.output.LayerNorm.original_module.weight                           1_024  torch.float16
base_model.model.roberta.encoder.layer.16.output.LayerNorm.original_module.bias                             1_024  torch.float16
base_model.model.roberta.encoder.layer.16.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.16.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.17.attention.self.query.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.17.attention.self.query.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.17.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.17.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.17.attention.self.key.base_layer.weight                            524_288  torch.uint8
base_model.model.roberta.encoder.layer.17.attention.self.key.base_layer.bias                                1_024  torch.float16
base_model.model.roberta.encoder.layer.17.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.17.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.17.attention.self.value.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.17.attention.self.value.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.17.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.17.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.17.attention.output.dense.weight                                   524_288  torch.uint8
base_model.model.roberta.encoder.layer.17.attention.output.dense.bias                                       1_024  torch.float16
base_model.model.roberta.encoder.layer.17.attention.output.LayerNorm.original_module.weight                 1_024  torch.float16
base_model.model.roberta.encoder.layer.17.attention.output.LayerNorm.original_module.bias                   1_024  torch.float16
base_model.model.roberta.encoder.layer.17.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.17.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.17.intermediate.dense.weight                                     2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.17.intermediate.dense.bias                                           4_096  torch.float16
base_model.model.roberta.encoder.layer.17.output.dense.weight                                           2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.17.output.dense.bias                                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.17.output.LayerNorm.original_module.weight                           1_024  torch.float16
base_model.model.roberta.encoder.layer.17.output.LayerNorm.original_module.bias                             1_024  torch.float16
base_model.model.roberta.encoder.layer.17.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.17.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.18.attention.self.query.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.18.attention.self.query.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.18.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.18.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.18.attention.self.key.base_layer.weight                            524_288  torch.uint8
base_model.model.roberta.encoder.layer.18.attention.self.key.base_layer.bias                                1_024  torch.float16
base_model.model.roberta.encoder.layer.18.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.18.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.18.attention.self.value.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.18.attention.self.value.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.18.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.18.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.18.attention.output.dense.weight                                   524_288  torch.uint8
base_model.model.roberta.encoder.layer.18.attention.output.dense.bias                                       1_024  torch.float16
base_model.model.roberta.encoder.layer.18.attention.output.LayerNorm.original_module.weight                 1_024  torch.float16
base_model.model.roberta.encoder.layer.18.attention.output.LayerNorm.original_module.bias                   1_024  torch.float16
base_model.model.roberta.encoder.layer.18.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.18.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.18.intermediate.dense.weight                                     2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.18.intermediate.dense.bias                                           4_096  torch.float16
base_model.model.roberta.encoder.layer.18.output.dense.weight                                           2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.18.output.dense.bias                                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.18.output.LayerNorm.original_module.weight                           1_024  torch.float16
base_model.model.roberta.encoder.layer.18.output.LayerNorm.original_module.bias                             1_024  torch.float16
base_model.model.roberta.encoder.layer.18.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.18.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.19.attention.self.query.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.19.attention.self.query.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.19.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.19.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.19.attention.self.key.base_layer.weight                            524_288  torch.uint8
base_model.model.roberta.encoder.layer.19.attention.self.key.base_layer.bias                                1_024  torch.float16
base_model.model.roberta.encoder.layer.19.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.19.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.19.attention.self.value.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.19.attention.self.value.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.19.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.19.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.19.attention.output.dense.weight                                   524_288  torch.uint8
base_model.model.roberta.encoder.layer.19.attention.output.dense.bias                                       1_024  torch.float16
base_model.model.roberta.encoder.layer.19.attention.output.LayerNorm.original_module.weight                 1_024  torch.float16
base_model.model.roberta.encoder.layer.19.attention.output.LayerNorm.original_module.bias                   1_024  torch.float16
base_model.model.roberta.encoder.layer.19.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.19.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.19.intermediate.dense.weight                                     2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.19.intermediate.dense.bias                                           4_096  torch.float16
base_model.model.roberta.encoder.layer.19.output.dense.weight                                           2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.19.output.dense.bias                                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.19.output.LayerNorm.original_module.weight                           1_024  torch.float16
base_model.model.roberta.encoder.layer.19.output.LayerNorm.original_module.bias                             1_024  torch.float16
base_model.model.roberta.encoder.layer.19.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.19.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.20.attention.self.query.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.20.attention.self.query.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.20.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.20.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.20.attention.self.key.base_layer.weight                            524_288  torch.uint8
base_model.model.roberta.encoder.layer.20.attention.self.key.base_layer.bias                                1_024  torch.float16
base_model.model.roberta.encoder.layer.20.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.20.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.20.attention.self.value.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.20.attention.self.value.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.20.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.20.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.20.attention.output.dense.weight                                   524_288  torch.uint8
base_model.model.roberta.encoder.layer.20.attention.output.dense.bias                                       1_024  torch.float16
base_model.model.roberta.encoder.layer.20.attention.output.LayerNorm.original_module.weight                 1_024  torch.float16
base_model.model.roberta.encoder.layer.20.attention.output.LayerNorm.original_module.bias                   1_024  torch.float16
base_model.model.roberta.encoder.layer.20.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.20.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.20.intermediate.dense.weight                                     2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.20.intermediate.dense.bias                                           4_096  torch.float16
base_model.model.roberta.encoder.layer.20.output.dense.weight                                           2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.20.output.dense.bias                                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.20.output.LayerNorm.original_module.weight                           1_024  torch.float16
base_model.model.roberta.encoder.layer.20.output.LayerNorm.original_module.bias                             1_024  torch.float16
base_model.model.roberta.encoder.layer.20.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.20.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.21.attention.self.query.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.21.attention.self.query.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.21.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.21.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.21.attention.self.key.base_layer.weight                            524_288  torch.uint8
base_model.model.roberta.encoder.layer.21.attention.self.key.base_layer.bias                                1_024  torch.float16
base_model.model.roberta.encoder.layer.21.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.21.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.21.attention.self.value.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.21.attention.self.value.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.21.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.21.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.21.attention.output.dense.weight                                   524_288  torch.uint8
base_model.model.roberta.encoder.layer.21.attention.output.dense.bias                                       1_024  torch.float16
base_model.model.roberta.encoder.layer.21.attention.output.LayerNorm.original_module.weight                 1_024  torch.float16
base_model.model.roberta.encoder.layer.21.attention.output.LayerNorm.original_module.bias                   1_024  torch.float16
base_model.model.roberta.encoder.layer.21.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.21.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.21.intermediate.dense.weight                                     2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.21.intermediate.dense.bias                                           4_096  torch.float16
base_model.model.roberta.encoder.layer.21.output.dense.weight                                           2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.21.output.dense.bias                                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.21.output.LayerNorm.original_module.weight                           1_024  torch.float16
base_model.model.roberta.encoder.layer.21.output.LayerNorm.original_module.bias                             1_024  torch.float16
base_model.model.roberta.encoder.layer.21.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.21.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.22.attention.self.query.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.22.attention.self.query.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.22.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.22.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.22.attention.self.key.base_layer.weight                            524_288  torch.uint8
base_model.model.roberta.encoder.layer.22.attention.self.key.base_layer.bias                                1_024  torch.float16
base_model.model.roberta.encoder.layer.22.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.22.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.22.attention.self.value.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.22.attention.self.value.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.22.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.22.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.22.attention.output.dense.weight                                   524_288  torch.uint8
base_model.model.roberta.encoder.layer.22.attention.output.dense.bias                                       1_024  torch.float16
base_model.model.roberta.encoder.layer.22.attention.output.LayerNorm.original_module.weight                 1_024  torch.float16
base_model.model.roberta.encoder.layer.22.attention.output.LayerNorm.original_module.bias                   1_024  torch.float16
base_model.model.roberta.encoder.layer.22.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.22.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.22.intermediate.dense.weight                                     2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.22.intermediate.dense.bias                                           4_096  torch.float16
base_model.model.roberta.encoder.layer.22.output.dense.weight                                           2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.22.output.dense.bias                                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.22.output.LayerNorm.original_module.weight                           1_024  torch.float16
base_model.model.roberta.encoder.layer.22.output.LayerNorm.original_module.bias                             1_024  torch.float16
base_model.model.roberta.encoder.layer.22.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.22.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.roberta.encoder.layer.23.attention.self.query.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.23.attention.self.query.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.23.attention.self.query.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.23.attention.self.query.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.23.attention.self.key.base_layer.weight                            524_288  torch.uint8
base_model.model.roberta.encoder.layer.23.attention.self.key.base_layer.bias                                1_024  torch.float16
base_model.model.roberta.encoder.layer.23.attention.self.key.lora_A.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.23.attention.self.key.lora_B.default.weight                          8_192  torch.float32
base_model.model.roberta.encoder.layer.23.attention.self.value.base_layer.weight                          524_288  torch.uint8
base_model.model.roberta.encoder.layer.23.attention.self.value.base_layer.bias                              1_024  torch.float16
base_model.model.roberta.encoder.layer.23.attention.self.value.lora_A.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.23.attention.self.value.lora_B.default.weight                        8_192  torch.float32
base_model.model.roberta.encoder.layer.23.attention.output.dense.weight                                   524_288  torch.uint8
base_model.model.roberta.encoder.layer.23.attention.output.dense.bias                                       1_024  torch.float16
base_model.model.roberta.encoder.layer.23.attention.output.LayerNorm.original_module.weight                 1_024  torch.float16
base_model.model.roberta.encoder.layer.23.attention.output.LayerNorm.original_module.bias                   1_024  torch.float16
base_model.model.roberta.encoder.layer.23.attention.output.LayerNorm.modules_to_save.default.weight         1_024  torch.float16
base_model.model.roberta.encoder.layer.23.attention.output.LayerNorm.modules_to_save.default.bias           1_024  torch.float16
base_model.model.roberta.encoder.layer.23.intermediate.dense.weight                                     2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.23.intermediate.dense.bias                                           4_096  torch.float16
base_model.model.roberta.encoder.layer.23.output.dense.weight                                           2_097_152  torch.uint8
base_model.model.roberta.encoder.layer.23.output.dense.bias                                                 1_024  torch.float16
base_model.model.roberta.encoder.layer.23.output.LayerNorm.original_module.weight                           1_024  torch.float16
base_model.model.roberta.encoder.layer.23.output.LayerNorm.original_module.bias                             1_024  torch.float16
base_model.model.roberta.encoder.layer.23.output.LayerNorm.modules_to_save.default.weight                   1_024  torch.float16
base_model.model.roberta.encoder.layer.23.output.LayerNorm.modules_to_save.default.bias                     1_024  torch.float16
base_model.model.qa_outputs.original_module.weight                                                          2_048  torch.float16
base_model.model.qa_outputs.original_module.bias                                                                2  torch.float16
base_model.model.qa_outputs.modules_to_save.default.weight                                                  2_048  torch.float16
base_model.model.qa_outputs.modules_to_save.default.bias                                                        2  torch.float16
---------------------------------------------------------------------------------------------------  ------------  -------------
TOTAL                                                                                                 204_599_300